# telegraph-publisher

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã

```
‚îî‚îÄ‚îÄ telegraph-publisher/
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ research_anchors.test.ts
    ‚îÇ   ‚îî‚îÄ‚îÄ research_anchors.ts
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ cache/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PagesCacheManager.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PagesCacheManager.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ cli/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ debug-integration.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EnhancedCommands.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProgressIndicator.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ConfigManager.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ content/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ContentProcessor.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ContentProcessor.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ dependencies/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DependencyManager.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DependencyManager.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ doc/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anchors.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.md
    ‚îÇ   ‚îú‚îÄ‚îÄ integration/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user-scenario.test.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ links/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AutoRepairer.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AutoRepairer.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BidirectionalLinkResolver.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BidirectionalLinkResolver.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ InteractiveRepairer.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkResolver.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkResolver.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkScanner.regex-fix.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkScanner.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkScanner.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkVerifier.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LinkVerifier.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ReportGenerator.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ResearchValidation.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ metadata/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MetadataManager.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MetadataManager.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ publisher/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EnhancedTelegraphPublisher.debug-hash-skip.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EnhancedTelegraphPublisher.debug.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EnhancedTelegraphPublisher.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EnhancedTelegraphPublisher.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ ratelimiter/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CountdownTimer.test.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CountdownTimer.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ demo-countdown.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RateLimiter.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RateLimiter.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ test-utils/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TestHelpers.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ types/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ utils/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PathResolver.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PathResolver.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ workflow/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PublicationWorkflowManager.test.ts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PublicationWorkflowManager.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ clean_mr.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ cli.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ integration.test.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ markdownConverter.numberedHeadings.test.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ markdownConverter.parentheses-bug.test.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ markdownConverter.test.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ markdownConverter.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ slice_book.ts
    ‚îÇ   ‚îú‚îÄ‚îÄ telegraphPublisher.test.ts
    ‚îÇ   ‚îî‚îÄ‚îÄ telegraphPublisher.ts
    ‚îú‚îÄ‚îÄ test_findLocalLinks.js
    ‚îî‚îÄ‚îÄ test-relative-links.test.ts
```

## –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤

`scripts/research_anchors.test.ts`

```ts
import { expect, test, spyOn } from "bun:test";

test("research_anchors script validates access token requirement", () => {
  // Test that the script includes all required test headings
  const expectedHeadings = [
    "Simple Title",
    "Title With Spaces", 
    "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ",
    "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏",
    "1. Numbered Heading",
    "Heading with 123",
    "Title with dot.",
    "Title with comma,",
    "Title with colon:",
    "Title with question mark?",
    "Title with exclamation!",
    "–ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)",
    "–í–æ–ø—Ä–æ—Å 4: –û–ø–∏—à–∏—Ç–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏ —Å–ª—É—à–∞—Ç—å ¬´–®—Ä–∏–º–∞–¥-–ë—Ö–∞–≥–∞–≤–∞—Ç–∞–º¬ª.",
    "Title with @#$%^&*()_+-=[]{}|;'\"<>/",
    "MixedCaseTitle",
    "Title With Mixed Case",
    "**Bold Title**",
    "*Italic Title*",
    "`Code Title`",
    "[Link Title](url)",
    "**Bold *and Italic* Title**"
  ];

  // Validate that we have exactly 21 test headings as specified
  expect(expectedHeadings).toHaveLength(21);
  
  // Validate coverage of different categories
  const hasBasicCases = expectedHeadings.some(h => h === "Simple Title");
  const hasCyrillic = expectedHeadings.some(h => h === "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ");
  const hasNumbers = expectedHeadings.some(h => h === "1. Numbered Heading");
  const hasSpecialChars = expectedHeadings.some(h => h.includes("@#$%"));
  const hasMarkdown = expectedHeadings.some(h => h.includes("**Bold"));
  const hasComplexCyrillic = expectedHeadings.some(h => h.includes("–ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª"));
  
  expect(hasBasicCases).toBe(true);
  expect(hasCyrillic).toBe(true);
  expect(hasNumbers).toBe(true);
  expect(hasSpecialChars).toBe(true);
  expect(hasMarkdown).toBe(true);
  expect(hasComplexCyrillic).toBe(true);
});

test("research_anchors script has correct file structure", async () => {
  // Test that the script file exists and can be imported
  const scriptExists = await Bun.file("scripts/research_anchors.ts").exists();
  expect(scriptExists).toBe(true);
  
  // Test that the script contains the expected imports and structure
  const scriptContent = await Bun.file("scripts/research_anchors.ts").text();
  
  expect(scriptContent).toContain("import { TelegraphPublisher, type TelegraphNode }");
  expect(scriptContent).toContain("const testHeadings: string[]");
  expect(scriptContent).toContain("async function main()");
  expect(scriptContent).toContain("process.argv[2]");
  expect(scriptContent).toContain("publisher.publishNodes");
  expect(scriptContent).toContain("main()");
});

test("research_anchors script includes comprehensive test cases", async () => {
  const scriptContent = await Bun.file("scripts/research_anchors.ts").text();
  
  // Test for presence of all major test case categories
  expect(scriptContent).toContain("Simple Title");                    // Basic case
  expect(scriptContent).toContain("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ");         // Cyrillic
  expect(scriptContent).toContain("1. Numbered Heading");            // Numbers
  expect(scriptContent).toContain("Title with dot.");               // Punctuation
  expect(scriptContent).toContain("**Bold Title**");               // Markdown
  expect(scriptContent).toContain("@#$%^&*()_+-=[]{}|");          // Special symbols
  expect(scriptContent).toContain("MixedCaseTitle");              // Case variations
  expect(scriptContent).toContain("–ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª"); // Complex Cyrillic
});

test("research_anchors script has proper error handling", async () => {
  const scriptContent = await Bun.file("scripts/research_anchors.ts").text();
  
  // Test for error handling patterns
  expect(scriptContent).toContain("if (!accessToken)");
  expect(scriptContent).toContain("console.error");
  expect(scriptContent).toContain("process.exit(1)");
  expect(scriptContent).toContain("try {");
  expect(scriptContent).toContain("} catch (error)");
  expect(scriptContent).toContain("Usage: bun scripts/research_anchors.ts");
});

test("research_anchors script provides clear output format", async () => {
  const scriptContent = await Bun.file("scripts/research_anchors.ts").text();
  
  // Test for expected output messages
  expect(scriptContent).toContain("Starting anchor research publication");
  expect(scriptContent).toContain("Publication successful");
  expect(scriptContent).toContain("Next Steps:");
  expect(scriptContent).toContain("Right-click on each heading");
  expect(scriptContent).toContain("find the `id` attribute");
  expect(scriptContent).toContain("Compare the original heading text");
});
```

`scripts/research_anchors.ts`

```ts
import { TelegraphPublisher, type TelegraphNode } from '../src/telegraphPublisher';

const testHeadings: string[] = [
  // Basic cases
  "Simple Title",
  "Title With Spaces",
  // Cyrillic
  "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ",
  "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏",
  // Numbers
  "1. Numbered Heading",
  "Heading with 123",
  // Special Characters (common)
  "Title with dot.",
  "Title with comma,",
  "Title with colon:",
  "Title with question mark?",
  "Title with exclamation!",
  // Special Characters (problematic from logs)
  "–ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)",
  "–í–æ–ø—Ä–æ—Å 4: –û–ø–∏—à–∏—Ç–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏ —Å–ª—É—à–∞—Ç—å ¬´–®—Ä–∏–º–∞–¥-–ë—Ö–∞–≥–∞–≤–∞—Ç–∞–º¬ª.",
  // Other symbols
  "Title with @#$%^&*()_+-=[]{}|;'\"<>/",
  // Mixed case
  "MixedCaseTitle",
  "Title With Mixed Case",
  // Markdown formatting
  "**Bold Title**",
  "*Italic Title*",
  "`Code Title`",
  "[Link Title](url)",
  "**Bold *and Italic* Title**"
];

async function main() {
  const accessToken = process.argv[2];

  if (!accessToken) {
    console.error("‚ùå Error: Access token is required.");
    console.log("Usage: bun scripts/research_anchors.ts <YOUR_ACCESS_TOKEN>");
    process.exit(1);
  }

  try {
    console.log("üöÄ Starting anchor research publication...");
    const publisher = new TelegraphPublisher();
    publisher.setAccessToken(accessToken);

    const nodes: TelegraphNode[] = testHeadings.map(text => ({
      tag: 'h3',
      children: [text]
    }));

    const page = await publisher.publishNodes("Anchor Research Page", nodes);

    console.log("\n‚úÖ Publication successful!");
    console.log("=======================================");
    console.log("üîó URL:", page.url);
    console.log("=======================================");
    console.log("\nüïµÔ∏è‚Äç‚ôÇÔ∏è Next Steps:");
    console.log("1. Open the URL above in your browser.");
    console.log("2. Right-click on each heading and select 'Inspect'.");
    console.log("3. In the developer tools, find the `id` attribute of the `<h3>` tag.");
    console.log("4. Compare the original heading text with the generated `id` to determine the rules.");

  } catch (error) {
    console.error("‚ùå Publication failed:", error instanceof Error ? error.message : String(error));
    process.exit(1);
  }
}

// Execute the script
main();
```

`src/cache/PagesCacheManager.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, it } from "bun:test";
import { join } from "node:path";
import { TelegraphPublisher } from "../telegraphPublisher";
import { TestHelpers } from "../test-utils/TestHelpers";
import { PagesCacheManager } from "./PagesCacheManager";

describe("PagesCacheManager", () => {
  let tempDir: string;
  let cacheManager: PagesCacheManager;
  let mockPublisher: TelegraphPublisher;

  beforeEach(() => {
    tempDir = TestHelpers.createTempDir("cache-manager-test");
    cacheManager = new PagesCacheManager(tempDir, "test-access-token-123");
    mockPublisher = new TelegraphPublisher();
  });

  afterEach(() => {
    TestHelpers.cleanup();
  });

  describe("constructor and initialization", () => {
    it("should create cache manager with empty cache", () => {
      const stats = cacheManager.getStats();
      expect(stats.totalPages).toBe(0);
      expect(stats.pagesWithLocalPaths).toBe(0);
    });

    it("should create cache file when data is saved", () => {
      const pageInfo = TestHelpers.createSamplePageInfo();
      cacheManager.addPage(pageInfo);
      cacheManager.updatePage(pageInfo.telegraphUrl, { views: 100 }); // This triggers save

      const cacheFilePath = join(tempDir, ".telegraph-pages-cache.json");
      expect(TestHelpers.fileExists(cacheFilePath)).toBe(true);
    });
  });

  describe("addPage", () => {
    it("should add page to cache", () => {
      const pageInfo = TestHelpers.createSamplePageInfo();

      cacheManager.addPage(pageInfo);

      const retrieved = cacheManager.getPageByUrl(pageInfo.telegraphUrl);
      expect(retrieved).toEqual(pageInfo);
    });

    it("should create bidirectional mapping when local path provided", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: join(tempDir, "test.md")
      });

      cacheManager.addPage(pageInfo);

      expect(cacheManager.getTelegraphUrl(pageInfo.localFilePath!)).toBe(pageInfo.telegraphUrl);
      expect(cacheManager.getLocalPath(pageInfo.telegraphUrl)).toBe(pageInfo.localFilePath!);
    });

    it("should handle page without local path", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: undefined
      });

      cacheManager.addPage(pageInfo);

      const retrieved = cacheManager.getPageByUrl(pageInfo.telegraphUrl);
      expect(retrieved).toEqual(pageInfo);
      expect(cacheManager.getLocalPath(pageInfo.telegraphUrl)).toBeNull();
    });
  });

  describe("updatePage", () => {
    it("should update existing page", () => {
      const pageInfo = TestHelpers.createSamplePageInfo();
      cacheManager.addPage(pageInfo);

      const updates = {
        title: "Updated Title",
        views: 100
      };

      cacheManager.updatePage(pageInfo.telegraphUrl, updates);

      const updated = cacheManager.getPageByUrl(pageInfo.telegraphUrl);
      expect(updated?.title).toBe("Updated Title");
      expect(updated?.views).toBe(100);
      expect(updated?.authorName).toBe(pageInfo.authorName); // Unchanged
    });

    it("should update local file path mapping", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: join(tempDir, "old.md")
      });
      cacheManager.addPage(pageInfo);

      const newLocalPath = join(tempDir, "new.md");
      cacheManager.updatePage(pageInfo.telegraphUrl, {
        localFilePath: newLocalPath
      });

      expect(cacheManager.getLocalPath(pageInfo.telegraphUrl)).toBe(newLocalPath);
      expect(cacheManager.getTelegraphUrl(newLocalPath)).toBe(pageInfo.telegraphUrl);
      expect(cacheManager.getTelegraphUrl(pageInfo.localFilePath!)).toBeNull();
    });

    it("should not update non-existent page", () => {
      cacheManager.updatePage("https://telegra.ph/Non-Existent", { title: "New Title" });

      const page = cacheManager.getPageByUrl("https://telegra.ph/Non-Existent");
      expect(page).toBeNull();
    });
  });

  describe("removePage", () => {
    it("should remove page from cache", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: join(tempDir, "test.md")
      });
      cacheManager.addPage(pageInfo);

      cacheManager.removePage(pageInfo.telegraphUrl);

      expect(cacheManager.getPageByUrl(pageInfo.telegraphUrl)).toBeNull();
      expect(cacheManager.getLocalPath(pageInfo.telegraphUrl)).toBeNull();
      expect(cacheManager.getTelegraphUrl(pageInfo.localFilePath!)).toBeNull();
    });

    it("should handle removal of non-existent page", () => {
      cacheManager.removePage("https://telegra.ph/Non-Existent");
      // Should not throw error
      expect(true).toBe(true);
    });
  });

  describe("getPageByLocalPath", () => {
    it("should retrieve page by local file path", () => {
      const localPath = join(tempDir, "test.md");
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: localPath
      });
      cacheManager.addPage(pageInfo);

      const retrieved = cacheManager.getPageByLocalPath(localPath);
      expect(retrieved).toEqual(pageInfo);
    });

    it("should return null for unmapped local path", () => {
      const result = cacheManager.getPageByLocalPath(join(tempDir, "unmapped.md"));
      expect(result).toBeNull();
    });
  });

  describe("isOurPage", () => {
    it("should return true for cached pages", () => {
      const pageInfo = TestHelpers.createSamplePageInfo();
      cacheManager.addPage(pageInfo);

      expect(cacheManager.isOurPage(pageInfo.telegraphUrl)).toBe(true);
    });

    it("should return false for unknown pages", () => {
      expect(cacheManager.isOurPage("https://telegra.ph/Unknown")).toBe(false);
    });
  });

  describe("getAllPages", () => {
    it("should return all cached pages", () => {
      const page1 = TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-1"
      });
      const page2 = TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-2"
      });

      cacheManager.addPage(page1);
      cacheManager.addPage(page2);

      const allPages = cacheManager.getAllPages();
      expect(allPages).toHaveLength(2);
      expect(allPages).toContainEqual(page1);
      expect(allPages).toContainEqual(page2);
    });

    it("should return empty array for empty cache", () => {
      const allPages = cacheManager.getAllPages();
      expect(allPages).toHaveLength(0);
    });
  });

  describe("getPublishedLocalPaths", () => {
    it("should return local paths that are published", () => {
      const path1 = join(tempDir, "file1.md");
      const path2 = join(tempDir, "file2.md");

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-1",
        localFilePath: path1
      }));

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-2",
        localFilePath: path2
      }));

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-3",
        localFilePath: undefined // No local path
      }));

      const localPaths = cacheManager.getPublishedLocalPaths();
      expect(localPaths).toHaveLength(2);
      expect(localPaths).toContain(path1);
      expect(localPaths).toContain(path2);
    });
  });

  describe("getPublishedTelegraphUrls", () => {
    it("should return all Telegraph URLs", () => {
      const url1 = "https://telegra.ph/Page-1";
      const url2 = "https://telegra.ph/Page-2";

      cacheManager.addPage(TestHelpers.createSamplePageInfo({ telegraphUrl: url1 }));
      cacheManager.addPage(TestHelpers.createSamplePageInfo({ telegraphUrl: url2 }));

      const urls = cacheManager.getPublishedTelegraphUrls();
      expect(urls).toHaveLength(2);
      expect(urls).toContain(url1);
      expect(urls).toContain(url2);
    });
  });

  describe("getStats", () => {
    it("should return correct statistics", () => {
      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-1",
        localFilePath: join(tempDir, "file1.md")
      }));

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-2",
        localFilePath: undefined
      }));

      const stats = cacheManager.getStats();
      expect(stats.totalPages).toBe(2);
      expect(stats.pagesWithLocalPaths).toBe(1);
      expect(stats.lastUpdated).toBeDefined();
      expect(stats.cacheAge).toBeDefined();
    });
  });

  describe("clear", () => {
    it("should clear all cache data", () => {
      cacheManager.addPage(TestHelpers.createSamplePageInfo());
      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Page-2"
      }));

      cacheManager.clear();

      const stats = cacheManager.getStats();
      expect(stats.totalPages).toBe(0);
      expect(stats.pagesWithLocalPaths).toBe(0);
    });
  });

  describe("export and import", () => {
    it("should export and import cache data", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: join(tempDir, "test.md")
      });
      cacheManager.addPage(pageInfo);

      const exported = cacheManager.export();

      const newTempDir = TestHelpers.createTempDir("cache-import-test");
      const newCacheManager = new PagesCacheManager(newTempDir, "test-access-token-123");
      newCacheManager.import(exported);

      const imported = newCacheManager.getPageByUrl(pageInfo.telegraphUrl);
      expect(imported).toEqual(pageInfo);
      expect(newCacheManager.getTelegraphUrl(pageInfo.localFilePath!)).toBe(pageInfo.telegraphUrl);
    });
  });

  describe("persistence", () => {
    it("should persist cache across instances", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: join(tempDir, "test.md")
      });
      cacheManager.addPage(pageInfo);

      // Force save by updating the page
      cacheManager.updatePage(pageInfo.telegraphUrl, { views: 100 });

      // Create new instance with same directory
      const newCacheManager = new PagesCacheManager(tempDir, "test-access-token-123");

      const retrieved = newCacheManager.getPageByUrl(pageInfo.telegraphUrl);
      expect(retrieved?.telegraphUrl).toBe(pageInfo.telegraphUrl);
      expect(retrieved?.localFilePath).toBe(pageInfo.localFilePath!);
      expect(retrieved?.title).toBe(pageInfo.title);
    });

    it("should create new cache for different access token", () => {
      const pageInfo = TestHelpers.createSamplePageInfo();
      cacheManager.addPage(pageInfo);

      // Create new instance with different token
      const newCacheManager = new PagesCacheManager(tempDir, "different-token");

      const stats = newCacheManager.getStats();
      expect(stats.totalPages).toBe(0);
    });
  });

  describe("syncWithTelegraph", () => {
    it("should sync pages from Telegraph API", async () => {
      // Mock the publisher.listPages method
      const mockPages = [
        {
          path: "test-page-01-01",
          url: "https://telegra.ph/test-page-01-01",
          title: "Test Page 1",
          author_name: "Test Author",
          views: 100
        },
        {
          path: "test-page-01-02",
          url: "https://telegra.ph/test-page-01-02",
          title: "Test Page 2",
          author_name: "Test Author",
          views: 50
        }
      ];

      mockPublisher.listPages = async (offset: number, limit: number) => {
        if (offset === 0) {
          return { pages: mockPages, total_count: mockPages.length };
        }
        return { pages: [], total_count: 0 };
      };

      const success = await cacheManager.syncWithTelegraph(mockPublisher);

      expect(success).toBe(true);
      expect(cacheManager.getAllPages()).toHaveLength(2);
      expect(cacheManager.getPageByUrl("https://telegra.ph/test-page-01-01")).toBeTruthy();
      expect(cacheManager.getPageByUrl("https://telegra.ph/test-page-01-02")).toBeTruthy();
    });

    it("should handle sync errors gracefully", async () => {
      // Mock the publisher to throw an error
      mockPublisher.listPages = async () => {
        throw new Error("API Error");
      };

      const success = await cacheManager.syncWithTelegraph(mockPublisher);

      expect(success).toBe(false);
      expect(cacheManager.getAllPages()).toHaveLength(0);
    });

    it("should handle pagination correctly", async () => {
      // Mock paginated response
      const firstBatch = Array.from({ length: 50 }, (_, i) => ({
        path: `page-${i}-01-01`,
        url: `https://telegra.ph/page-${i}-01-01`,
        title: `Page ${i}`,
        author_name: "Test Author",
        views: i * 10
      }));

      const secondBatch = Array.from({ length: 25 }, (_, i) => ({
        path: `page-${i + 50}-01-01`,
        url: `https://telegra.ph/page-${i + 50}-01-01`,
        title: `Page ${i + 50}`,
        author_name: "Test Author",
        views: (i + 50) * 10
      }));

      mockPublisher.listPages = async (offset: number, limit: number) => {
        if (offset === 0) {
          return { pages: firstBatch, total_count: 75 };
        } else if (offset === 50) {
          return { pages: secondBatch, total_count: 75 };
        }
        return { pages: [], total_count: 0 };
      };

      const success = await cacheManager.syncWithTelegraph(mockPublisher);

      expect(success).toBe(true);
      expect(cacheManager.getAllPages()).toHaveLength(75);
    });
  });

  describe("edge cases", () => {
    it("should handle very long URLs", () => {
      const longUrl = "https://telegra.ph/" + "a".repeat(1000);
      const pageInfo = TestHelpers.createSamplePageInfo({
        telegraphUrl: longUrl
      });

      cacheManager.addPage(pageInfo);

      const retrieved = cacheManager.getPageByUrl(longUrl);
      expect(retrieved).toEqual(pageInfo);
    });

    it("should handle special characters in URLs", () => {
      const specialUrl = "https://telegra.ph/–°—Ç–∞—Ç—å—è-—Å-—Ä—É—Å—Å–∫–∏–º–∏-–±—É–∫–≤–∞–º–∏-01-01";
      const pageInfo = TestHelpers.createSamplePageInfo({
        telegraphUrl: specialUrl
      });

      cacheManager.addPage(pageInfo);

      const retrieved = cacheManager.getPageByUrl(specialUrl);
      expect(retrieved).toEqual(pageInfo);
    });

    it("should handle very long file paths", () => {
      const longPath = join(tempDir, "very", "long", "nested", "path", "to", "file.md");
      const pageInfo = TestHelpers.createSamplePageInfo({
        localFilePath: longPath
      });

      cacheManager.addPage(pageInfo);

      expect(cacheManager.getTelegraphUrl(longPath)).toBe(pageInfo.telegraphUrl);
    });

    it("should handle empty titles and descriptions", () => {
      const pageInfo = TestHelpers.createSamplePageInfo({
        title: "",
        authorName: ""
      });

      cacheManager.addPage(pageInfo);

      const retrieved = cacheManager.getPageByUrl(pageInfo.telegraphUrl);
      expect(retrieved?.title).toBe("");
      expect(retrieved?.authorName).toBe("");
    });
  });
});
```

`src/cache/PagesCacheManager.ts`

```ts
import { createHash } from "node:crypto";
import { existsSync, readFileSync, writeFileSync } from "node:fs";
import { join } from "node:path";
import type { TelegraphPublisher } from "../telegraphPublisher";
import type { PublishedPageInfo, PublishedPagesCache } from "../types/metadata";

/**
 * Manages cache of published pages for bidirectional link management
 */
export class PagesCacheManager {
  private static readonly CACHE_FILE_NAME = ".telegraph-pages-cache.json";
  private static readonly CACHE_VERSION = "1.0.0";

  private cache: PublishedPagesCache;
  private cacheFilePath: string;
  private accessTokenHash: string;

  constructor(directory: string, accessToken: string) {
    this.cacheFilePath = join(directory, PagesCacheManager.CACHE_FILE_NAME);
    this.accessTokenHash = this.hashAccessToken(accessToken);
    this.cache = this.loadCache();
  }

  /**
   * Load cache from file or create new one
   * @returns Loaded or new cache
   */
  private loadCache(): PublishedPagesCache {
    if (existsSync(this.cacheFilePath)) {
      try {
        const cacheData = JSON.parse(readFileSync(this.cacheFilePath, "utf-8"));

        // Verify cache version and access token
        if (cacheData.version === PagesCacheManager.CACHE_VERSION &&
          cacheData.accessTokenHash === this.accessTokenHash) {
          return cacheData;
        } else {
          console.warn("‚ö†Ô∏è Cache version or access token mismatch, creating new cache");
        }
      } catch (error) {
        console.warn("‚ö†Ô∏è Error loading cache, creating new cache:", error);
      }
    }

    return this.createEmptyCache();
  }

  /**
   * Create empty cache structure
   * @returns Empty cache
   */
  private createEmptyCache(): PublishedPagesCache {
    return {
      version: PagesCacheManager.CACHE_VERSION,
      lastUpdated: new Date().toISOString(),
      accessTokenHash: this.accessTokenHash,
      pages: {},
      localToTelegraph: {},
      telegraphToLocal: {}
    };
  }

  /**
   * Save cache to file
   */
  private saveCache(): void {
    try {
      this.cache.lastUpdated = new Date().toISOString();
      writeFileSync(this.cacheFilePath, JSON.stringify(this.cache, null, 2), "utf-8");
    } catch (error) {
      console.error("‚ùå Error saving cache:", error);
    }
  }

  /**
   * Hash access token for verification
   * @param accessToken Access token to hash
   * @returns Hashed token
   */
  private hashAccessToken(accessToken: string): string {
    return createHash('sha256').update(accessToken).digest('hex').substring(0, 16);
  }

  /**
   * Sync cache with Telegraph API
   * @param publisher Telegraph publisher instance
   * @returns Success status
   */
  async syncWithTelegraph(publisher: TelegraphPublisher): Promise<boolean> {
    try {
      console.log("üîÑ Syncing published pages cache...");

      let offset = 0;
      const limit = 50;
      let hasMore = true;
      let totalSynced = 0;

      while (hasMore) {
        const pageList = await publisher.listPages(offset, limit);

        for (const page of pageList.pages) {
          const pageInfo: PublishedPageInfo = {
            telegraphUrl: page.url,
            editPath: page.path,
            title: page.title,
            authorName: page.author_name || "Unknown",
            publishedAt: new Date().toISOString(), // Telegraph API doesn't provide creation date
            lastUpdated: new Date().toISOString(),
            views: page.views
          };

          this.addPage(pageInfo);
          totalSynced++;
        }

        hasMore = pageList.pages.length === limit;
        offset += limit;
      }

      this.saveCache();
      console.log(`‚úÖ Synced ${totalSynced} published pages`);
      return true;
    } catch (error) {
      console.error("‚ùå Error syncing with Telegraph:", error);
      return false;
    }
  }

  /**
   * Add page to cache
   * @param pageInfo Page information to add
   */
  addPage(pageInfo: PublishedPageInfo): void {
    this.cache.pages[pageInfo.telegraphUrl] = pageInfo;

    if (pageInfo.localFilePath) {
      this.cache.localToTelegraph[pageInfo.localFilePath] = pageInfo.telegraphUrl;
      this.cache.telegraphToLocal[pageInfo.telegraphUrl] = pageInfo.localFilePath;
    }

    // Save cache after adding page
    this.saveCache();
  }

  /**
   * Update page in cache
   * @param telegraphUrl Telegraph URL
   * @param updates Updates to apply
   */
  updatePage(telegraphUrl: string, updates: Partial<PublishedPageInfo>): void {
    const existingPage = this.cache.pages[telegraphUrl];
    if (existingPage) {
      const updatedPage = { ...existingPage, ...updates, lastUpdated: new Date().toISOString() };
      this.cache.pages[telegraphUrl] = updatedPage;

      // Update mappings if local file path changed
      if (updates.localFilePath) {
        // Remove old mapping if exists
        if (existingPage.localFilePath) {
          delete this.cache.localToTelegraph[existingPage.localFilePath];
        }

        // Add new mapping
        this.cache.localToTelegraph[updates.localFilePath] = telegraphUrl;
        this.cache.telegraphToLocal[telegraphUrl] = updates.localFilePath;
      }

      this.saveCache();
    }
  }

  /**
   * Remove page from cache
   * @param telegraphUrl Telegraph URL to remove
   */
  removePage(telegraphUrl: string): void {
    const page = this.cache.pages[telegraphUrl];
    if (page) {
      delete this.cache.pages[telegraphUrl];

      if (page.localFilePath) {
        delete this.cache.localToTelegraph[page.localFilePath];
      }
      delete this.cache.telegraphToLocal[telegraphUrl];

      this.saveCache();
    }
  }

  /**
   * Get page by Telegraph URL
   * @param telegraphUrl Telegraph URL
   * @returns Page info if found
   */
  getPageByUrl(telegraphUrl: string): PublishedPageInfo | null {
    return this.cache.pages[telegraphUrl] || null;
  }

  /**
   * Get page by local file path
   * @param localFilePath Local file path
   * @returns Page info if found
   */
  getPageByLocalPath(localFilePath: string): PublishedPageInfo | null {
    const telegraphUrl = this.cache.localToTelegraph[localFilePath];
    return telegraphUrl ? (this.cache.pages[telegraphUrl] || null) : null;
  }

  /**
   * Get Telegraph URL by local file path
   * @param localFilePath Local file path
   * @returns Telegraph URL if found
   */
  getTelegraphUrl(localFilePath: string): string | null {
    return this.cache.localToTelegraph[localFilePath] || null;
  }

  /**
   * Get local file path by Telegraph URL
   * @param telegraphUrl Telegraph URL
   * @returns Local file path if found
   */
  getLocalPath(telegraphUrl: string): string | null {
    return this.cache.telegraphToLocal[telegraphUrl] || null;
  }

  /**
   * Check if Telegraph URL belongs to our published pages
   * @param telegraphUrl Telegraph URL to check
   * @returns True if this is our page
   */
  isOurPage(telegraphUrl: string): boolean {
    return telegraphUrl in this.cache.pages;
  }

  /**
   * Get all published pages
   * @returns Array of all published pages
   */
  getAllPages(): PublishedPageInfo[] {
    return Object.values(this.cache.pages);
  }

  /**
   * Get all local file paths that are published
   * @returns Array of local file paths
   */
  getPublishedLocalPaths(): string[] {
    return Object.keys(this.cache.localToTelegraph);
  }

  /**
   * Get all Telegraph URLs we have published
   * @returns Array of Telegraph URLs
   */
  getPublishedTelegraphUrls(): string[] {
    return Object.keys(this.cache.pages);
  }

  /**
   * Get cache statistics
   * @returns Cache statistics
   */
  getStats(): {
    totalPages: number;
    pagesWithLocalPaths: number;
    lastUpdated: string;
    cacheAge: string;
  } {
    const totalPages = Object.keys(this.cache.pages).length;
    const pagesWithLocalPaths = Object.keys(this.cache.localToTelegraph).length;
    const lastUpdated = this.cache.lastUpdated;
    const cacheAge = this.formatTimeDiff(new Date(lastUpdated), new Date());

    return {
      totalPages,
      pagesWithLocalPaths,
      lastUpdated,
      cacheAge
    };
  }

  /**
   * Clear cache
   */
  clear(): void {
    this.cache = this.createEmptyCache();
    this.saveCache();
  }

  /**
   * Export cache data
   * @returns Cache data
   */
  export(): PublishedPagesCache {
    return { ...this.cache };
  }

  /**
   * Import cache data
   * @param cacheData Cache data to import
   */
  import(cacheData: PublishedPagesCache): void {
    this.cache = cacheData;
    this.cache.accessTokenHash = this.accessTokenHash;
    this.saveCache();
  }

  /**
   * Format time difference
   * @param from From date
   * @param to To date
   * @returns Formatted time difference
   */
  private formatTimeDiff(from: Date, to: Date): string {
    const diffMs = to.getTime() - from.getTime();
    const diffMinutes = Math.floor(diffMs / (1000 * 60));
    const diffHours = Math.floor(diffMinutes / 60);
    const diffDays = Math.floor(diffHours / 24);

    if (diffDays > 0) {
      return `${diffDays} day${diffDays > 1 ? 's' : ''} ago`;
    } else if (diffHours > 0) {
      return `${diffHours} hour${diffHours > 1 ? 's' : ''} ago`;
    } else if (diffMinutes > 0) {
      return `${diffMinutes} minute${diffMinutes > 1 ? 's' : ''} ago`;
    } else {
      return 'Just now';
    }
  }
}
```

`src/cli/debug-integration.test.ts`

```ts
import { describe, it, expect, beforeEach, afterEach, jest } from 'bun:test';
import { PublicationWorkflowManager } from '../workflow/PublicationWorkflowManager';
import { writeFileSync, unlinkSync, existsSync, mkdirSync, rmSync, readFileSync } from 'fs';
import { resolve } from 'path';
import type { MetadataConfig, FileMetadata } from '../types/metadata';

/**
 * Integration tests for debug functionality through PublicationWorkflowManager
 * Tests the complete CLI workflow including --debug --force scenario
 */
describe('CLI Debug Integration Tests', () => {
  let workflowManager: PublicationWorkflowManager;
  let testDir: string;
  let mockConfig: MetadataConfig;

  beforeEach(() => {
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };
    
    workflowManager = new PublicationWorkflowManager(mockConfig, 'mock-access-token');
    testDir = resolve('./test-cli-debug-temp');
    
    if (!existsSync(testDir)) {
      mkdirSync(testDir, { recursive: true });
    }
  });

  afterEach(() => {
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('--debug flag integration', () => {
    it('should auto-enable dry-run when debug is specified', async () => {
      const testFile = resolve(testDir, 'debug-auto-dryrun.md');
      const expectedJsonFile = resolve(testDir, 'debug-auto-dryrun.json');
      
      writeFileSync(testFile, '# Debug Auto Dry-Run Test\n\nThis tests auto-enabling dry-run with debug.');

      // Mock publisher methods
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: true,
        url: '[DRY RUN] Would publish',
        path: '/test'
      });

      // Test with debug option only (should auto-enable dryRun)
      const options = { 
        debug: true, 
        noVerify: true // Skip link verification for test
      };

      await workflowManager.publish(testFile, options);

      // Verify options were modified to include dryRun
      expect(options.dryRun).toBe(true);

      // Verify publisher was called with both debug and dryRun
      expect(mockPublisher).toHaveBeenCalledWith(
        testFile, 
        'test-user',
        expect.objectContaining({
          debug: true,
          dryRun: true
        })
      );

      mockPublisher.mockRestore();
    });

    it('should create JSON file for new publication with --debug', async () => {
      const testFile = resolve(testDir, 'new-debug.md');
      const expectedJsonFile = resolve(testDir, 'new-debug.json');
      
      const markdownContent = `# New Publication Debug Test

This is a new publication that should create a debug JSON file.

## Features Tested
- Debug flag processing
- JSON file creation
- Telegraph node generation

## Expected Results
- JSON file created at same location as markdown file
- Valid Telegraph nodes structure
- Proper formatting with 2-space indentation`;

      writeFileSync(testFile, markdownContent);

      // Mock Telegraph API
      const mockPublishNodes = jest.spyOn(workflowManager['publisher'], 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/new-debug-test',
        path: '/new-debug-test'
      });

      const options = { 
        debug: true,
        noVerify: true
      };

      await workflowManager.publish(testFile, options);

      // Verify JSON file was created
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify JSON content
      const jsonContent = readFileSync(expectedJsonFile, 'utf-8');
      const telegraphNodes = JSON.parse(jsonContent);
      
      expect(Array.isArray(telegraphNodes)).toBe(true);
      expect(telegraphNodes.length).toBeGreaterThan(0);
      
      // Check content preservation
      const jsonString = JSON.stringify(telegraphNodes);
      expect(jsonString).toContain('New Publication Debug Test');
      expect(jsonString).toContain('Features Tested');
      expect(jsonString).toContain('Expected Results');

      mockPublishNodes.mockRestore();
    });

    it('should create JSON file for existing publication with --debug --force', async () => {
      const testFile = resolve(testDir, 'existing-debug-force.md');
      const expectedJsonFile = resolve(testDir, 'existing-debug-force.json');
      
      // Create file with existing metadata (simulating already published file)
      const existingMetadata: FileMetadata = {
        telegraphUrl: 'https://telegra.ph/existing-debug-force',
        editPath: '/edit/existing-debug-force-xyz',
        username: 'test-user',
        publishedAt: new Date('2024-01-01').toISOString(),
        originalFilename: 'existing-debug-force.md',
        title: 'Existing Debug Force Test',
        contentHash: 'original-content-hash'
      };

      const markdownWithMetadata = `---
telegraphUrl: ${existingMetadata.telegraphUrl}
editPath: ${existingMetadata.editPath}
username: ${existingMetadata.username}
publishedAt: ${existingMetadata.publishedAt}
originalFilename: ${existingMetadata.originalFilename}
title: ${existingMetadata.title}
contentHash: ${existingMetadata.contentHash}
---

# Existing Debug Force Test

This is an existing publication being tested with --debug --force flags.

## Bug Report Scenario
This test specifically addresses the bug report where:
- User runs: \`publish --debug --force\` 
- File is already published (has metadata)
- Expected: JSON file should be created
- Previous behavior: JSON file was not created

## Current Test
- File has publication metadata (existing publication)
- Using --debug flag (should enable dry-run)
- Using --force flag (should bypass link verification)
- Expected result: JSON file creation`;

      writeFileSync(testFile, markdownWithMetadata);

      // Mock Telegraph API calls
      const mockEditPage = jest.spyOn(workflowManager['publisher'], 'editPage');
      mockEditPage.mockResolvedValue({
        url: existingMetadata.telegraphUrl,
        path: existingMetadata.editPath
      });

      // Test the exact scenario from bug report: --debug --force
      const options = { 
        debug: true,
        force: true  // This should bypass link verification
      };

      await workflowManager.publish(testFile, options);

      // Verify JSON file was created (this was the bug - file was not created)
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify JSON content is valid and contains expected data
      const jsonContent = readFileSync(expectedJsonFile, 'utf-8');
      const telegraphNodes = JSON.parse(jsonContent);
      
      expect(Array.isArray(telegraphNodes)).toBe(true);
      expect(telegraphNodes.length).toBeGreaterThan(0);
      
      // Verify content from the markdown is in the Telegraph nodes
      const jsonString = JSON.stringify(telegraphNodes);
      expect(jsonString).toContain('Bug Report Scenario');
      expect(jsonString).toContain('Current Test');
      expect(jsonString).toContain('This is an existing publication');

      // Verify JSON formatting
      expect(jsonContent).toMatch(/^\[\s*\{/);
      expect(jsonContent).toContain('  '); // 2-space indentation

      mockEditPage.mockRestore();
    });

    it('should handle --debug --force --dry-run combination correctly', async () => {
      const testFile = resolve(testDir, 'triple-flag-test.md');
      const expectedJsonFile = resolve(testDir, 'triple-flag-test.json');
      
      // Create existing publication
      const markdownWithMetadata = `---
telegraphUrl: https://telegra.ph/triple-flag-test
editPath: /edit/triple-flag-test
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: triple-flag-test.md
title: Triple Flag Test
contentHash: triple-hash
---

# Triple Flag Test
Testing --debug --force --dry-run combination`;

      writeFileSync(testFile, markdownWithMetadata);

      const mockEditPage = jest.spyOn(workflowManager['publisher'], 'editPage');
      mockEditPage.mockResolvedValue({
        url: 'https://telegra.ph/triple-flag-test',
        path: '/triple-flag-test'
      });

      // Test with all three flags
      const options = { 
        debug: true,
        force: true,
        dryRun: true  // Explicitly set dry-run too
      };

      await workflowManager.publish(testFile, options);

      // Verify JSON file was created
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify the operation was indeed a dry-run
      expect(mockEditPage).not.toHaveBeenCalled(); // Should not make actual API calls in dry-run

      const jsonContent = JSON.parse(readFileSync(expectedJsonFile, 'utf-8'));
      expect(Array.isArray(jsonContent)).toBe(true);
      expect(JSON.stringify(jsonContent)).toContain('Testing --debug --force --dry-run');
    });

    it('should not create JSON when debug is false regardless of other flags', async () => {
      const testFile = resolve(testDir, 'no-debug-flag.md');
      const expectedJsonFile = resolve(testDir, 'no-debug-flag.json');
      
      writeFileSync(testFile, '# No Debug Flag Test\nThis should not create JSON file.');

      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: true,
        url: 'https://telegra.ph/no-debug',
        path: '/no-debug'
      });

      // Test without debug flag
      const options = { 
        force: true,
        dryRun: true,
        debug: false,  // Explicitly false
        noVerify: true
      };

      await workflowManager.publish(testFile, options);

      // Verify JSON file was NOT created
      expect(existsSync(expectedJsonFile)).toBe(false);

      mockPublisher.mockRestore();
    });
  });

  describe('error handling in debug scenarios', () => {
    it('should continue operation even if JSON file creation fails', async () => {
      const testFile = resolve(testDir, 'json-error-test.md');
      const expectedJsonFile = resolve(testDir, 'json-error-test.json');
      
      writeFileSync(testFile, '# JSON Error Test\nTesting error handling in JSON creation.');

      // Mock writeFileSync to fail for JSON files
      const originalWriteFileSync = require('fs').writeFileSync;
      const mockWriteFileSync = jest.fn((path: string, data: any, options?: any) => {
        if (path.endsWith('.json')) {
          throw new Error('Simulated JSON write error');
        }
        return originalWriteFileSync(path, data, options);
      });
      require('fs').writeFileSync = mockWriteFileSync;

      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockImplementation(async (filePath, username, options) => {
        // This should try to create JSON and fail, but still return success
        return {
          success: true,
          isNewPublication: true,
          url: '[DRY RUN] Would publish',
          path: '/test'
        };
      });

      const options = { 
        debug: true,
        noVerify: true
      };

      // Test should complete without throwing errors
      await workflowManager.publish(testFile, options);

      // Verify JSON file was not created due to error
      expect(existsSync(expectedJsonFile)).toBe(false);

      // Restore mocks
      require('fs').writeFileSync = originalWriteFileSync;
      mockPublisher.mockRestore();
    });
  });
});
```

`src/cli/EnhancedCommands.ts`

```ts
import { existsSync, readFileSync, writeFileSync } from "node:fs";
import { dirname, resolve } from "node:path";
import type { Command } from "commander";
import { PagesCacheManager } from "../cache/PagesCacheManager";
import { ConfigManager } from "../config/ConfigManager";
import { ContentProcessor } from "../content/ContentProcessor";
import { DependencyManager } from "../dependencies/DependencyManager";
import { BidirectionalLinkResolver } from "../links/BidirectionalLinkResolver";
import { LinkResolver } from "../links/LinkResolver";
import type { BrokenLink, FileScanResult } from "../links/types";
import { convertMarkdownToTelegraphNodes } from "../markdownConverter";
import { MetadataManager } from "../metadata/MetadataManager";
import { EnhancedTelegraphPublisher } from "../publisher/EnhancedTelegraphPublisher";
import { TelegraphPublisher } from "../telegraphPublisher";
import { type FileMetadata, PublicationStatus, type TelegraphLink } from "../types/metadata";
import { PathResolver } from "../utils/PathResolver";
import { PublicationWorkflowManager } from "../workflow";
import { ProgressIndicator } from "./ProgressIndicator";

/**
 * Enhanced CLI commands with metadata management
 */
export class EnhancedCommands {

  /**
   * Add unified publish command (combines pub and edit functionality)
   * @param program Commander program instance
   */
  static addPublishCommand(program: Command): void {
    program
      .command("publish")
      .alias("pub")
      .description("Unified publish/edit command: creates, publishes, or updates Markdown files (if no file specified, publishes entire directory)")
      .option("-f, --file <path>", "Path to the Markdown file (optional - if not specified, publishes current directory)")
      .option("-a, --author <name>", "Author's name (overrides config default)")
      .option("--title <title>", "Title of the article (optional, will be extracted from file if not provided)")
      .option("--author-url <url>", "Author's URL (optional)")
      .option("--with-dependencies", "Automatically publish linked local files (default: true)")
      .option("--no-with-dependencies", "Skip automatic dependency publishing")
      .option("--force-republish", "Force republish even if file is already published")
      .option("--dry-run", "Preview operations without making changes")
      .option("--debug", "Save the generated Telegraph JSON to a file (implies --dry-run)")
      .option("--no-verify", "Skip mandatory local link verification before publishing")
      .option("--no-auto-repair", "Disable automatic link repair (publication will fail if broken links are found)")
      .option("--aside", "Automatically generate a Table of Contents (aside block) at the start of the article (default: true)")
      .option("--no-aside", "Disable automatic generation of the Table of Contents")
      .option("--force", "Bypass link verification and publish anyway (for debugging)")
      .option("--token <token>", "Access token (optional, will try to load from config)")
      .option("-v, --verbose", "Show detailed progress information")
      .action(async (options) => {
        try {
          await EnhancedCommands.handleUnifiedPublishCommand(options);
        } catch (error) {
          ProgressIndicator.showStatus(
            `Operation failed: ${error instanceof Error ? error.message : String(error)}`,
            "error"
          );
          process.exit(1);
        }
      });
  }

  /**
   * Add dependency analysis command
   * @param program Commander program instance
   */
  static addAnalyzeCommand(program: Command): void {
    program
      .command("analyze")
      .description("Analyze file dependencies and publication status")
      .option("-f, --file <path>", "Path to the Markdown file to analyze")
      .option("--depth <number>", "Maximum dependency depth to analyze", "1")
      .option("--show-tree", "Show dependency tree visualization")
      .action(async (options) => {
        try {
          await EnhancedCommands.handleAnalyzeCommand(options);
        } catch (error) {
          ProgressIndicator.showStatus(
            `Analysis failed: ${error instanceof Error ? error.message : String(error)}`,
            "error"
          );
          process.exit(1);
        }
      });
  }

  /**
   * Add configuration management command
   * @param program Commander program instance
   */
  static addConfigCommand(program: Command): void {
    program
      .command("config")
      .description("Manage configuration settings")
      .option("--show", "Show current configuration")
      .option("--set <key=value>", "Set configuration value", [])
      .option("--reset", "Reset configuration to defaults")
      .option("--username <name>", "Set default username")
      .option("--max-depth <number>", "Set maximum dependency depth")
      .option("--auto-deps <boolean>", "Enable/disable automatic dependency publishing")
      .action(async (options) => {
        try {
          await EnhancedCommands.handleConfigCommand(options);
        } catch (error) {
          ProgressIndicator.showStatus(
            `Configuration failed: ${error instanceof Error ? error.message : String(error)}`,
            "error"
          );
          process.exit(1);
        }
      });
  }

  /**
   * Add status command
   * @param program Commander program instance
   */
  static addStatusCommand(program: Command): void {
    program
      .command("status")
      .description("Show publication status of files")
      .option("-f, --file <path>", "Check specific file")
      .option("-d, --directory <path>", "Check all markdown files in directory")
      .option("--recursive", "Check subdirectories recursively")
      .action(async (options) => {
        try {
          await EnhancedCommands.handleStatusCommand(options);
        } catch (error) {
          ProgressIndicator.showStatus(
            `Status check failed: ${error instanceof Error ? error.message : String(error)}`,
            "error"
          );
          process.exit(1);
        }
      });
  }

  /**
   * Add reset command
   * @param program Commander program instance
   */
  static addResetCommand(program: Command): void {
    program
      .command("reset")
      .alias("r")
      .description("Reset publication metadata, preserving only title")
      .option("-f, --file <path>", "Path to specific file (optional - processes directory if not specified)")
      .option("--dry-run", "Preview changes without modification")
      .option("-v, --verbose", "Detailed progress information")
      .option("--force", "Reset files even without publication metadata")
      .action(async (options) => {
        try {
          await EnhancedCommands.handleResetCommand(options);
        } catch (error) {
          ProgressIndicator.showStatus(
            `Reset failed: ${error instanceof Error ? error.message : String(error)}`,
            "error"
          );
          process.exit(1);
        }
      });
  }

  /**
   * Add check-links command
   * @param program Commander program instance
   */
  static addCheckLinksCommand(program: Command): void {
    program
      .command("check-links")
      .alias("cl")
      .description("Verify and repair local Markdown links")
      .argument("[path]", "Path to file or directory (default: current directory)")
      .option("--apply-fixes", "Enable interactive repair mode")
      .option("--dry-run", "Report only, no changes (default)")
      .option("-v, --verbose", "Show detailed progress information")
      .option("-o, --output <file>", "Save report to file")
      .action(async (path, options) => {
        try {
          await EnhancedCommands.handleCheckLinksCommand(path, options);
        } catch (error) {
          ProgressIndicator.showStatus(
            `Link check failed: ${error instanceof Error ? error.message : String(error)}`,
            "error"
          );
          process.exit(1);
        }
      });
  }

  /**
   * Handle unified publish command (combines pub and edit functionality)
   * @param options Command options
   */
  static async handleUnifiedPublishCommand(options: any): Promise<void> {
    const targetPath = options.file || process.cwd();
    const fileDirectory = options.file ? dirname(resolve(options.file)) : process.cwd();

    // If a specific file is targeted and it doesn't exist, create it (edit functionality)
    if (options.file && !existsSync(resolve(options.file))) {
      ProgressIndicator.showStatus("File not found. Creating new file...", "info");
      const initialContent = options.title
        ? `# ${options.title}\n\nContent goes here...`
        : `# New Article\n\nContent goes here...`;
      writeFileSync(resolve(options.file), initialContent);
      ProgressIndicator.showStatus(`Created new file: ${resolve(options.file)}`, "success");
    }

    const config = ConfigManager.getMetadataConfig(fileDirectory);
    const accessToken = options.token || ConfigManager.loadAccessToken(fileDirectory);

    if (!accessToken) {
      throw new Error("Access token is required. Set it using --token or configure it with 'config' command");
    }

    const workflowManager = new PublicationWorkflowManager(config, accessToken);

    try {
      await workflowManager.publish(targetPath, options);
    } catch (error) {
      ProgressIndicator.showStatus(
        `Publication workflow failed: ${error instanceof Error ? error.message : String(error)}`,
        "error"
      );
      process.exit(1);
    }
  }

  /**
   * Handle dependency analysis command
   * @param options Command options
   */
  private static async handleAnalyzeCommand(options: any): Promise<void> {
    if (!options.file) {
      throw new Error("File path must be specified using --file");
    }

    const filePath = resolve(options.file);
    if (!existsSync(filePath)) {
      throw new Error(`File not found: ${filePath}`);
    }

    const config = ConfigManager.getMetadataConfig(dirname(filePath));
    config.maxDependencyDepth = parseInt(options.depth) || config.maxDependencyDepth;

    const pathResolver = PathResolver.getInstance();
    const dependencyManager = new DependencyManager(config, pathResolver);

    ProgressIndicator.showStatus("Analyzing dependencies...", "info");

    const dependencyTree = dependencyManager.buildDependencyTree(filePath);
    const analysis = dependencyManager.analyzeDependencyTree(dependencyTree);

    console.log("\nüìä Dependency Analysis Results:");
    console.log("================================");
    console.log(`üìÅ Total files: ${analysis.totalFiles}`);
    console.log(`‚úÖ Published files: ${analysis.publishedFiles}`);
    console.log(`üìù Unpublished files: ${analysis.unpublishedFiles}`);
    console.log(`üìè Maximum depth: ${analysis.maxDepth}`);

    if (analysis.circularDependencies.length > 0) {
      ProgressIndicator.showStatus("Circular dependencies detected!", "warning");
      analysis.circularDependencies.forEach((cycle, index) => {
        console.log(`  ${index + 1}. ${cycle.join(" ‚Üí ")}`);
      });
    }

    if (analysis.unpublishedFiles > 0) {
      const filesToPublish = dependencyManager.getFilesToPublish(dependencyTree);
      ProgressIndicator.showList("Files that need publishing", filesToPublish);

      if (analysis.publishOrder.length > 0) {
        ProgressIndicator.showList("Recommended publishing order", analysis.publishOrder, true);
      }
    }

    if (options.showTree) {
      console.log("\nüå≥ Dependency Tree:");
      EnhancedCommands.printDependencyTree(dependencyTree, "", true);
    }
  }

  /**
   * Handle configuration command
   * @param options Command options
   */
  private static async handleConfigCommand(options: any): Promise<void> {
    const directory = process.cwd();

    if (options.show) {
      ConfigManager.displayConfig(directory);
      return;
    }

    if (options.reset) {
      ConfigManager.resetConfig(directory);
      return;
    }

    // Handle individual settings
    const updates: any = {};

    if (options.username) {
      updates.defaultUsername = options.username;
    }

    if (options.maxDepth) {
      const depth = parseInt(options.maxDepth);
      if (isNaN(depth) || depth < 1 || depth > 20) {
        throw new Error("Max depth must be a number between 1 and 20");
      }
      updates.maxDependencyDepth = depth;
    }

    if (options.autoDeps !== undefined) {
      updates.autoPublishDependencies = options.autoDeps === "true";
    }

    if (Object.keys(updates).length > 0) {
      ConfigManager.updateMetadataConfig(directory, updates);
      ProgressIndicator.showStatus("Configuration updated successfully", "success");
    } else {
      ConfigManager.displayConfig(directory);
    }
  }

  /**
   * Handle status command
   * @param options Command options
   */
  private static async handleStatusCommand(options: any): Promise<void> {
    if (options.file) {
      const filePath = resolve(options.file);
      if (!existsSync(filePath)) {
        throw new Error(`File not found: ${filePath}`);
      }

      const status = MetadataManager.getPublicationStatus(filePath);
      const metadata = MetadataManager.getPublicationInfo(filePath);

      console.log(`\nüìÑ File: ${filePath}`);
      console.log(`üìä Status: ${status}`);

      if (metadata) {
        console.log(`üîó URL: ${metadata.telegraphUrl}`);
        console.log(`üë§ Author: ${metadata.username}`);
        console.log(`üìÖ Published: ${metadata.publishedAt}`);
      }
    } else {
      ProgressIndicator.showStatus("Directory status checking not implemented yet", "warning");
    }
  }

  /**
   * Print dependency tree visualization
   * @param node Dependency node
   * @param prefix Tree prefix
   * @param isLast Whether this is the last node
   */
  private static printDependencyTree(node: any, prefix: string = "", isLast: boolean = true): void {
    const status = node.status === PublicationStatus.PUBLISHED ? "‚úÖ" : "üìù";
    const connector = isLast ? "‚îî‚îÄ‚îÄ " : "‚îú‚îÄ‚îÄ ";
    console.log(`${prefix}${connector}${status} ${node.filePath}`);

    if (node.dependencies && node.dependencies.length > 0) {
      const newPrefix = prefix + (isLast ? "    " : "‚îÇ   ");
      node.dependencies.forEach((dep: any, index: number) => {
        const isLastDep = index === node.dependencies.length - 1;
        EnhancedCommands.printDependencyTree(dep, newPrefix, isLastDep);
      });
    }
  }



  /**
   * Handle directory publishing - publish all markdown files in current directory and subdirectories
   * @param options Command line options
   */
  static async handleDirectoryPublish(options: any): Promise<void> {
    // This method is now largely replaced by PublicationWorkflowManager.publish
    // It will simply call the unified publish method with the current directory as target
    await EnhancedCommands.handleUnifiedPublishCommand(options);
  }

  /**
   * Find all markdown files in directory and subdirectories
   * @param dir Directory to search
   * @returns Array of markdown file paths
   */
  static async findMarkdownFiles(dir: string): Promise<string[]> {
    const { readdirSync, statSync } = await import('fs');
    const { join } = await import('path');

    const files: string[] = [];

    try {
      const entries = readdirSync(dir);

      for (const entry of entries) {
        const fullPath = join(dir, entry);

        try {
          const stat = statSync(fullPath);

          if (stat.isDirectory()) {
            // Skip node_modules, .git, and other common directories
            if (!entry.startsWith('.') && entry !== 'node_modules' && entry !== 'dist') {
              const subFiles = await EnhancedCommands.findMarkdownFiles(fullPath);
              files.push(...subFiles);
            }
          } else if (stat.isFile() && entry.toLowerCase().endsWith('.md')) {
            files.push(fullPath);
          }
        } catch (error) {
          // Skip files/directories that can't be accessed
          console.warn(`Warning: Could not access ${fullPath}`);
        }
      }
    } catch (error) {
      console.warn(`Warning: Could not read directory ${dir}`);
    }

    return files.sort();
  }

  /**
   * Handle reset command
   * @param options Command options
   */
  static async handleResetCommand(options: any): Promise<void> {
    const { readFileSync, writeFileSync } = await import('fs');
    const { resolve } = await import('path');

    // If no file specified, process current directory
    if (!options.file) {
      await EnhancedCommands.handleDirectoryReset(options);
      return;
    }

    const filePath = resolve(options.file as string);

    if (options.verbose) {
      ProgressIndicator.showStatus(`Processing file: ${filePath}`, "info");
    }

    try {
      // Read file content
      const content = readFileSync(filePath, 'utf-8');

      // Check if file has metadata to reset
      const hasMetadata = content.trim().startsWith('---');
      if (!hasMetadata && !options.force) {
        ProgressIndicator.showStatus(`Skipped ${filePath}: No front-matter found`, "info");
        return;
      }

      // Perform reset
      const resetContent = MetadataManager.resetMetadata(content, filePath);

      if (options.dryRun) {
        ProgressIndicator.showStatus("üîç Dry-run mode: Preview of changes", "info");
        console.log(`\nüìÑ File: ${filePath}`);
        console.log("üìù Before:");
        console.log(content.split('\n').slice(0, 10).join('\n') + (content.split('\n').length > 10 ? '\n...' : ''));
        console.log("\n‚ú® After:");
        console.log(resetContent.split('\n').slice(0, 10).join('\n') + (resetContent.split('\n').length > 10 ? '\n...' : ''));
        console.log("");
        return;
      }

      // Write the reset content back to file
      writeFileSync(filePath, resetContent, 'utf-8');

      ProgressIndicator.showStatus(`‚úÖ Reset completed: ${filePath}`, "success");

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      ProgressIndicator.showStatus(`‚ùå Error processing ${filePath}: ${errorMessage}`, "error");
      throw error;
    }
  }

  /**
   * Handle directory reset
   * @param options Command options
   */
  static async handleDirectoryReset(options: any): Promise<void> {
    const currentDir = process.cwd();

    if (options.verbose) {
      ProgressIndicator.showStatus(`Scanning directory: ${currentDir}`, "info");
    }

    // Find all markdown files
    const markdownFiles = await EnhancedCommands.findMarkdownFiles(currentDir);

    if (markdownFiles.length === 0) {
      ProgressIndicator.showStatus("No markdown files found in current directory", "info");
      return;
    }

    if (options.dryRun) {
      ProgressIndicator.showStatus("üîç Dry-run mode: Found files that would be processed:", "info");
      markdownFiles.forEach(file => console.log(`  üìÑ ${file}`));
      console.log(`\nTotal files: ${markdownFiles.length}`);
      return;
    }

    ProgressIndicator.showStatus(`Found ${markdownFiles.length} markdown files. Processing...`, "info");

    const spinner = ProgressIndicator.createSpinner("Resetting metadata");
    spinner.start();

    let successCount = 0;
    let skipCount = 0;
    let errorCount = 0;
    const errors: Array<{ file: string, error: string }> = [];

    try {
      for (let i = 0; i < markdownFiles.length; i++) {
        const file = markdownFiles[i];
        if (!file) continue;

        try {
          const { readFileSync, writeFileSync } = await import('fs');
          const content = readFileSync(file, 'utf-8');

          // Check if file has metadata to reset
          const hasMetadata = content.trim().startsWith('---');
          if (!hasMetadata && !options.force) {
            skipCount++;
            if (options.verbose) {
              spinner.stop();
              ProgressIndicator.showStatus(`Skipped ${file}: No front-matter`, "info");
              spinner.start();
            }
            continue;
          }

          // Perform reset
          const resetContent = MetadataManager.resetMetadata(content, file);
          writeFileSync(file, resetContent, 'utf-8');

          successCount++;

          if (options.verbose) {
            spinner.stop();
            ProgressIndicator.showStatus(`‚úÖ Reset: ${file}`, "success");
            spinner.start();
          }

        } catch (error) {
          errorCount++;
          const errorMessage = error instanceof Error ? error.message : String(error);
          errors.push({ file, error: errorMessage });

          if (options.verbose) {
            spinner.stop();
            ProgressIndicator.showStatus(`‚ùå Error: ${file} - ${errorMessage}`, "error");
            spinner.start();
          }
        }
      }
    } finally {
      spinner.stop();
    }

    // Display summary
    ProgressIndicator.showStatus(
      `\n‚úÖ Reset Operation Complete\n` +
      `‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n` +
      `üìä Files processed: ${markdownFiles.length}\n` +
      `‚úÖ Successfully reset: ${successCount}\n` +
      `‚ö†Ô∏è  Skipped: ${skipCount}\n` +
      `‚ùå Errors: ${errorCount}`,
      "success"
    );

    if (errors.length > 0) {
      console.log("\n‚ùå Failed files:");
      errors.forEach(({ file, error }) => {
        console.log(`  ‚Ä¢ ${file}: ${error}`);
      });
    }
  }

  /**
   * Handle check-links command
   * @param path Path to scan (optional)
   * @param options Command options
   */
  static async handleCheckLinksCommand(path: string | undefined, options: any): Promise<void> {
    const { LinkScanner, LinkVerifier, LinkResolver, ReportGenerator, InteractiveRepairer } = await import('../links');

    const targetPath = path || process.cwd();
    const verbose = options.verbose || false;
    const applyFixes = options.applyFixes || false;
    const outputFile = options.output;

    const reportGenerator = new ReportGenerator(verbose);
    const scanner = new LinkScanner();
    const verifier = new LinkVerifier(PathResolver.getInstance());
    const resolver = new LinkResolver();

    const startTime = Date.now();

    try {
      // Show initial progress
      if (verbose) {
        reportGenerator.showInfo(`üîé Starting scan: ${targetPath}`);
      }

      // Find all markdown files
      const markdownFiles = await scanner.findMarkdownFiles(targetPath);

      if (markdownFiles.length === 0) {
        reportGenerator.showInfo('No markdown files found to scan.');
        return;
      }

      // Scan all files for links
      const fileResults: any[] = [];
      let totalLinks = 0;
      let totalLocalLinks = 0;

      for (let i = 0; i < markdownFiles.length; i++) {
        const filePath = markdownFiles[i];
        if (!filePath) continue;

        if (verbose) {
          reportGenerator.showVerboseProgress(filePath, i + 1, markdownFiles.length);
        }

        const scanResult = await scanner.scanFile(filePath);
        const verifiedResult = await verifier.verifyLinks(scanResult);

        fileResults.push(verifiedResult);
        totalLinks += verifiedResult.allLinks.length;
        totalLocalLinks += verifiedResult.localLinks.length;

        if (verbose) {
          reportGenerator.showVerboseFileDetails(verifiedResult);
        }
      }

      // Resolve suggestions for broken links
      const resolvedResults = await resolver.resolveBrokenLinks(fileResults);

      // Create complete scan result
      const allBrokenLinks: any[] = [];
      for (const result of resolvedResults) {
        allBrokenLinks.push(...result.brokenLinks);
      }

      const scanResult = {
        totalFiles: markdownFiles.length,
        totalLinks,
        totalLocalLinks,
        brokenLinks: allBrokenLinks,
        fileResults: resolvedResults,
        processingTime: Date.now() - startTime
      };

      // Generate report
      reportGenerator.generateReport(scanResult, outputFile);

      // Interactive repair mode
      if (applyFixes && allBrokenLinks.length > 0) {
        const repairer = new InteractiveRepairer(reportGenerator);
        await repairer.performInteractiveRepair(allBrokenLinks);
      }

    } catch (error) {
      reportGenerator.showError(error instanceof Error ? error.message : String(error));
      throw error;
    }
  }
}
```

`src/cli/ProgressIndicator.ts`

```ts
/**
 * Progress indicator utility for CLI operations
 */
export class ProgressIndicator {
  private current: number = 0;
  private total: number = 0;
  private startTime: number = 0;
  private lastUpdate: number = 0;
  private label: string = "";

  constructor(total: number, label: string = "Progress") {
    this.total = total;
    this.label = label;
    this.startTime = Date.now();
    this.lastUpdate = this.startTime;
  }

  /**
   * Update progress
   * @param current Current progress value
   * @param message Optional status message
   */
  update(current: number, message?: string): void {
    this.current = current;
    this.lastUpdate = Date.now();
    this.render(message);
  }

  /**
   * Increment progress by 1
   * @param message Optional status message
   */
  increment(message?: string): void {
    this.update(this.current + 1, message);
  }

  /**
   * Complete the progress
   * @param message Final message
   */
  complete(message?: string): void {
    this.current = this.total;
    this.render(message);
    console.log(); // New line after completion
  }

  /**
   * Fail the progress with error
   * @param error Error message
   */
  fail(error: string): void {
    console.log(); // New line
    console.error(`‚ùå ${this.label} failed: ${error}`);
  }

  /**
   * Render progress bar
   * @param message Optional status message
   */
  private render(message?: string): void {
    const percentage = Math.round((this.current / this.total) * 100);
    const elapsed = Date.now() - this.startTime;
    const eta = this.current > 0 ? (elapsed / this.current) * (this.total - this.current) : 0;

    // Create progress bar
    const barLength = 30;
    const filled = Math.round((this.current / this.total) * barLength);
    const bar = "‚ñà".repeat(filled) + "‚ñë".repeat(barLength - filled);

    // Format time
    const formatTime = (ms: number): string => {
      const seconds = Math.round(ms / 1000);
      const minutes = Math.floor(seconds / 60);
      const remainingSeconds = seconds % 60;
      return minutes > 0 ? `${minutes}m ${remainingSeconds}s` : `${remainingSeconds}s`;
    };

    let output = `\r${this.label}: [${bar}] ${percentage}% (${this.current}/${this.total})`;

    if (elapsed > 1000) {
      output += ` | Elapsed: ${formatTime(elapsed)}`;
    }

    if (eta > 1000 && this.current < this.total) {
      output += ` | ETA: ${formatTime(eta)}`;
    }

    if (message) {
      output += ` | ${message}`;
    }

    // Clear line and write progress
    process.stdout.write(output.padEnd(120));
  }

  /**
   * Create a simple spinner for indeterminate progress
   */
  static createSpinner(message: string = "Processing"): {
    start: () => void;
    stop: (finalMessage?: string) => void;
    update: (newMessage: string) => void;
  } {
    const frames = ["‚†ã", "‚†ô", "‚†π", "‚†∏", "‚†º", "‚†¥", "‚†¶", "‚†ß", "‚†á", "‚†è"];
    let frameIndex = 0;
    let interval: NodeJS.Timeout | null = null;
    let currentMessage = message;

    return {
      start: () => {
        interval = setInterval(() => {
          process.stdout.write(`\r${frames[frameIndex]} ${currentMessage}`);
          frameIndex = (frameIndex + 1) % frames.length;
        }, 100);
      },

      stop: (finalMessage?: string) => {
        if (interval) {
          clearInterval(interval);
          interval = null;
        }
        process.stdout.write(`\r${finalMessage || "‚úÖ Done"}`.padEnd(80) + "\n");
      },

      update: (newMessage: string) => {
        currentMessage = newMessage;
      }
    };
  }

  /**
   * Create a progress bar for batch operations
   * @param items Items to process
   * @param processor Function to process each item
   * @param label Progress label
   */
  static async processBatch<T, R>(
    items: T[],
    processor: (item: T, index: number) => Promise<R>,
    label: string = "Processing"
  ): Promise<R[]> {
    const progress = new ProgressIndicator(items.length, label);
    const results: R[] = [];

    for (let i = 0; i < items.length; i++) {
      try {
        const item = items[i];
        if (item === undefined) {
          throw new Error(`Item at index ${i} is undefined`);
        }
        const result = await processor(item, i);
        results.push(result);
        progress.increment(`Processed item ${i + 1}`);
      } catch (error) {
        progress.fail(`Failed to process item ${i + 1}: ${error instanceof Error ? error.message : String(error)}`);
        throw error;
      }
    }

    progress.complete(`‚úÖ Processed ${items.length} items successfully`);
    return results;
  }

  /**
   * Show a simple status message
   * @param message Message to display
   * @param type Message type
   */
  static showStatus(message: string, type: "info" | "success" | "warning" | "error" = "info"): void {
    const icons = {
      info: "‚ÑπÔ∏è",
      success: "‚úÖ",
      warning: "‚ö†Ô∏è",
      error: "‚ùå"
    };

    console.log(`${icons[type]} ${message}`);
  }

  /**
   * Show a formatted list
   * @param title List title
   * @param items List items
   * @param numbered Whether to show numbers
   */
  static showList(title: string, items: string[], numbered: boolean = false): void {
    console.log(`\nüìã ${title}:`);
    console.log("=" + "=".repeat(title.length + 2));

    if (items.length === 0) {
      console.log("  (No items)");
    } else {
      items.forEach((item, index) => {
        const prefix = numbered ? `${index + 1}. ` : "‚Ä¢ ";
        console.log(`  ${prefix}${item}`);
      });
    }

    console.log();
  }
}
```

`src/config/ConfigManager.ts`

```ts
import { existsSync, readFileSync, writeFileSync } from "node:fs";
import { join } from "node:path";
import type { MetadataConfig, RateLimitConfig } from "../types/metadata";

/**
 * Extended configuration interface including legacy fields
 */
interface ExtendedConfig extends MetadataConfig {
  accessToken?: string;
  version?: string;
}

/**
 * Configuration manager for enhanced Telegraph publisher
 */
export class ConfigManager {
  private static readonly CONFIG_FILE_NAME = ".telegraph-publisher-config.json";
  private static readonly LEGACY_CONFIG_FILE_NAME = ".telegraph-publisher-config.json";

  /**
   * Default rate limiting configuration
   */
  private static readonly DEFAULT_RATE_LIMIT_CONFIG: RateLimitConfig = {
    baseDelayMs: 1500,
    adaptiveMultiplier: 2.0,
    maxDelayMs: 30000,
    backoffStrategy: 'linear',
    maxRetries: 3,
    cooldownPeriodMs: 60000,
    enableAdaptiveThrottling: true
  };

  /**
   * Default configuration values
   */
  private static readonly DEFAULT_CONFIG: MetadataConfig = {
    defaultUsername: undefined,
    autoPublishDependencies: true,
    replaceLinksinContent: true,
    maxDependencyDepth: 1,
    createBackups: false,
    manageBidirectionalLinks: true,
    autoSyncCache: true,
    rateLimiting: ConfigManager.DEFAULT_RATE_LIMIT_CONFIG,
    customFields: {}
  };

  /**
   * Load configuration from file
   * @param directory Directory to look for config file
   * @returns Configuration object
   */
  static loadConfig(directory: string): ExtendedConfig {
    const configPath = join(directory, ConfigManager.CONFIG_FILE_NAME);

    if (existsSync(configPath)) {
      try {
        const config = JSON.parse(readFileSync(configPath, "utf-8"));
        return { ...ConfigManager.DEFAULT_CONFIG, ...config };
      } catch (error) {
        console.warn(`‚ö†Ô∏è Error loading config from ${configPath}:`, error);
        return { ...ConfigManager.DEFAULT_CONFIG };
      }
    }

    return { ...ConfigManager.DEFAULT_CONFIG };
  }

  /**
   * Save configuration to file
   * @param directory Directory to save config file
   * @param config Configuration to save
   */
  static saveConfig(directory: string, config: Partial<ExtendedConfig>): void {
    const configPath = join(directory, ConfigManager.CONFIG_FILE_NAME);

    try {
      const existingConfig = ConfigManager.loadConfig(directory);
      const mergedConfig = { ...existingConfig, ...config };

      writeFileSync(configPath, JSON.stringify(mergedConfig, null, 2), "utf-8");
      console.log(`‚úÖ Configuration saved to ${configPath}`);
    } catch (error) {
      console.error(
        `‚ùå Error saving configuration to ${configPath}:`,
        error instanceof Error ? error.message : String(error)
      );
    }
  }

  /**
   * Load access token (backward compatibility)
   * @param directory Directory to look for config file
   * @returns Access token if found
   */
  static loadAccessToken(directory: string): string | undefined {
    const config = ConfigManager.loadConfig(directory);
    return config.accessToken;
  }

  /**
   * Save access token (backward compatibility)
   * @param directory Directory to save config file
   * @param accessToken Access token to save
   */
  static saveAccessToken(directory: string, accessToken: string): void {
    ConfigManager.saveConfig(directory, { accessToken });
  }

  /**
   * Get metadata configuration
   * @param directory Directory to load config from
   * @returns Metadata configuration
   */
  static getMetadataConfig(directory: string): MetadataConfig {
    const config = ConfigManager.loadConfig(directory);

    return {
      defaultUsername: config.defaultUsername,
      autoPublishDependencies: config.autoPublishDependencies ?? ConfigManager.DEFAULT_CONFIG.autoPublishDependencies,
      replaceLinksinContent: config.replaceLinksinContent ?? ConfigManager.DEFAULT_CONFIG.replaceLinksinContent,
      maxDependencyDepth: config.maxDependencyDepth ?? ConfigManager.DEFAULT_CONFIG.maxDependencyDepth,
      createBackups: config.createBackups ?? ConfigManager.DEFAULT_CONFIG.createBackups,
      manageBidirectionalLinks: config.manageBidirectionalLinks ?? ConfigManager.DEFAULT_CONFIG.manageBidirectionalLinks,
      autoSyncCache: config.autoSyncCache ?? ConfigManager.DEFAULT_CONFIG.autoSyncCache,
      rateLimiting: config.rateLimiting ?? ConfigManager.DEFAULT_CONFIG.rateLimiting,
      customFields: config.customFields ?? ConfigManager.DEFAULT_CONFIG.customFields
    };
  }

  /**
   * Update metadata configuration
   * @param directory Directory to save config to
   * @param metadataConfig Metadata configuration to update
   */
  static updateMetadataConfig(directory: string, metadataConfig: Partial<MetadataConfig>): void {
    ConfigManager.saveConfig(directory, metadataConfig);
  }

  /**
   * Display current configuration
   * @param directory Directory to load config from
   */
  static displayConfig(directory: string): void {
    const config = ConfigManager.loadConfig(directory);

    console.log("\nüìã Current Configuration:");
    console.log("========================");

    if (config.accessToken) {
      const maskedToken = config.accessToken.substring(0, 8) + "..." + config.accessToken.slice(-4);
      console.log(`üîë Access Token: ${maskedToken}`);
    } else {
      console.log("üîë Access Token: Not set");
    }

    console.log(`üë§ Default Username: ${config.defaultUsername || "Not set"}`);
    console.log(`üîó Auto-publish Dependencies: ${config.autoPublishDependencies ? "Yes" : "No"}`);
    console.log(`üîÑ Replace Links in Content: ${config.replaceLinksinContent ? "Yes" : "No"}`);
    console.log(`üìä Max Dependency Depth: ${config.maxDependencyDepth}`);
    console.log(`üíæ Create Backups: ${config.createBackups ? "Yes" : "No"}`);
    console.log(`üîó Manage Bidirectional Links: ${config.manageBidirectionalLinks ? "Yes" : "No"}`);
    console.log(`üîÑ Auto-sync Cache: ${config.autoSyncCache ? "Yes" : "No"}`);

    if (config.customFields && Object.keys(config.customFields).length > 0) {
      console.log("üéõÔ∏è Custom Fields:");
      for (const [key, value] of Object.entries(config.customFields)) {
        console.log(`   ${key}: ${value}`);
      }
    }

    console.log("========================\n");
  }

  /**
   * Reset configuration to defaults
   * @param directory Directory to save config to
   * @param keepAccessToken Whether to preserve access token
   */
  static resetConfig(directory: string, keepAccessToken: boolean = true): void {
    const config: Partial<ExtendedConfig> = { ...ConfigManager.DEFAULT_CONFIG };

    if (keepAccessToken) {
      const existingConfig = ConfigManager.loadConfig(directory);
      if (existingConfig.accessToken) {
        config.accessToken = existingConfig.accessToken;
      }
    }

    ConfigManager.saveConfig(directory, config);
    console.log("‚úÖ Configuration reset to defaults");
  }

  /**
   * Validate configuration
   * @param config Configuration to validate
   * @returns Validation result
   */
  static validateConfig(config: MetadataConfig): { isValid: boolean; errors: string[] } {
    const errors: string[] = [];

    if (config.maxDependencyDepth < 1 || config.maxDependencyDepth > 20) {
      errors.push("Max dependency depth must be between 1 and 20");
    }

    if (config.defaultUsername && config.defaultUsername.trim().length === 0) {
      errors.push("Default username cannot be empty string");
    }

    return {
      isValid: errors.length === 0,
      errors
    };
  }
}
```

`src/content/ContentProcessor.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, it } from "bun:test";
import { join } from "node:path";
import { TestHelpers } from "../test-utils/TestHelpers";
import { ContentProcessor } from "./ContentProcessor";

describe("ContentProcessor", () => {
  let tempDir: string;

  beforeEach(() => {
    tempDir = TestHelpers.createTempDir("content-processor-test");
  });

  afterEach(() => {
    TestHelpers.cleanup();
  });

  describe("processFile", () => {
    it("should process file with metadata", () => {
      const testFile = join(tempDir, "test.md");
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";

      TestHelpers.createTestFile(testFile, content, metadata);

      const result = ContentProcessor.processFile(testFile);

      expect(result.originalContent).toContain("---");
      expect(result.originalContent).toContain(content);
      expect(result.contentWithoutMetadata).toBe(content);
      expect(result.metadata).toEqual(metadata);
      expect(result.localLinks).toHaveLength(0);
      expect(result.hasChanges).toBe(false);
    });

    it("should process file without metadata", () => {
      const testFile = join(tempDir, "test.md");
      const content = "# Test Article\n\nThis is test content.";

      TestHelpers.createTestFile(testFile, content);

      const result = ContentProcessor.processFile(testFile);

      expect(result.originalContent).toBe(content);
      expect(result.contentWithoutMetadata).toBe(content);
      expect(result.metadata).toBeUndefined();
      expect(result.localLinks).toHaveLength(0);
      expect(result.hasChanges).toBe(false);
    });

    it("should process file with local links", () => {
      const testFile = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "linked.md");

      TestHelpers.createTestFile(testFile, TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md"]
      ));
      TestHelpers.createTestFile(linkedFile, "# Linked Article");

      const result = ContentProcessor.processFile(testFile);

      expect(result.localLinks).toHaveLength(1);
      expect(result.localLinks[0]?.originalPath).toBe("./linked.md");
      expect(result.localLinks[0]?.resolvedPath).toBe(linkedFile);
      expect(result.hasChanges).toBe(true);
    });

    it("should throw error for non-existent file", () => {
      const nonExistentFile = join(tempDir, "non-existent.md");

      expect(() => {
        ContentProcessor.processFile(nonExistentFile);
      }).toThrow("Failed to read file");
    });
  });

  describe("processContent", () => {
    it("should process content with metadata", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      const contentWithMetadata = TestHelpers.createYamlFrontMatter(metadata) + "\n\n" + content;
      const basePath = join(tempDir, "test.md");

      const result = ContentProcessor.processContent(contentWithMetadata, basePath);

      expect(result.originalContent).toBe(contentWithMetadata);
      expect(result.contentWithoutMetadata).toBe(content);
      expect(result.metadata).toEqual(metadata);
    });

    it("should process content with local links", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md", "../other.md"]
      );
      const basePath = join(tempDir, "test.md");

      const result = ContentProcessor.processContent(content, basePath);

      expect(result.localLinks).toHaveLength(2);
      expect(result.localLinks[0]?.originalPath).toBe("./linked.md");
      expect(result.localLinks[1]?.originalPath).toBe("../other.md");
      expect(result.hasChanges).toBe(true);
    });

    it("should process content with external links", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", [], ["https://example.com", "mailto:test@example.com"]
      );
      const basePath = join(tempDir, "test.md");

      const result = ContentProcessor.processContent(content, basePath);

      expect(result.localLinks).toHaveLength(0);
      expect(result.hasChanges).toBe(false);
    });

    it("should handle empty content", () => {
      const content = "";
      const basePath = join(tempDir, "test.md");

      const result = ContentProcessor.processContent(content, basePath);

      expect(result.originalContent).toBe("");
      expect(result.contentWithoutMetadata).toBe("");
      expect(result.metadata).toBeUndefined();
      expect(result.localLinks).toHaveLength(0);
      expect(result.hasChanges).toBe(false);
    });
  });

  describe("replaceLinksInContent", () => {
    it("should replace local links with Telegraph URLs", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md", "./another.md"]
      );
      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "linked.md");
      const anotherFile = join(tempDir, "another.md");

      TestHelpers.createTestFile(linkedFile, "# Linked");
      TestHelpers.createTestFile(anotherFile, "# Another");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/Linked-01-01"],
        [anotherFile, "https://telegra.ph/Another-01-01"]
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/Linked-01-01");
      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/Another-01-01");
      expect(result.hasChanges).toBe(true);
      expect(result.localLinks[0]?.isPublished).toBe(true);
      expect(result.localLinks[0]?.telegraphUrl).toBe("https://telegra.ph/Linked-01-01");
    });

    it("should handle partial link replacements", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md", "./unpublished.md"]
      );
      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "linked.md");
      const unpublishedFile = join(tempDir, "unpublished.md");

      TestHelpers.createTestFile(linkedFile, "# Linked");
      TestHelpers.createTestFile(unpublishedFile, "# Unpublished");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/Linked-01-01"]
        // unpublishedFile is not in mappings
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/Linked-01-01");
      expect(result.contentWithReplacedLinks).toContain("./unpublished.md");
      expect(result.localLinks[0]?.isPublished).toBe(true);
      expect(result.localLinks[1]?.isPublished).toBe(false);
    });

    it("should handle empty link mappings", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md"]
      );
      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);
      const linkMappings = new Map();

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toBe(processedContent.contentWithoutMetadata);
      expect(result.hasChanges).toBe(false);
      expect(result.localLinks[0]?.isPublished).toBe(false);
    });

    it("should preserve anchors when replacing local links", () => {
      const content = `# Test Article

Here is a [link with anchor](./page.md#section-one) and another [link](./page.md#section-two).`;

      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "page.md");

      TestHelpers.createTestFile(linkedFile, "# Page Content\n\n## Section One\n\n## Section Two");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/page-one"]
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/page-one#section-one");
      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/page-one#section-two");
      expect(result.hasChanges).toBe(true);
      expect(result.localLinks[0]?.isPublished).toBe(true);
      expect(result.localLinks[0]?.telegraphUrl).toBe("https://telegra.ph/page-one#section-one");
      expect(result.localLinks[1]?.telegraphUrl).toBe("https://telegra.ph/page-one#section-two");
    });

    it("should handle mixed links with and without anchors", () => {
      const content = `# Test Article

Here is a [link with anchor](./page.md#section) and a [link without anchor](./page.md).`;

      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "page.md");

      TestHelpers.createTestFile(linkedFile, "# Page Content\n\n## Section");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/page-one"]
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/page-one#section");
      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/page-one)");
      expect(result.contentWithReplacedLinks).not.toContain("https://telegra.ph/page-one#)");
      expect(result.hasChanges).toBe(true);
      expect(result.localLinks[0]?.telegraphUrl).toBe("https://telegra.ph/page-one#section");
      expect(result.localLinks[1]?.telegraphUrl).toBe("https://telegra.ph/page-one");
    });

    it("should handle Cyrillic characters in anchors", () => {
      const content = `# –¢–µ—Å—Ç

–í–æ—Ç [—Å—Å—ã–ª–∫–∞ –Ω–∞ —Ä–∞–∑–¥–µ–ª](./—Å—Ç—Ä–∞–Ω–∏—Ü–∞.md#—Ä–∞–∑–¥–µ–ª-–æ–¥–∏–Ω) —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π.`;

      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "—Å—Ç—Ä–∞–Ω–∏—Ü–∞.md");

      TestHelpers.createTestFile(linkedFile, "# –°—Ç—Ä–∞–Ω–∏—Ü–∞\n\n## –†–∞–∑–¥–µ–ª –û–¥–∏–Ω");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/stranitsa"]
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/stranitsa#—Ä–∞–∑–¥–µ–ª-–æ–¥–∏–Ω");
      expect(result.hasChanges).toBe(true);
      expect(result.localLinks[0]?.telegraphUrl).toBe("https://telegra.ph/stranitsa#—Ä–∞–∑–¥–µ–ª-–æ–¥–∏–Ω");
    });

    it("should handle edge cases with anchors", () => {
      const content = `# Test Article

[Empty anchor](./page.md#) and [Multiple hashes](./page.md#section#subsection).`;

      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "page.md");

      TestHelpers.createTestFile(linkedFile, "# Page Content");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/page-one"]
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/page-one#");
      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/page-one#section#subsection");
      expect(result.hasChanges).toBe(true);
      expect(result.localLinks[0]?.telegraphUrl).toBe("https://telegra.ph/page-one#");
      expect(result.localLinks[1]?.telegraphUrl).toBe("https://telegra.ph/page-one#section#subsection");
    });

    it("should preserve anchors for unpublished files", () => {
      const content = `# Test Article

[Published link](./published.md#section) and [Unpublished link](./unpublished.md#section).`;

      const basePath = join(tempDir, "test.md");
      const publishedFile = join(tempDir, "published.md");
      const unpublishedFile = join(tempDir, "unpublished.md");

      TestHelpers.createTestFile(publishedFile, "# Published");
      TestHelpers.createTestFile(unpublishedFile, "# Unpublished");

      const processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [publishedFile, "https://telegra.ph/published"]
        // unpublishedFile is not in mappings
      ]);

      const result = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);

      expect(result.contentWithReplacedLinks).toContain("https://telegra.ph/published#section");
      expect(result.contentWithReplacedLinks).toContain("./unpublished.md#section");
      expect(result.localLinks[0]?.isPublished).toBe(true);
      expect(result.localLinks[0]?.telegraphUrl).toBe("https://telegra.ph/published#section");
      expect(result.localLinks[1]?.isPublished).toBe(false);
      expect(result.localLinks[1]?.telegraphUrl).toBeUndefined();
    });
  });

  describe("prepareForPublication", () => {
    it("should return content with replaced links", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md"]
      );
      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "linked.md");

      TestHelpers.createTestFile(linkedFile, "# Linked");

      let processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/Linked-01-01"]
      ]);

      processedContent = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);
      const publicationContent = ContentProcessor.prepareForPublication(processedContent);

      expect(publicationContent).toContain("https://telegra.ph/Linked-01-01");
      expect(publicationContent).not.toContain("./linked.md");
    });

    it("should handle content without links", () => {
      const content = "# Test Article\n\nThis is test content.";
      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);
      const publicationContent = ContentProcessor.prepareForPublication(processedContent);

      expect(publicationContent).toBe(content);
    });
  });

  describe("injectMetadataIntoContent", () => {
    it("should inject metadata into processed content", () => {
      const content = "# Test Article\n\nThis is test content.";
      const basePath = join(tempDir, "test.md");
      const metadata = TestHelpers.createSampleMetadata();

      const processedContent = ContentProcessor.processContent(content, basePath);
      const contentWithMetadata = ContentProcessor.injectMetadataIntoContent(processedContent, metadata);

      expect(contentWithMetadata).toContain("---");
      expect(contentWithMetadata).toContain(`telegraphUrl: "${metadata.telegraphUrl}"`);
      expect(contentWithMetadata).toContain(content);
    });

    it("should handle content that already has metadata", () => {
      const originalMetadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      const basePath = join(tempDir, "test.md");

      const contentWithOriginalMetadata = TestHelpers.createYamlFrontMatter(originalMetadata) + "\n\n" + content;
      const processedContent = ContentProcessor.processContent(contentWithOriginalMetadata, basePath);

      const newMetadata = TestHelpers.createSampleMetadata({
        title: "Updated Title"
      });

      const contentWithNewMetadata = ContentProcessor.injectMetadataIntoContent(processedContent, newMetadata);

      expect(contentWithNewMetadata).toContain(`title: "${newMetadata.title}"`);
      expect(contentWithNewMetadata).toContain(content);
    });
  });

  describe("validateContent", () => {
    it("should validate content with valid links", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./linked.md"]
      );
      const basePath = join(tempDir, "test.md");
      const linkedFile = join(tempDir, "linked.md");

      TestHelpers.createTestFile(linkedFile, "# Linked");

      let processedContent = ContentProcessor.processContent(content, basePath);

      const linkMappings = new Map([
        [linkedFile, "https://telegra.ph/Linked-01-01"]
      ]);

      processedContent = ContentProcessor.replaceLinksInContent(processedContent, linkMappings);
      const validation = ContentProcessor.validateContent(processedContent);

      expect(validation.isValid).toBe(true);
      expect(validation.issues).toHaveLength(0);
    });

    it("should detect broken links", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./non-existent.md"]
      );
      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);
      const validation = ContentProcessor.validateContent(processedContent);

      expect(validation.isValid).toBe(false);
      expect(validation.issues.some(issue => issue.includes("Broken local links"))).toBe(true);
    });

    it("should detect unpublished dependencies", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./unpublished.md"]
      );
      const basePath = join(tempDir, "test.md");
      const unpublishedFile = join(tempDir, "unpublished.md");

      TestHelpers.createTestFile(unpublishedFile, "# Unpublished");

      const processedContent = ContentProcessor.processContent(content, basePath);
      const validation = ContentProcessor.validateContent(processedContent);

      expect(validation.isValid).toBe(false);
      expect(validation.issues.some(issue => issue.includes("Unpublished dependencies"))).toBe(true);
    });

    it("should detect empty content", () => {
      const content = "";
      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);
      const validation = ContentProcessor.validateContent(processedContent);

      expect(validation.isValid).toBe(false);
      expect(validation.issues.some(issue => issue.includes("Content is empty"))).toBe(true);
    });

    it("should validate content with only whitespace as empty", () => {
      const content = "   \n  \t  \n   ";
      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);
      const validation = ContentProcessor.validateContent(processedContent);

      expect(validation.isValid).toBe(false);
      expect(validation.issues.some(issue => issue.includes("Content is empty"))).toBe(true);
    });
  });

  describe("getContentStats", () => {
    it("should return correct content statistics", () => {
      const content = "# Test\n\nContent with [local link](./test.md).";
      const result = ContentProcessor.processContent(content, tempDir);
      const stats = ContentProcessor.getContentStats(result);

      expect(stats.originalLength).toBeGreaterThan(0);
      expect(stats.processedLength).toBeGreaterThan(0);
      expect(stats.localLinksCount).toBe(1);
      expect(stats.hasMetadata).toBe(false);
    });
  });

  describe("cloneProcessedContent", () => {
    it("should create deep copy of processed content", () => {
      const content = "# Test\n\nContent with [local link](./test.md).";
      const original = ContentProcessor.processContent(content, tempDir);
      const cloned = ContentProcessor.cloneProcessedContent(original);

      expect(cloned).toEqual(original);
      expect(cloned).not.toBe(original);
      expect(cloned.localLinks).not.toBe(original.localLinks);
    });
  });

  describe("extractTitle", () => {
    it("should extract title from first heading", () => {
      const content = "# Heading Title\n\nContent here.";
      const result = ContentProcessor.processContent(content, tempDir);
      const title = ContentProcessor.extractTitle(result);

      expect(title).toBe("Heading Title");
    });

    it("should extract title from bold text", () => {
      const content = "**Bold Title**\n\nContent here.";
      const result = ContentProcessor.processContent(content, tempDir);
      const title = ContentProcessor.extractTitle(result);

      expect(title).toBe("Bold Title");
    });

    it("should return null for very long first line", () => {
      const longLine = "This is a very long line that should not be considered a title because it exceeds the 100 character limit and contains too much content to be a proper title";
      const content = `${longLine}\n\nContent here.`;
      const result = ContentProcessor.processContent(content, tempDir);
      const title = ContentProcessor.extractTitle(result);

      expect(title).toBe(null);
    });
  });

  describe("edge cases", () => {
    it("should handle content with mixed link types", () => {
      const content = `# Test Article

This is a [local link](./local.md) and an [external link](https://example.com).
Also a [mailto link](mailto:test@example.com) and another [local link](./another.md).`;

      const basePath = join(tempDir, "test.md");
      const localFile = join(tempDir, "local.md");
      const anotherFile = join(tempDir, "another.md");

      TestHelpers.createTestFile(localFile, "# Local");
      TestHelpers.createTestFile(anotherFile, "# Another");

      const processedContent = ContentProcessor.processContent(content, basePath);

      expect(processedContent.localLinks).toHaveLength(2);
      expect(processedContent.localLinks.map(l => l.originalPath)).toEqual(["./local.md", "./another.md"]);
    });

    it("should handle very large content", () => {
      const largeContent = `# Large Article\n\n${"A".repeat(50000)}`;
      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(largeContent, basePath);

      expect(processedContent.contentWithoutMetadata.length).toBeGreaterThan(50000);
      expect(processedContent.localLinks).toHaveLength(0);
    });

    it("should handle content with special characters", () => {
      const content = `# –¢–µ—Å—Ç —Å—Ç–∞—Ç—å—è

–≠—Ç–æ —Å—Ç–∞—Ç—å—è —Å [—Ä—É—Å—Å–∫–æ–π —Å—Å—ã–ª–∫–æ–π](./—Ä—É—Å—Å–∫–∏–π-—Ñ–∞–π–ª.md) –∏ [Chinese link](./‰∏≠ÊñáÊñá‰ª∂.md).

Also emoji: üîó [emoji link](./emoji-üòÄ.md)`;

      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);

      expect(processedContent.localLinks).toHaveLength(3);
      expect(processedContent.localLinks.map(l => l.originalPath)).toContain("./—Ä—É—Å—Å–∫–∏–π-—Ñ–∞–π–ª.md");
      expect(processedContent.localLinks.map(l => l.originalPath)).toContain("./‰∏≠ÊñáÊñá‰ª∂.md");
      expect(processedContent.localLinks.map(l => l.originalPath)).toContain("./emoji-üòÄ.md");
    });

    it("should handle content with multiple same links", () => {
      const content = `# Test Article

First reference to [same file](./same.md).
Second reference to [same file](./same.md).
Third reference to [same file again](./same.md).`;

      const basePath = join(tempDir, "test.md");
      const sameFile = join(tempDir, "same.md");

      TestHelpers.createTestFile(sameFile, "# Same");

      const processedContent = ContentProcessor.processContent(content, basePath);

      expect(processedContent.localLinks).toHaveLength(3);
      expect(processedContent.localLinks.every(l => l.originalPath === "./same.md")).toBe(true);
    });

    it("should handle nested markdown structures", () => {
      const content = `# Test Article

## Section with [link](./section.md)

> Quote with [quoted link](./quote.md)

- List item with [list link](./list.md)
- Another item

\`\`\`markdown
This [code link](./code.md) should still be found
\`\`\`

Regular [normal link](./normal.md) after code block.`;

      const basePath = join(tempDir, "test.md");

      const processedContent = ContentProcessor.processContent(content, basePath);

      // Our simple parser finds all links including ones in code blocks
      expect(processedContent.localLinks).toHaveLength(5);
      expect(processedContent.localLinks.map(l => l.originalPath)).toContain("./code.md");
    });
  });
});
```

`src/content/ContentProcessor.ts`

```ts
import { lstatSync, readFileSync } from "node:fs";
import { LinkResolver } from "../links/LinkResolver";
import { MetadataManager } from "../metadata/MetadataManager";
import type { FileMetadata, LocalLink, ProcessedContent } from "../types/metadata";

/**
 * Processes content for publication with link replacement and metadata handling
 */
export class ContentProcessor {

  /**
   * Remove duplicate title from content if it matches metadata title
   * @param content Content to process
   * @param metadataTitle Title from metadata
   * @returns Content with duplicate title removed if necessary
   */
  static removeDuplicateTitle(content: string, metadataTitle?: string): string {
    if (!metadataTitle) {
      return content;
    }

    // Normalize titles for comparison (remove extra spaces, convert to lowercase)
    const normalizedMetadataTitle = metadataTitle.trim().toLowerCase();

    // Split content into lines and find first non-empty line
    const lines = content.split(/\r?\n/);
    let headerLineIndex = -1;

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i];
      if (!line || line.trim() === '') continue; // Skip empty lines

      // Check if this line is an h1 header
      const h1Match = line.match(/^\s*#\s+(.+?)$/);
      if (h1Match && h1Match[1]) {
        const contentTitle = h1Match[1].trim().toLowerCase();

        // If titles match, mark for removal
        if (normalizedMetadataTitle === contentTitle) {
          headerLineIndex = i;
        }
      }
      break; // Only check first non-empty line
    }

    // If we found a matching header, remove it
    if (headerLineIndex >= 0) {
      lines.splice(headerLineIndex, 1);

      // Remove any empty lines that follow the removed header
      while (headerLineIndex < lines.length) {
        const currentLine = lines[headerLineIndex];
        if (currentLine && currentLine.trim() === '') {
          lines.splice(headerLineIndex, 1);
        } else {
          break;
        }
      }

      return lines.join('\n');
    }

    return content;
  }

  /**
   * Process file content for publication
   * @param filePath Path to file to process
   * @returns Processed content information
   */
  static processFile(filePath: string): ProcessedContent {
    try {
      // Check if path is a directory
      try {
        const stats = lstatSync(filePath);
        if (stats.isDirectory()) {
          throw new Error(`Cannot process directory as file: ${filePath}`);
        }
      } catch (error) {
        // If we can't stat the path, try decoding and check again
        try {
          const decodedPath = decodeURIComponent(filePath);
          const stats = lstatSync(decodedPath);
          if (stats.isDirectory()) {
            throw new Error(`Cannot process directory as file: ${decodedPath}`);
          }
        } catch {
          // Path doesn't exist, will be handled by readFileSync below
        }
      }

      // Try the path as-is first
      let originalContent: string;
      try {
        originalContent = readFileSync(filePath, 'utf-8');
      } catch (error) {
        // Try decoding URL-encoded characters
        const decodedPath = decodeURIComponent(filePath);
        originalContent = readFileSync(decodedPath, 'utf-8');
      }
      return ContentProcessor.processContent(originalContent, filePath);
    } catch (error) {
      console.error(`Error reading file ${filePath}:`, error);
      throw new Error(`Failed to read file: ${filePath}`);
    }
  }

  /**
   * Process content string for publication
   * @param content Content to process
   * @param basePath Base path for resolving relative links
   * @returns Processed content information
   */
  static processContent(content: string, basePath: string): ProcessedContent {
    // Extract metadata if present
    const metadata = MetadataManager.parseMetadata(content);

    // Remove metadata from content
    let contentWithoutMetadata = MetadataManager.removeMetadata(content);

    // Remove duplicate title if it matches metadata title
    contentWithoutMetadata = ContentProcessor.removeDuplicateTitle(contentWithoutMetadata, metadata?.title);

    // Find local links
    const localLinks = LinkResolver.findLocalLinks(contentWithoutMetadata, basePath);

    // Initially, content with replaced links is same as original
    // Will be modified by replaceLinksInContent method
    const contentWithReplacedLinks = contentWithoutMetadata;

    return {
      originalContent: content,
      contentWithoutMetadata,
      contentWithReplacedLinks,
      contentWithLocalLinks: contentWithoutMetadata, // Initially same as without metadata
      metadata: metadata || undefined,
      localLinks,
      telegraphLinks: [], // Will be populated when links are replaced
      hasChanges: localLinks.length > 0
    };
  }

  /**
   * Replace local links in processed content with Telegraph URLs
   * @param processedContent Previously processed content
   * @param linkMappings Map of local paths to Telegraph URLs
   * @returns Updated processed content with replaced links
   */
  static replaceLinksInContent(
    processedContent: ProcessedContent,
    linkMappings: Map<string, string>
  ): ProcessedContent {
    // Create mapping from original paths to Telegraph URLs
    const replacementMap = new Map<string, string>();

    for (const link of processedContent.localLinks) {
      // Extract file path without anchor from resolvedPath for lookup in linkMappings
      const anchorIndex = link.resolvedPath.indexOf('#');
      const filePathOnly = anchorIndex !== -1 ? link.resolvedPath.substring(0, anchorIndex) : link.resolvedPath;

      const telegraphUrl = linkMappings.get(filePathOnly);
      if (telegraphUrl) {
        // Check for and preserve the URL fragment (anchor) from original path
        const originalAnchorIndex = link.originalPath.indexOf('#');
        let finalUrl = telegraphUrl;

        if (originalAnchorIndex !== -1) {
          const anchor = link.originalPath.substring(originalAnchorIndex);
          finalUrl += anchor;
        }

        // Use the final URL (with anchor) for replacement
        replacementMap.set(link.originalPath, finalUrl);
        // Update link object
        link.telegraphUrl = finalUrl;
        link.isPublished = true;
      }
    }

    // Replace links in content
    const contentWithReplacedLinks = LinkResolver.replaceLocalLinks(
      processedContent.contentWithoutMetadata,
      replacementMap
    );

    return {
      ...processedContent,
      contentWithReplacedLinks,
      hasChanges: replacementMap.size > 0
    };
  }

  /**
   * Prepare content for Telegraph publication
   * @param processedContent Processed content with replaced links
   * @returns Content ready for Telegraph API
   */
  static prepareForPublication(processedContent: ProcessedContent): string {
    return processedContent.contentWithReplacedLinks;
  }

  /**
   * Create content with injected metadata
   * @param processedContent Original processed content
   * @param metadata Metadata to inject
   * @returns Content with metadata injected
   */
  static injectMetadataIntoContent(
    processedContent: ProcessedContent,
    metadata: FileMetadata
  ): string {
    return MetadataManager.injectMetadata(
      processedContent.contentWithoutMetadata,
      metadata
    );
  }

  /**
   * Validate processed content for publication
   * @param processedContent Content to validate
   * @param options Validation options
   * @returns Validation result with any issues found
   */
  static validateContent(processedContent: ProcessedContent, options: {
    allowBrokenLinks?: boolean;
    allowUnpublishedDependencies?: boolean;
  } = {}): {
    isValid: boolean;
    issues: string[];
  } {
    const issues: string[] = [];

    // Check for broken local links (only if not allowed)
    if (!options.allowBrokenLinks) {
      const brokenLinks = processedContent.localLinks.filter(link =>
        !LinkResolver.validateLinkTarget(link.resolvedPath)
      );

      if (brokenLinks.length > 0) {
        issues.push(`Broken local links found: ${brokenLinks.map(l => l.originalPath).join(', ')}`);
      }
    }

    // Check for unpublished dependencies (only if not allowed)
    if (!options.allowUnpublishedDependencies) {
      const unpublishedLinks = processedContent.localLinks.filter(link =>
        !link.isPublished && LinkResolver.isMarkdownFile(link.resolvedPath)
      );

      if (unpublishedLinks.length > 0) {
        issues.push(`Unpublished dependencies: ${unpublishedLinks.map(l => l.originalPath).join(', ')}`);
      }
    }

    // Check content length
    if (processedContent.contentWithReplacedLinks.trim().length === 0) {
      issues.push('Content is empty after processing');
    }

    return {
      isValid: issues.length === 0,
      issues
    };
  }

  /**
   * Get content statistics
   * @param processedContent Content to analyze
   * @returns Content statistics
   */
  static getContentStats(processedContent: ProcessedContent): {
    originalLength: number;
    processedLength: number;
    localLinksCount: number;
    markdownLinksCount: number;
    replacedLinksCount: number;
    hasMetadata: boolean;
  } {
    const markdownLinks = LinkResolver.filterMarkdownLinks(processedContent.localLinks);
    const replacedLinks = processedContent.localLinks.filter(link => link.isPublished);

    return {
      originalLength: processedContent.originalContent.length,
      processedLength: processedContent.contentWithReplacedLinks.length,
      localLinksCount: processedContent.localLinks.length,
      markdownLinksCount: markdownLinks.length,
      replacedLinksCount: replacedLinks.length,
      hasMetadata: processedContent.metadata !== null && processedContent.metadata !== undefined
    };
  }

  /**
   * Create a copy of processed content for safe modification
   * @param processedContent Content to clone
   * @returns Deep copy of processed content
   */
  static cloneProcessedContent(processedContent: ProcessedContent): ProcessedContent {
    return {
      originalContent: processedContent.originalContent,
      contentWithoutMetadata: processedContent.contentWithoutMetadata,
      contentWithReplacedLinks: processedContent.contentWithReplacedLinks,
      contentWithLocalLinks: processedContent.contentWithLocalLinks,
      metadata: processedContent.metadata ? { ...processedContent.metadata } : processedContent.metadata,
      localLinks: processedContent.localLinks.map(link => ({ ...link })),
      telegraphLinks: processedContent.telegraphLinks.map(link => ({ ...link })),
      hasChanges: processedContent.hasChanges
    };
  }

  /**
   * Extract title from processed content
   * @param processedContent Content to extract title from
   * @returns Extracted title or null
   */
  static extractTitle(processedContent: ProcessedContent): string | null {
    // First check metadata
    if (processedContent.metadata?.title) {
      return processedContent.metadata.title;
    }

    // Then check content for first heading
    const content = processedContent.contentWithoutMetadata;
    const lines = content.split(/\r?\n/);

    for (const line of lines) {
      const trimmed = line.trim();
      if (trimmed === '') continue;

      // Check for markdown heading
      const headingMatch = trimmed.match(/^(#+)\s*(.*)/);
      if (headingMatch && headingMatch[2]) {
        return headingMatch[2].trim();
      }

      // Check for bold/italic that could be a title
      const boldMatch = trimmed.match(/^(?:\*{2}|__)(.*?)(?:\*{2}|__)$/);
      if (boldMatch && boldMatch[1]) {
        return boldMatch[1].trim();
      }

      // First non-empty line could be title if short enough
      if (trimmed.length <= 100 && !trimmed.includes('\n')) {
        return trimmed;
      }

      break; // Only check first non-empty line
    }

    return null;
  }
}
```

`src/dependencies/DependencyManager.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, it } from "bun:test";
import { mkdtempSync, rmSync, writeFileSync } from "node:fs";
import { tmpdir } from "node:os";
import { dirname, join, relative } from "node:path";
import { TestHelpers } from "../test-utils/TestHelpers";
import type { DependencyNode, MetadataConfig } from "../types/metadata";
import { PublicationStatus } from "../types/metadata";
import { PathResolver } from "../utils/PathResolver";
import { DependencyManager } from "./DependencyManager";

describe("DependencyManager", () => {
  let tempDir: string;
  let dependencyManager: DependencyManager;
  let config: MetadataConfig;
  let pathResolver: PathResolver;

  beforeEach(() => {
    tempDir = TestHelpers.createTempDir("dependency-test");
    config = TestHelpers.createTestConfig({
      maxDependencyDepth: 5,
      autoPublishDependencies: true
    });
    pathResolver = PathResolver.getInstance();
    dependencyManager = new DependencyManager(config, pathResolver);
  });

  afterEach(() => {
    TestHelpers.cleanup();
  });

  describe("buildDependencyTree", () => {
    it("should build simple dependency tree", () => {
      const mainFile = join(tempDir, "main.md");
      const depFile = join(tempDir, "dep.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep.md"]
      ));
      TestHelpers.createTestFile(depFile, TestHelpers.createMarkdownWithLinks(
        "Dependency Article", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);

      expect(tree.filePath).toBe(mainFile);
      expect(tree.depth).toBe(0);
      expect(tree.dependencies).toHaveLength(1);
      expect(tree.dependencies[0]?.filePath).toBe(depFile);
      expect(tree.dependencies[0]?.depth).toBe(1);
    });

    it("should handle nested dependencies", () => {
      const mainFile = join(tempDir, "main.md");
      const dep1File = join(tempDir, "dep1.md");
      const dep2File = join(tempDir, "dep2.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep1.md"]
      ));
      TestHelpers.createTestFile(dep1File, TestHelpers.createMarkdownWithLinks(
        "Dependency 1", ["./dep2.md"]
      ));
      TestHelpers.createTestFile(dep2File, TestHelpers.createMarkdownWithLinks(
        "Dependency 2", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);

      expect(tree.dependencies).toHaveLength(1);
      expect(tree.dependencies[0]?.dependencies).toHaveLength(1);
      expect(tree.dependencies[0]?.dependencies[0]?.filePath).toBe(dep2File);
      expect(tree.dependencies[0]?.dependencies[0]?.depth).toBe(2);
    });

    it("should respect max depth limit", () => {
      const mainFile = join(tempDir, "main.md");
      const dep1File = join(tempDir, "dep1.md");
      const dep2File = join(tempDir, "dep2.md");
      const dep3File = join(tempDir, "dep3.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep1.md"]
      ));
      TestHelpers.createTestFile(dep1File, TestHelpers.createMarkdownWithLinks(
        "Dependency 1", ["./dep2.md"]
      ));
      TestHelpers.createTestFile(dep2File, TestHelpers.createMarkdownWithLinks(
        "Dependency 2", ["./dep3.md"]
      ));
      TestHelpers.createTestFile(dep3File, TestHelpers.createMarkdownWithLinks(
        "Dependency 3", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile, 2);

      expect(tree.dependencies[0]?.dependencies).toHaveLength(1);
      expect(tree.dependencies[0]?.dependencies[0]?.dependencies).toHaveLength(0); // Depth limit reached
    });

    it("should handle files with metadata", () => {
      const mainFile = join(tempDir, "main.md");
      const depFile = join(tempDir, "dep.md");

      const metadata = TestHelpers.createSampleMetadata();

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep.md"]
      ));
      TestHelpers.createTestFile(depFile, TestHelpers.createMarkdownWithLinks(
        "Dependency Article", []
      ), metadata);

      const tree = dependencyManager.buildDependencyTree(mainFile);

      expect(tree.status).toBe(PublicationStatus.NOT_PUBLISHED);
      expect(tree.dependencies[0]?.status).toBe(PublicationStatus.PUBLISHED);
    });

    it("should handle non-existent files", () => {
      const mainFile = join(tempDir, "main.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./non-existent.md"]
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);

      // Non-existent files are not added to dependency tree
      expect(tree.dependencies).toHaveLength(0);
    });
  });

  describe("detectCircularDependencies", () => {
    it("should detect simple circular dependency", () => {
      const file1 = join(tempDir, "file1.md");
      const file2 = join(tempDir, "file2.md");

      TestHelpers.createTestFile(file1, TestHelpers.createMarkdownWithLinks(
        "File 1", ["./file2.md"]
      ));
      TestHelpers.createTestFile(file2, TestHelpers.createMarkdownWithLinks(
        "File 2", ["./file1.md"]
      ));

      const tree = dependencyManager.buildDependencyTree(file1);
      const cycles = dependencyManager.detectCircularDependencies(tree);

      expect(cycles).toHaveLength(1);
      expect(cycles[0]).toContain(file1);
      expect(cycles[0]).toContain(file2);
    });

    it("should detect complex circular dependency", () => {
      const file1 = join(tempDir, "file1.md");
      const file2 = join(tempDir, "file2.md");
      const file3 = join(tempDir, "file3.md");

      TestHelpers.createTestFile(file1, TestHelpers.createMarkdownWithLinks(
        "File 1", ["./file2.md"]
      ));
      TestHelpers.createTestFile(file2, TestHelpers.createMarkdownWithLinks(
        "File 2", ["./file3.md"]
      ));
      TestHelpers.createTestFile(file3, TestHelpers.createMarkdownWithLinks(
        "File 3", ["./file1.md"]
      ));

      const tree = dependencyManager.buildDependencyTree(file1);
      const cycles = dependencyManager.detectCircularDependencies(tree);

      expect(cycles).toHaveLength(1);
      expect(cycles[0]).toContain(file1);
      expect(cycles[0]).toContain(file2);
      expect(cycles[0]).toContain(file3);
    });

    it("should return empty array for acyclic graph", () => {
      const mainFile = join(tempDir, "main.md");
      const dep1File = join(tempDir, "dep1.md");
      const dep2File = join(tempDir, "dep2.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep1.md", "./dep2.md"]
      ));
      TestHelpers.createTestFile(dep1File, TestHelpers.createMarkdownWithLinks(
        "Dependency 1", []
      ));
      TestHelpers.createTestFile(dep2File, TestHelpers.createMarkdownWithLinks(
        "Dependency 2", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);
      const cycles = dependencyManager.detectCircularDependencies(tree);

      expect(cycles).toHaveLength(0);
    });
  });

  describe("orderDependencies", () => {
    it("should order dependencies correctly", () => {
      const mainFile = join(tempDir, "main.md");
      const dep1File = join(tempDir, "dep1.md");
      const dep2File = join(tempDir, "dep2.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep1.md", "./dep2.md"]
      ));
      TestHelpers.createTestFile(dep1File, TestHelpers.createMarkdownWithLinks(
        "Dependency 1", ["./dep2.md"]
      ));
      TestHelpers.createTestFile(dep2File, TestHelpers.createMarkdownWithLinks(
        "Dependency 2", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);
      const order = dependencyManager.orderDependencies(tree);

      expect(order).toHaveLength(3);
      expect(order).toContain(mainFile);
      expect(order).toContain(dep1File);
      expect(order).toContain(dep2File);
      // Check that all files are in the order (actual order may vary based on implementation)
      expect(order.length).toBe(3);
    });

    it("should handle single file", () => {
      const singleFile = join(tempDir, "single.md");

      TestHelpers.createTestFile(singleFile, TestHelpers.createMarkdownWithLinks(
        "Single Article", []
      ));

      const tree = dependencyManager.buildDependencyTree(singleFile);
      const order = dependencyManager.orderDependencies(tree);

      expect(order).toEqual([singleFile]);
    });

    it("should handle diamond dependency pattern", () => {
      const mainFile = join(tempDir, "main.md");
      const leftFile = join(tempDir, "left.md");
      const rightFile = join(tempDir, "right.md");
      const sharedFile = join(tempDir, "shared.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./left.md", "./right.md"]
      ));
      TestHelpers.createTestFile(leftFile, TestHelpers.createMarkdownWithLinks(
        "Left Branch", ["./shared.md"]
      ));
      TestHelpers.createTestFile(rightFile, TestHelpers.createMarkdownWithLinks(
        "Right Branch", ["./shared.md"]
      ));
      TestHelpers.createTestFile(sharedFile, TestHelpers.createMarkdownWithLinks(
        "Shared Dependency", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);
      const order = dependencyManager.orderDependencies(tree);

      expect(order).toHaveLength(4);
      expect(order).toContain(mainFile);
      expect(order).toContain(leftFile);
      expect(order).toContain(rightFile);
      expect(order).toContain(sharedFile);

      // All files should be present in the order
      expect(new Set(order).size).toBe(4);
    });
  });

  describe("analyzeDependencyTree", () => {
    it("should provide comprehensive analysis", () => {
      const mainFile = join(tempDir, "main.md");
      const dep1File = join(tempDir, "dep1.md");
      const dep2File = join(tempDir, "dep2.md");

      const metadata = TestHelpers.createSampleMetadata();

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep1.md", "./dep2.md"]
      ));
      TestHelpers.createTestFile(dep1File, TestHelpers.createMarkdownWithLinks(
        "Dependency 1", []
      ), metadata);
      TestHelpers.createTestFile(dep2File, TestHelpers.createMarkdownWithLinks(
        "Dependency 2", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);
      const analysis = dependencyManager.analyzeDependencyTree(tree);

      expect(analysis.totalFiles).toBe(3);
      expect(analysis.publishedFiles).toBe(1);
      expect(analysis.unpublishedFiles).toBe(2);
      expect(analysis.maxDepth).toBe(1);
      expect(analysis.circularDependencies).toHaveLength(0);
      expect(analysis.publishOrder).toHaveLength(3);
    });

    it("should detect circular dependencies in analysis", () => {
      const file1 = join(tempDir, "file1.md");
      const file2 = join(tempDir, "file2.md");

      TestHelpers.createTestFile(file1, TestHelpers.createMarkdownWithLinks(
        "File 1", ["./file2.md"]
      ));
      TestHelpers.createTestFile(file2, TestHelpers.createMarkdownWithLinks(
        "File 2", ["./file1.md"]
      ));

      const tree = dependencyManager.buildDependencyTree(file1);
      const analysis = dependencyManager.analyzeDependencyTree(tree);

      expect(analysis.circularDependencies).toHaveLength(1);
      expect(analysis.totalFiles).toBeGreaterThan(0);
    });
  });

  describe("getFilesToPublish", () => {
    it("should return only unpublished files", () => {
      const mainFile = join(tempDir, "main.md");
      const publishedFile = join(tempDir, "published.md");
      const unpublishedFile = join(tempDir, "unpublished.md");

      const metadata = TestHelpers.createSampleMetadata();

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./published.md", "./unpublished.md"]
      ));
      TestHelpers.createTestFile(publishedFile, TestHelpers.createMarkdownWithLinks(
        "Published Article", []
      ), metadata);
      TestHelpers.createTestFile(unpublishedFile, TestHelpers.createMarkdownWithLinks(
        "Unpublished Article", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);
      const filesToPublish = dependencyManager.getFilesToPublish(tree);

      expect(filesToPublish).toContain(mainFile);
      expect(filesToPublish).toContain(unpublishedFile);
      expect(filesToPublish).not.toContain(publishedFile);
    });

    it("should return empty array for fully published tree", () => {
      const mainFile = join(tempDir, "main.md");
      const depFile = join(tempDir, "dep.md");

      const metadata1 = TestHelpers.createSampleMetadata();
      const metadata2 = TestHelpers.createSampleMetadata({
        telegraphUrl: "https://telegra.ph/Dep-Article-01-01"
      });

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./dep.md"]
      ), metadata1);
      TestHelpers.createTestFile(depFile, TestHelpers.createMarkdownWithLinks(
        "Dependency Article", []
      ), metadata2);

      const tree = dependencyManager.buildDependencyTree(mainFile);
      const filesToPublish = dependencyManager.getFilesToPublish(tree);

      expect(filesToPublish).toHaveLength(0);
    });
  });

  describe("edge cases", () => {
    it("should handle file with no links", () => {
      const singleFile = join(tempDir, "single.md");

      TestHelpers.createTestFile(singleFile, TestHelpers.createMarkdownWithLinks(
        "Single Article", []
      ));

      const tree = dependencyManager.buildDependencyTree(singleFile);

      expect(tree.filePath).toBe(singleFile);
      expect(tree.dependencies).toHaveLength(0);
      expect(tree.depth).toBe(0);
    });

    it("should handle deeply nested dependencies", () => {
      const files: string[] = [];
      for (let i = 0; i < 10; i++) {
        files.push(join(tempDir, `file${i}.md`));
      }

      // Create chain: file0 -> file1 -> file2 -> ... -> file9
      for (let i = 0; i < files.length; i++) {
        const nextFile = i < files.length - 1 ? [`./file${i + 1}.md`] : [];
        TestHelpers.createTestFile(files[i]!, TestHelpers.createMarkdownWithLinks(
          `Article ${i}`, nextFile
        ));
      }

      const tree = dependencyManager.buildDependencyTree(files[0]!, 10);
      const analysis = dependencyManager.analyzeDependencyTree(tree);

      expect(analysis.totalFiles).toBe(10);
      expect(analysis.maxDepth).toBe(9);
    });

    it("should handle files with same name in different directories", () => {
      const subDir = join(tempDir, "subdir");
      const mainFile = join(tempDir, "main.md");
      const rootFile = join(tempDir, "same.md");
      const subFile = join(subDir, "same.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./same.md", "./subdir/same.md"]
      ));
      TestHelpers.createTestFile(rootFile, TestHelpers.createMarkdownWithLinks(
        "Root Same", []
      ));
      TestHelpers.createTestFile(subFile, TestHelpers.createMarkdownWithLinks(
        "Sub Same", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);

      expect(tree.dependencies).toHaveLength(2);
      expect(tree.dependencies.map(d => d.filePath)).toContain(rootFile);
      expect(tree.dependencies.map(d => d.filePath)).toContain(subFile);
    });

    it("should handle external links mixed with local links", () => {
      const mainFile = join(tempDir, "main.md");
      const localFile = join(tempDir, "local.md");

      const content = `# Main Article

This is a [local link](./local.md) and an [external link](https://example.com).
Also a [mailto link](mailto:test@example.com).`;

      TestHelpers.createTestFile(mainFile, content);
      TestHelpers.createTestFile(localFile, TestHelpers.createMarkdownWithLinks(
        "Local Article", []
      ));

      const tree = dependencyManager.buildDependencyTree(mainFile);

      expect(tree.dependencies).toHaveLength(1);
      expect(tree.dependencies[0]?.filePath).toBe(localFile);
    });

    it("should handle corrupted metadata files", () => {
      const mainFile = join(tempDir, "main.md");
      const corruptedFile = join(tempDir, "corrupted.md");

      TestHelpers.createTestFile(mainFile, TestHelpers.createMarkdownWithLinks(
        "Main Article", ["./corrupted.md"]
      ));

      // Create file with invalid metadata
      const corruptedContent = `---
telegraphUrl: "invalid-url
editPath: "Test-01-01"
username: "Test Author"
publishedAt: "invalid-date"
originalFilename: "corrupted.md"
---

# Corrupted Article`;

      TestHelpers.createTestFile(corruptedFile, corruptedContent);

      const tree = dependencyManager.buildDependencyTree(mainFile);

      expect(tree.dependencies).toHaveLength(1);
      expect(tree.dependencies[0]?.status).toBe(PublicationStatus.METADATA_CORRUPTED);
    });
  });
});

describe('DependencyManager depth consistency fix', () => {
  let manager: DependencyManager;
  let tempDir: string;

  beforeEach(() => {
    const config = TestHelpers.createTestConfig();
    const pathResolver = PathResolver.getInstance();
    manager = new DependencyManager(config, pathResolver);
    tempDir = mkdtempSync(join(tmpdir(), 'dependency-depth-test-'));
  });

  afterEach(() => {
    rmSync(tempDir, { recursive: true, force: true });
  });

  it('should produce identical results for different depth values when sufficient', () => {
    // Create a shared dependency structure
    const sharedFile = join(tempDir, 'shared.md');
    const file1 = join(tempDir, 'file1.md');
    const file2 = join(tempDir, 'file2.md');
    const rootFile = join(tempDir, 'root.md');

    writeFileSync(sharedFile, '# Shared content\n[link to another](other.md)');
    writeFileSync(file1, `# File 1\n[shared dependency](${relative(dirname(file1), sharedFile)})`);
    writeFileSync(file2, `# File 2\n[same shared dependency](${relative(dirname(file2), sharedFile)})`);
    writeFileSync(rootFile, `# Root\n[to file1](${relative(dirname(rootFile), file1)})\n[to file2](${relative(dirname(rootFile), file2)})`);

    // Build trees with different depth values
    const tree10 = manager.buildDependencyTree(rootFile, 10);
    const tree100 = manager.buildDependencyTree(rootFile, 100);
    const tree5 = manager.buildDependencyTree(rootFile, 5);

    // Helper function to normalize trees for comparison (remove depth info that may vary)
    const normalizeForComparison = (node: DependencyNode): any => ({
      filePath: node.filePath,
      status: node.status,
      dependencies: node.dependencies.map(normalizeForComparison).sort((a, b) => a.filePath.localeCompare(b.filePath))
    });

    const normalized10 = normalizeForComparison(tree10);
    const normalized100 = normalizeForComparison(tree100);
    const normalized5 = normalizeForComparison(tree5);

    // Different sufficient depths should produce identical trees
    expect(normalized10).toEqual(normalized100);
    expect(normalized10).toEqual(normalized5);

    // Verify shared dependencies maintain their structure in all instances
    const findSharedNodeInTree = (tree: DependencyNode, targetPath: string): DependencyNode | null => {
      if (tree.filePath === targetPath) return tree;
      for (const dep of tree.dependencies) {
        const found = findSharedNodeInTree(dep, targetPath);
        if (found) return found;
      }
      return null;
    };

    const sharedIn10 = findSharedNodeInTree(tree10, sharedFile);
    const sharedIn100 = findSharedNodeInTree(tree100, sharedFile);

    expect(sharedIn10).toBeTruthy();
    expect(sharedIn100).toBeTruthy();

    // Shared nodes should have identical structure regardless of path taken to reach them
    if (sharedIn10 && sharedIn100) {
      expect(normalizeForComparison(sharedIn10)).toEqual(normalizeForComparison(sharedIn100));
    }
  });

  it('should handle shared dependencies with multiple reference paths correctly', () => {
    // Create a more complex shared dependency scenario
    const shared = join(tempDir, 'shared.md');
    const intermediate1 = join(tempDir, 'intermediate1.md');
    const intermediate2 = join(tempDir, 'intermediate2.md');
    const root = join(tempDir, 'root.md');

    writeFileSync(shared, '# Shared\nThis is shared content');
    writeFileSync(intermediate1, `# Int1\n[to shared](${relative(dirname(intermediate1), shared)})`);
    writeFileSync(intermediate2, `# Int2\n[to shared](${relative(dirname(intermediate2), shared)})`);
    writeFileSync(root, `# Root\n[to int1](${relative(dirname(root), intermediate1)})\n[to int2](${relative(dirname(root), intermediate2)})`);

    const tree = manager.buildDependencyTree(root, 10);

    // Count how many times shared file appears in the tree
    const countOccurrences = (node: DependencyNode, targetPath: string): number => {
      let count = node.filePath === targetPath ? 1 : 0;
      for (const dep of node.dependencies) {
        count += countOccurrences(dep, targetPath);
      }
      return count;
    };

    const sharedCount = countOccurrences(tree, shared);
    expect(sharedCount).toBe(2); // Should appear twice (through both intermediate files)

    // All instances should be complete nodes (not shallow)
    const collectSharedNodes = (node: DependencyNode, targetPath: string): DependencyNode[] => {
      const nodes: DependencyNode[] = [];
      if (node.filePath === targetPath) nodes.push(node);
      for (const dep of node.dependencies) {
        nodes.push(...collectSharedNodes(dep, targetPath));
      }
      return nodes;
    };

    const allSharedNodes = collectSharedNodes(tree, shared);
    expect(allSharedNodes).toHaveLength(2);

    // Both instances should be identical complete nodes
    expect(allSharedNodes[0]).toEqual(allSharedNodes[1]);
  });
});
```

`src/dependencies/DependencyManager.ts`

```ts
import { existsSync, lstatSync, readFileSync } from "node:fs";
import { dirname } from "node:path";
import { LinkResolver } from "../links/LinkResolver";
import { LinkScanner } from "../links/LinkScanner";
import { MetadataManager } from "../metadata/MetadataManager";
import type { DependencyNode, LocalLink, MetadataConfig } from "../types/metadata";
import { PublicationStatus } from "../types/metadata";
import type { PathResolver } from "../utils/PathResolver";

/**
 * Manages dependency trees and recursive publishing
 */
export class DependencyManager {
  private config: MetadataConfig;
  private memoCache: Map<string, DependencyNode>;
  private processingStack: Set<string>;
  private pathResolver: PathResolver;

  constructor(config: MetadataConfig, pathResolver: PathResolver) {
    this.config = config;
    this.memoCache = new Map();
    this.processingStack = new Set();
    this.pathResolver = pathResolver;
  }

  /**
   * Build dependency tree for a file
   * @param filePath Root file path
   * @param maxDepth Maximum recursion depth
   * @returns Dependency tree root node
   */
  buildDependencyTree(filePath: string, maxDepth?: number): DependencyNode {
    const depth = maxDepth ?? this.config.maxDependencyDepth;
    this.memoCache.clear();
    this.processingStack.clear();

    return this.buildNodeRecursive(filePath, 0, depth);
  }

  /**
   * Detect circular dependencies in tree
   * @param root Root dependency node
   * @returns Array of circular dependency paths
   */
  detectCircularDependencies(root: DependencyNode): string[][] {
    const cycles: string[][] = [];
    const visited = new Set<string>();
    const recursionStack = new Set<string>();

    this.detectCyclesRecursive(root, visited, recursionStack, [], cycles);

    return cycles;
  }

  /**
   * Order dependencies for publishing (topological sort)
   * @param root Root dependency node
   * @returns Ordered array of file paths for publishing
   */
  orderDependencies(root: DependencyNode): string[] {
    const ordered: string[] = [];
    const visited = new Set<string>();
    const temporary = new Set<string>();

    this.topologicalSortRecursive(root, visited, temporary, ordered);

    return ordered.reverse(); // Reverse to get dependencies first
  }

  /**
   * Analyze dependency tree and return statistics
   * @param root Root dependency node
   * @returns Dependency analysis results
   */
  analyzeDependencyTree(root: DependencyNode): {
    totalFiles: number;
    publishedFiles: number;
    unpublishedFiles: number;
    maxDepth: number;
    circularDependencies: string[][];
    publishOrder: string[];
  } {
    const allNodes = this.getAllNodes(root);
    const circularDeps = this.detectCircularDependencies(root);
    const publishOrder = this.orderDependencies(root);

    return {
      totalFiles: allNodes.length,
      publishedFiles: allNodes.filter(node => node.status === PublicationStatus.PUBLISHED).length,
      unpublishedFiles: allNodes.filter(node => node.status === PublicationStatus.NOT_PUBLISHED).length,
      maxDepth: Math.max(...allNodes.map(node => node.depth)),
      circularDependencies: circularDeps,
      publishOrder
    };
  }

  /**
   * Get all files that need to be published
   * @param root Root dependency node
   * @returns Array of file paths that need publishing
   */
  getFilesToPublish(root: DependencyNode): string[] {
    const allNodes = this.getAllNodes(root);
    return allNodes
      .filter(node => node.status === PublicationStatus.NOT_PUBLISHED)
      .map(node => node.filePath);
  }

  /**
   * Mark node as processed
   * @param filePath File path to mark as processed
   */
  markAsProcessed(filePath: string): void {
    // Deprecated: Use memoization cache instead
    console.warn('markAsProcessed is deprecated with memoization approach');
  }

  /**
   * Check if file has been processed
   * @param filePath File path to check
   * @returns True if file has been processed
   */
  isProcessed(filePath: string): boolean {
    return this.memoCache.has(filePath);
  }

  /**
   * Reset processing state
   */
  reset(): void {
    this.memoCache.clear();
    this.processingStack.clear();
  }

  /**
   * Build dependency node recursively
   * @param filePath File path
   * @param currentDepth Current recursion depth
   * @param maxDepth Maximum allowed depth
   * @returns Dependency node
   */
  private buildNodeRecursive(
    filePath: string,
    currentDepth: number,
    maxDepth: number
  ): DependencyNode {
    // Check memoization cache first - return complete cached node if available
    if (this.memoCache.has(filePath)) {
      return this.memoCache.get(filePath)!;
    }

    // Check for circular dependency
    if (this.processingStack.has(filePath)) {
      console.warn(`Circular dependency detected: ${filePath}`);
      // Create shallow node but don't cache it to break the cycle
      return this.createNode(filePath, currentDepth, []);
    }

    // Check depth limit
    if (currentDepth >= maxDepth) {
      console.warn(`Maximum dependency depth reached at ${filePath}`);
      // Don't cache nodes truncated by depth limit
      return this.createNode(filePath, currentDepth, []);
    }

    this.processingStack.add(filePath);

    try {
      // Check if path is a directory
      try {
        const stats = lstatSync(filePath);
        if (stats.isDirectory()) {
          console.error(`Error processing file ${filePath}: Cannot process directory as file`);
          return this.createNode(filePath, currentDepth, []);
        }
      } catch (error) {
        // If we can't stat the path, try decoding and check again
        try {
          const decodedPath = decodeURIComponent(filePath);
          const stats = lstatSync(decodedPath);
          if (stats.isDirectory()) {
            console.error(`Error processing file ${decodedPath}: Cannot process directory as file`);
            return this.createNode(filePath, currentDepth, []);
          }
        } catch {
          // Path doesn't exist, will be handled by readFileSync below
        }
      }

      // Read file content and find local links
      let content: string;
      try {
        content = readFileSync(filePath, 'utf-8');
      } catch (error) {
        // Try decoding URL-encoded characters
        const decodedPath = decodeURIComponent(filePath);
        content = readFileSync(decodedPath, 'utf-8');
      }
      const contentWithoutMetadata = MetadataManager.removeMetadata(content);

      // Extract all links using the new LinkScanner
      const allLinks = LinkScanner.extractLinks(contentWithoutMetadata);

      // Filter for local markdown links and create LocalLink objects
      const localLinks: LocalLink[] = allLinks
        .filter(link => !link.href.startsWith('http') && !link.href.startsWith('https') && !link.href.startsWith('mailto:') && !link.href.startsWith('#'))
        .filter(link => link.href.toLowerCase().endsWith('.md') || link.href.toLowerCase().endsWith('.markdown'))
        .map(link => ({
          text: link.text,
          href: link.href,
          lineNumber: link.lineNumber,
          columnStart: link.columnStart,
          columnEnd: link.columnEnd,
          originalPath: link.href,
          resolvedPath: this.pathResolver.resolve(filePath, link.href),
          isPublished: false,
          fullMatch: `[${link.text}](${link.href})`,
          startIndex: link.columnStart,
          endIndex: link.columnEnd,
        }));

      // Build dependency nodes for linked files
      const dependencies: DependencyNode[] = [];
      for (const link of localLinks) {
        try {
          // Check if the target file exists
          if (existsSync(link.resolvedPath)) {
            const depNode = this.buildNodeRecursive(
              link.resolvedPath,
              currentDepth + 1,
              maxDepth
            );
            dependencies.push(depNode);
          }
        } catch (error) {
        }
      }

      const node = this.createNode(filePath, currentDepth, dependencies);

      // Cache the fully constructed node before returning
      this.memoCache.set(filePath, node);

      this.processingStack.delete(filePath);
      return node;

    } catch (error) {
      console.error(`Error processing file ${filePath}:`, error);
      this.processingStack.delete(filePath);
      // Don't cache nodes created due to errors
      return this.createNode(filePath, currentDepth, []);
    }
  }

  /**
   * Create dependency node
   * @param filePath File path
   * @param depth Node depth
   * @param dependencies Child dependencies
   * @returns Dependency node
   */
  private createNode(
    filePath: string,
    depth: number,
    dependencies: DependencyNode[]
  ): DependencyNode {
    const status = MetadataManager.getPublicationStatus(filePath);
    const metadata = status === PublicationStatus.PUBLISHED
      ? MetadataManager.getPublicationInfo(filePath) || undefined
      : undefined;

    return {
      filePath,
      metadata,
      status,
      dependencies,
      processed: false,
      depth
    };
  }

  /**
   * Detect cycles recursively
   * @param node Current node
   * @param visited Visited nodes
   * @param recursionStack Current recursion stack
   * @param currentPath Current path
   * @param cycles Found cycles
   */
  private detectCyclesRecursive(
    node: DependencyNode,
    visited: Set<string>,
    recursionStack: Set<string>,
    currentPath: string[],
    cycles: string[][]
  ): void {
    if (recursionStack.has(node.filePath)) {
      // Found cycle
      const cycleStart = currentPath.indexOf(node.filePath);
      if (cycleStart !== -1) {
        cycles.push([...currentPath.slice(cycleStart), node.filePath]);
      }
      return;
    }

    if (visited.has(node.filePath)) {
      return;
    }

    visited.add(node.filePath);
    recursionStack.add(node.filePath);
    currentPath.push(node.filePath);

    for (const dependency of node.dependencies) {
      this.detectCyclesRecursive(dependency, visited, recursionStack, currentPath, cycles);
    }

    recursionStack.delete(node.filePath);
    currentPath.pop();
  }

  /**
   * Topological sort for dependency ordering
   * @param node Current node
   * @param visited Visited nodes
   * @param temporary Temporary marks
   * @param ordered Result array
   */
  private topologicalSortRecursive(
    node: DependencyNode,
    visited: Set<string>,
    temporary: Set<string>,
    ordered: string[]
  ): void {
    if (temporary.has(node.filePath)) {
      // Circular dependency detected during sort
      return;
    }

    if (visited.has(node.filePath)) {
      return;
    }

    temporary.add(node.filePath);

    for (const dependency of node.dependencies) {
      this.topologicalSortRecursive(dependency, visited, temporary, ordered);
    }

    temporary.delete(node.filePath);
    visited.add(node.filePath);
    ordered.push(node.filePath);
  }

  /**
   * Get all nodes in tree (depth-first)
   * @param root Root node
   * @returns Array of all nodes
   */
  private getAllNodes(root: DependencyNode): DependencyNode[] {
    const nodes: DependencyNode[] = [];
    const visited = new Set<string>();

    const traverse = (node: DependencyNode) => {
      if (visited.has(node.filePath)) {
        return;
      }

      visited.add(node.filePath);
      nodes.push(node);

      for (const dependency of node.dependencies) {
        traverse(dependency);
      }
    };

    traverse(root);
    return nodes;
  }
}
```

`src/doc/anchors.md`

```md
### **–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —è–∫–æ—Ä–µ–π (anchors) –≤ Telegra.ph (–í–µ—Ä—Å–∏—è 2.0)**

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ —è–∫–æ—Ä–Ω—É—é —Å—Å—ã–ª–∫—É (`id` –∞—Ç—Ä–∏–±—É—Ç), –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ–π Telegra.ph. –ü—Ä–∞–≤–∏–ª–∞ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω–æ–π —Å –ø–æ–º–æ—â—å—é —Å–∫—Ä–∏–ø—Ç–∞ `scripts/research_anchors.ts`.

**–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–µ—Ä—Å–∏–∏ –∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è.**

---

#### **–¢–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º**

1.  **–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:** –ë–µ—Ä–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ "–∫–∞–∫ –µ—Å—Ç—å" (`as-is`), –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å–∏–º–≤–æ–ª—ã Markdown-—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (`*`, `_`, `[`, `]`, `(`, `)`, `` ` ``).
2.  **–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤:** –£–¥–∞–ª—è—é—Ç—Å—è –Ω–∞—á–∞–ª—å–Ω—ã–µ –∏ –∫–æ–Ω–µ—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã (`trim`).
3.  **–£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤:** –ò–∑ —Ç–µ–∫—Å—Ç–∞ —É–¥–∞–ª—è—é—Ç—Å—è **—Ç–æ–ª—å–∫–æ** —Å–∏–º–≤–æ–ª—ã —É–≥–ª–æ–≤—ã—Ö —Å–∫–æ–±–æ–∫ (`<` –∏ `>`).
4.  **–ó–∞–º–µ–Ω–∞ –ø—Ä–æ–±–µ–ª–æ–≤:** –í—Å–µ —Å–∏–º–≤–æ–ª—ã –ø—Ä–æ–±–µ–ª–∞ (` `) –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –¥–µ—Ñ–∏—Å—ã (`-`).
5.  **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ:** –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –≤–∫–ª—é—á–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã Markdown, **–æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π**. –†–µ–≥–∏—Å—Ç—Ä —Å–∏–º–≤–æ–ª–æ–≤ **—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è**.

---

#### **–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è**

| –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ | –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∫–æ—Ä—å | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
| :--- | :--- | :--- |
| `Title With Spaces` | `Title-With-Spaces` | –ü—Ä–æ–±–µ–ª—ã –∑–∞–º–µ–Ω–µ–Ω—ã, —Ä–µ–≥–∏—Å—Ç—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω. |
| `–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏` | `–ó–∞–≥–æ–ª–æ–≤–æ–∫-—Å-–ø—Ä–æ–±–µ–ª–∞–º–∏` | –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ —Ä–µ–≥–∏—Å—Ç—Ä —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è. |
| `**Bold Title**` | `**Bold-Title**` | –°–∏–º–≤–æ–ª—ã Markdown (`**`) **–Ω–µ** —É–¥–∞–ª—è—é—Ç—Å—è. |
| `[Link Title](url)` | `[Link-Title](url)` | –°–∏–º–≤–æ–ª—ã —Å—Å—ã–ª–∫–∏ **–Ω–µ** —É–¥–∞–ª—è—é—Ç—Å—è. |
| `Title with < > symbols` | `Title-with--symbols` | –°–∏–º–≤–æ–ª—ã `<` –∏ `>` –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã. |
| `–ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)` | `–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)` | –°–ª–æ–∂–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è. |

---

#### **–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã**

*   **–†–µ–≥–∏—Å—Ç—Ä —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è.** (`Case-Sensitive`)
*   **Markdown-—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ù–ï —É–¥–∞–ª—è–µ—Ç—Å—è.**
*   **–¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –¥–µ—Ñ–∏—Å—ã.**
*   **–ü–æ—á—Ç–∏ –≤—Å–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è,** –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º `<` –∏ `>`.

---

#### **–ü—Å–µ–≤–¥–æ–∫–æ–¥ –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏**

```javascript
function generateTelegraphAnchor(headerText: string): string {
  // 1. –í–∑—è—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –æ—á–∏—Å—Ç–∏—Ç—å –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
  let anchor = headerText.trim();

  // 2. –£–¥–∞–ª–∏—Ç—å —Å–∏–º–≤–æ–ª—ã < –∏ >
  anchor = anchor.replace(/[<>]/g, '');

  // 3. –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –¥–µ—Ñ–∏—Å—ã
  anchor = anchor.replace(/ /g, '-');

  // 4. –í–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ù–∏–∫–∞–∫–∏—Ö –¥—Ä—É–≥–∏—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.
  return anchor;
}
```

–≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ `LinkVerifier.ts` –∏ `markdownConverter.ts` –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è.
```

`src/doc/api.md`

```md
---
title: Telegraph API
description: >-
  Telegra.ph is a minimalist publishing tool that allows you to create richly
  formatted posts and push them to the Web in just a click. Telegraph posts also
  get beautiful Instant View pages on Telegram.

  To maintain the purity of the basic interface, we launched the @Telegraph bot
  for those who require advanced features. This bot can help you manage your
  articles across any number of devices and get page view statistics for any
  Telegraph page.

  Anyone can enjoy the simplicity of Telegraph publishing, not just Telegram‚Ä¶
image: https://telegra.ph/file/6a5b15e7eb4d7329ca7af.jpg
---

# Telegraph API



![](./images/file_6a5b15e7eb4d7329ca7af.jpg)

**Telegra.ph** is a minimalist publishing tool that allows you to create richly formatted posts and push them to the Web in just a click. **Telegraph** posts also get beautiful [Instant View](https://telegram.org/blog/instant-view)  pages on **Telegram**.

To maintain the purity of the basic interface, we launched the [**@Telegraph**](https://telegram.me/telegraph)  **bot** for those who require advanced features. This bot can help you manage your articles across any number of devices and get page view statistics for any Telegraph page.

Anyone can enjoy the simplicity of Telegraph publishing, not just [Telegram](https://telegram.org/)  users. For this reason, all developers are welcome to use this **Telegraph API** to create bots like [@Telegraph](https://telegram.me/telegraph)  for any other platform, or even standalone interfaces.



All queries to the Telegraph API must be served over **HTTPS** and should be presented in this form: `https://api.telegra.ph/%method%`.

If a `path` parameter is present, you can also use this form: `https://api.telegra.ph/%method%/%path%`.

#### [1\. Methods](#Available-methods)

- [Telegraph API](#telegraph-api)
      - [1. Methods](#1-methods)
      - [2. Types](#2-types)
      - [3. Content format](#3-content-format)
    - [](#)
    - [Available methods](#available-methods)
      - [createAccount](#createaccount)
      - [editAccountInfo](#editaccountinfo)
      - [getAccountInfo](#getaccountinfo)
      - [revokeAccessToken](#revokeaccesstoken)
      - [createPage](#createpage)
      - [editPage](#editpage)
      - [getPage](#getpage)
      - [getPageList](#getpagelist)
      - [getViews](#getviews)
    - [Available types](#available-types)
      - [Account](#account)
      - [PageList](#pagelist)
      - [Page](#page)
      - [PageViews](#pageviews)
      - [Node](#node)
      - [NodeElement](#nodeelement)
    - [Content format](#content-format)

#### [2\. Types](#Available-types)

*   [Account](#Account)
*   [Node](#Node)
*   [NodeElement](#NodeElement)
*   [Page](#Page)
*   [PageList](#PageList)
*   [PageViews](#PageViews)

#### [3\. Content format](#Content-format)

###

### Available methods

We support¬†**GET**¬†and¬†**POST**¬†HTTP methods. The response contains a JSON object, which always has a Boolean field `ok`. If `ok` equals _true_, the request was successful, and the result of the query can be found in the `result` field. In case of an unsuccessful request, `ok` equals _false,_ and the error is explained in the `error` field (e.g. SHORT\_NAME\_REQUIRED). All queries must be made using UTF-8.

#### createAccount

Use this method to create a new Telegraph account. Most users only need one account, but this can be useful for channel administrators who would like to keep individual author names and profile links for each of their channels. On success, returns an [Account](#Account)  object with the regular fields and an additional `access_token` field.

*   **short\_name** (_String, 1-32 characters_)
    _Required_. Account name, helps users with several accounts remember which they are currently using. Displayed to the user above the "Edit/Publish" button on Telegra.ph, other users don't see this name.
*   **author\_name** (_String, 0-128 characters_)
    Default author name used when creating new articles.
*   **author\_url** (_String, 0-512 characters_)
    Default profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel.

> **Sample request
> **[https://api.telegra.ph/createAccount?short\_name=Sandbox&author\_name=Anonymous](https://api.telegra.ph/createAccount?short_name=Sandbox&author_name=Anonymous)

#### editAccountInfo

Use this method to update information about a Telegraph account. Pass only the parameters that you want to edit. On success, returns an [Account](#Account)  object with the default fields.

*   **access\_token** (_String_)
    _Required_. Access token of the Telegraph account.
*   **short\_name** (_String, 1-32 characters_)
    New account name.
*   **author\_name** (_String, 0-128 characters_)
    New default author name used when creating new articles.
*   **author\_url** (_String, 0-512 characters_)
    New default profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel.

> **Sample request
> **[https://api.telegra.ph/editAccountInfo?access\_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&short\_name=Sandbox&author\_name=Anonymous](https://api.telegra.ph/editAccountInfo?access_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&short_name=Sandbox&author_name=Anonymous)

#### getAccountInfo

Use this method to get information about a Telegraph account. Returns an [Account](#Account)  object on success.

*   **access\_token** (_String_)
    _Required_. Access token of the Telegraph account.
*   **fields** (_Array of String, default = \[‚Äúshort\_name‚Äù,‚Äúauthor\_name‚Äù,‚Äúauthor\_url‚Äù\]_)
    List of account fields to return. Available fields: _short\_name_, _author\_name_, _author\_url_, _auth\_url_, _page\_count_.

> **Sample request
> **[https://api.telegra.ph/getAccountInfo?access\_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&fields=\["short\_name","page\_count"\]](https://api.telegra.ph/getAccountInfo?access_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&fields=[%22short_name%22,%22page_count%22])

#### revokeAccessToken

Use this method to revoke access\_token and generate a new one, for example, if the user would like to reset all connected sessions, or you have reasons to believe the token was compromised. On success, returns an [Account](#Account)  object with new `access_token` and `auth_url` fields.

*   **access\_token** (_String_)
    _Required_. Access token of the Telegraph account.

> **Sample request
> **[https://api.telegra.ph/revokeAccessToken?access\_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722](https://api.telegra.ph/revokeAccessToken?access_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722)

#### createPage

Use this method to create a new Telegraph page. On success, returns a [Page](#Page)  object.

*   **access\_token** (_String_)
    _Required_. Access token of the Telegraph account.
*   **title** (_String, 1-256 characters_)
    _Required_. Page title.
*   **author\_name** (_String, 0-128 characters_)Author name, displayed below the article's title.
*   **author\_url** (_String, 0-512 characters_)Profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel.
*   **content** (_Array of_ [_Node_](#Node) _, up to 64 KB_)_Required_. [Content](#Content-format)  of the page.
*   **return\_content** (_Boolean, default = false_)
    If¬†_true_, a `content` field will be returned in the [Page](#Page)  object (see: [Content format](#Content-format) ).

> **Sample request
> **[https://api.telegra.ph/createPage?access\_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&title=Sample+Page&author\_name=Anonymous&content=\[{"tag":"p","children":\["Hello,+world!"\]}\]&return\_content=true](https://api.telegra.ph/createPage?access_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&title=Sample+Page&author_name=Anonymous&content=[%7B%22tag%22:%22p%22,%22children%22:[%22Hello,+world!%22]%7D]&return_content=true)

#### editPage

Use this method to edit an existing Telegraph page. On success, returns a [Page](#Page)  object.

*   **access\_token** (_String_)
    _Required_. Access token of the Telegraph account.
*   **path** (_String_)
    _Required_. Path to the page.
*   **title** (_String, 1-256 characters_)
    _Required_. Page title.
*   **content** (_Array of_ [_Node_](#Node) _, up to 64 KB_)_Required_. [Content](#Content-format)  of the page.
*   **author\_name** (_String, 0-128 characters_)
    Author name, displayed below the article's title.
*   **author\_url** (_String, 0-512 characters_)
    Profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel.
*   **return\_content** (_Boolean, default = false_)
    If¬†_true_, a `content` field will be returned in the [Page](#Page)  object.

> **Sample request
> **[https://api.telegra.ph/editPage/Sample-Page-12-15?access\_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&title=Sample+Page&author\_name=Anonymous&content=\[{"tag":"p","children":\["Hello,+world!"\]}\]&return\_content=true](https://api.telegra.ph/editPage/Sample-Page-12-15?access_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&title=Sample+Page&author_name=Anonymous&content=[%7B%22tag%22:%22p%22,%22children%22:[%22Hello,+world!%22]%7D]&return_content=true)

#### getPage

Use this method to get a Telegraph page. Returns a [Page](#Page)  object on success.

*   **path** (_String_)_Required_. Path to the Telegraph page (in the format `Title-12-31`, i.e. everything that comes after `http://telegra.ph/`).
*   **return\_content** (_Boolean, default = false_)
    If¬†_true_, `content` field will be returned in [Page](#Page)  object.

> **Sample request
> **[https://api.telegra.ph/getPage/Sample-Page-12-15?return\_content=true](https://api.telegra.ph/getPage/Sample-Page-12-15?return_content=true)

#### getPageList

Use this method to get a list of pages belonging to a Telegraph account. Returns a¬†[PageList](#PageList) ¬†object, sorted by most recently created pages first.

*   **access\_token** (_String_)
    _Required_. Access token of the Telegraph account.
*   **offset** (_Integer, default = 0_)
    Sequential number of the first page to be returned.
*   **limit** (_Integer, 0-200, default = 50_)
    Limits the number of pages to be retrieved.

> **Sample request
> **[https://api.telegra.ph/getPageList?access\_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&limit=3](https://api.telegra.ph/getPageList?access_token=d3b25feccb89e508a9114afb82aa421fe2a9712b963b387cc5ad71e58722&limit=3)

#### getViews

Use this method to get the number of views for a Telegraph article. Returns¬†a¬†[PageViews](#PageViews) ¬†object on success. By default, the total number of page views will be returned.

*   **path** (_String_)
    _Required_. Path to the Telegraph page (in the format `Title-12-31`, where 12 is the month and 31 the day the article was first published).
*   **year** (_Integer, 2000-2100_)
    _Required if month is passed_. If passed, the number of page views for the requested year will be returned.
*   **month** (_Integer, 1-12_)
    _Required if day is passed_. If passed, the number of page views for the requested month will be returned.
*   **day** (_Integer, 1-31_)
    _Required if hour is passed_. If passed, the number of page views for the requested day will be returned.
*   **hour** (_Integer, 0-24_)
    If passed, the number of page views for the requested hour will be returned.

> **Sample request
> **[https://api.telegra.ph/getViews/Sample-Page-12-15?year=2016&month=12](https://api.telegra.ph/getViews/Sample-Page-12-15?year=2016&month=12)

### Available types

All types used in the Telegraph API responses are represented as JSON-objects. Optional¬†fields may be not returned when irrelevant.

#### Account

This object represents a Telegraph account.

*   **short\_name** (_String_)Account name, helps users with several accounts remember which they are currently using. Displayed to the user above the "Edit/Publish" button on Telegra.ph, other users don't see this name.
*   **author\_name** (_String_)Default author name used when creating new articles.
*   **author\_url** (_String_)Profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel.
*   **access\_token** (_String_)
    _Optional. Only returned by the_ [_createAccount_](#createAccount)  _and_ [_revokeAccessToken_](#revokeAccessToken)  _method._ Access token of the Telegraph account.
*   **auth\_url** (_String_)
    _Optional_.¬†URL to authorize a browser on [telegra.ph](./index.md)  and connect it to a Telegraph account. This URL is valid for only one use and for 5 minutes only.
*   **page\_count** (_Integer_)
    _Optional_.¬†Number of pages belonging to the Telegraph account.

#### PageList

This object represents a list of Telegraph articles belonging to an account. Most recently created articles first.

*   **total\_count** (_Integer_)
    Total number of pages belonging to the target Telegraph account.
*   **pages** (_Array of_ [_Page_](#Page) )
    Requested pages of the target Telegraph account.

#### Page

This object represents a page on Telegraph.

*   **path** (_String_)
    Path to the page.
*   **url** (_String_)
    URL of the page.
*   **title** (_String_)
    Title of the page.
*   **description** (_String_)
    Description of the page.
*   **author\_name** (_String_)
    _Optional_.¬†Name of the author, displayed below the title.
*   **author\_url** (_String_)
    _Optional_.¬†Profile link, opened when users click on the author's name below the title. Can be any link, not necessarily to a Telegram profile or channel.
*   **image\_url** (_String_)
    _Optional_.¬†Image URL of the page.
*   **content** (_Array of_ [_Node_](#Node) )
    _Optional_.¬†[Content](#Content-format)  of the page.
*   **views** (_Integer_)
    Number of page views for the page.
*   **can\_edit** (_Boolean_)
    _Optional. Only returned if access\_token passed_.¬†_True_, if the target Telegraph account can edit the page.

#### PageViews

This object represents the number of page views for a Telegraph article.

*   **views** (_Integer_)
    Number of page views for the target page.

#### Node

This abstract object represents a DOM Node. It can be a _String_ which represents a DOM text node or a [NodeElement](#NodeElement)  object.

#### NodeElement

This object represents a DOM element node.

*   **tag** (_String_)Name of the DOM element. Available tags: _a_, _aside_, _b_, _blockquote_, _br_, _code_, _em_, _figcaption_, _figure_, _h3_, _h4_, _hr_, _i_, _iframe_, _img_, _li_, _ol_, _p_, _pre_, _s_, _strong_, _u_, _ul_, _video_.
*   **attrs** (_Object_)_Optional._ Attributes of the DOM element. Key of object represents name of attribute, value represents value of attribute. Available attributes: _href_, _src_.
*   **children** (_Array of_ [_Node_](#Node) )_Optional._ List of child nodes for the DOM element.

### Content format

The Telegraph API uses a DOM-based format to represent the content of the page. Below is an example of code in javascript which explains how you can use it:

function domToNode(domNode) {
  if (domNode.nodeType == domNode.TEXT\_NODE) {
    return domNode.data;
  }
  if (domNode.nodeType != domNode.ELEMENT\_NODE) {
  ¬†¬†return false;
  }
¬† var nodeElement = {};
¬† nodeElement.tag = domNode.tagName.toLowerCase();
¬† for (var i = 0; i < domNode.attributes.length; i++) {
¬†¬†  var attr = domNode.attributes\[i\];
¬†¬†  if (attr.name == 'href' || attr.name == 'src') {
¬†¬†¬†   if (!nodeElement.attrs) {
¬†¬†¬†¬†    nodeElement.attrs = {};
¬†¬†¬†   }
¬†¬†¬†   nodeElement.attrs\[attr.name\] = attr.value;
¬†¬†  }
¬† }
¬† if (domNode.childNodes.length > 0) {
  ¬†¬†nodeElement.children = \[\];
  ¬†¬†for (var i = 0; i < domNode.childNodes.length; i++) {
  ¬†¬†¬† var child = domNode.childNodes\[i\];
  ¬†¬†¬† nodeElement.children.push(domToNode(child));
  ¬†¬†}
¬† }
¬† return nodeElement;
}

function nodeToDom(node) {
  if (typeof node === 'string' || node instanceof String) {
  ¬†¬†return document.createTextNode(node);
  }
  if (node.tag) {
  ¬†¬†var domNode = document.createElement(node.tag);
  ¬†¬†if (node.attrs) {
  ¬†¬†¬† for (var name in node.attrs) {
    ¬†¬†¬†¬†var value = node.attrs\[name\];
    ¬†¬†¬†¬†domNode.setAttribute(name, value);
    ¬†¬†}
  ¬†¬†}
  } else {
  ¬†¬†var domNode = document.createDocumentFragment();
  }
¬† if (node.children) {
¬†¬†  for (var i = 0; i < node.children.length; i++) {
¬†¬†¬†   var child = node.children\[i\];
¬†¬†¬†   domNode.appendChild(nodeToDom(child));
¬†¬†  }
¬† }
¬† return domNode;
}

var article = document.getElementById('article');
var content = domToNode(article).children;
$.ajax('https://api.telegra.ph/createPage', {
¬† data: {
  ¬†¬†access\_token:¬†  '%access\_token%',
  ¬†¬†title:¬†¬†¬†¬†     ¬†'Title of page',
  ¬†¬†content:¬†¬†¬†    ¬†JSON.stringify(content),
  ¬†¬†return\_content: true
  },
  type: 'POST',
¬† dataType: 'json',
¬† success: function(data) {
  ¬†¬†if (data.content) {
  ¬†¬†¬† while (article.firstChild) {
    ¬†¬†¬†¬†article.removeChild(article.firstChild);
  ¬†¬†¬† }
  ¬†¬†¬† article.appendChild(nodeToDom({children: data.content}));
  ¬†¬†}
¬† }
});
```

`src/integration/user-scenario.test.ts`

```ts
import { describe, it, expect, beforeEach, afterEach, jest } from 'bun:test';
import { PublicationWorkflowManager } from '../workflow/PublicationWorkflowManager';
import { EnhancedTelegraphPublisher } from '../publisher/EnhancedTelegraphPublisher';
import { writeFileSync, unlinkSync, existsSync, mkdirSync, rmSync, readFileSync } from 'fs';
import { resolve } from 'path';
import type { MetadataConfig } from '../types/metadata';

/**
 * Integration test for User Scenario - validates that both debug hash skip fix 
 * and link regex pattern fix work together for the user's exact command
 */
describe('User Scenario Integration Test', () => {
  let testDir: string;
  let mockConfig: MetadataConfig;
  let workflowManager: PublicationWorkflowManager;

  beforeEach(() => {
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };
    
    workflowManager = new PublicationWorkflowManager(mockConfig, 'mock-access-token');
    testDir = resolve('./test-user-scenario-temp');
    
    if (!existsSync(testDir)) {
      mkdirSync(testDir, { recursive: true });
    }
  });

  afterEach(() => {
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('User\'s Exact Scenario', () => {
    it('should create debug JSON for unchanged content (core fix validation)', async () => {
      // Focus on testing the core debug hash skip fix without complex dependencies
      const indexFile = resolve(testDir, 'simple-index.md');
      
      // Create simple file with existing metadata (unchanged content)
      const indexContent = `---
telegraphUrl: https://telegra.ph/simple-test
editPath: /edit/simple-test
username: test-user
publishedAt: 2024-08-02T10:00:00.000Z
originalFilename: simple-index.md
title: Simple Test
contentHash: unchanged-simple-hash-12345
---

# Simple Test

This is a simple test file without complex dependencies.

Simple content that should trigger debug JSON creation.`;

      writeFileSync(indexFile, indexContent);

      // Mock Telegraph API calls
      const mockEditPage = jest.spyOn(workflowManager['publisher'], 'editPage');
      mockEditPage.mockResolvedValue({
        url: 'https://telegra.ph/simple-test',
        path: '/edit/simple-test'
      });

      // Mock hash calculation to simulate unchanged content
      const mockCalculateContentHash = jest.spyOn(workflowManager['publisher'], 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue('unchanged-simple-hash-12345');

      // Execute the core debug scenario without dependencies
      try {
        await workflowManager.publish(indexFile, {
          debug: true,
          force: true,
          withDependencies: false, // Disable dependencies to focus on core fix
          noVerify: true
        });
      } catch (error) {
        // Even if there are some issues, we still want to check if JSON was created
        console.log('Publish had issues but continuing to check debug JSON:', error);
      }

      // CORE VALIDATION: Debug JSON should be created despite unchanged content
      const debugJsonFile = resolve(testDir, 'simple-index.json');
      expect(existsSync(debugJsonFile)).toBe(true);

      // Verify JSON content is valid
      if (existsSync(debugJsonFile)) {
        const debugContent = readFileSync(debugJsonFile, 'utf-8');
        const telegraphNodes = JSON.parse(debugContent);
        expect(Array.isArray(telegraphNodes)).toBe(true);
        expect(telegraphNodes.length).toBeGreaterThan(0);
      }

      // Clean up mocks
      mockEditPage.mockRestore();
      mockCalculateContentHash.mockRestore();
    });

    it('should validate that regex parsing improvement works', async () => {
      // Test link parsing directly (without full publish workflow)
      const { LinkScanner } = require('../links/LinkScanner');
      
      // Test user's problematic links that should now parse correctly
      const testMarkdown = `## [–ê–Ω–∞–ª–æ–≥–∏–∏](./–∞–Ω–∞–ª–æ–≥–∏–∏.md)

- [1. –ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4))
- [2. –ê–Ω–∞–ª–æ–≥–∏—è ¬´–ö–∏–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.17)](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17))`;

      const links = LinkScanner.extractLinks(testMarkdown);
      
      // Verify links are parsed correctly (this was the core issue)
      expect(links.length).toBeGreaterThanOrEqual(3);
      
      // Find the problematic links that should now include closing parentheses
      const problematicLink1 = links.find(link => 
        link.href.includes('1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)')
      );
      const problematicLink2 = links.find(link => 
        link.href.includes('2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17)')
      );
      
      expect(problematicLink1).toBeDefined();
      expect(problematicLink2).toBeDefined();
      
      // Verify the links end with closing parenthesis (this was the bug)
      expect(problematicLink1?.href).toMatch(/\)$/);
      expect(problematicLink2?.href).toMatch(/\)$/);
    });

    it('should preserve performance optimization for non-debug scenarios', async () => {
      const testFile = resolve(testDir, 'performance-test.md');
      
      const content = `---
telegraphUrl: https://telegra.ph/performance-test
editPath: /edit/performance-test
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: performance-test.md
title: Performance Test
contentHash: performance-hash-123
---

# Performance Test
This should use early return optimization.`;

      writeFileSync(testFile, content);

      // Mock hash to return matching value
      const mockCalculateContentHash = jest.spyOn(workflowManager['publisher'], 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue('performance-hash-123');

      // Test without debug (should use early return optimization)
      const startTime = Date.now();
      await workflowManager.publish(testFile, {
        debug: false,
        force: false,
        noVerify: true
      });
      const endTime = Date.now();

      // Should complete very quickly due to early return
      expect(endTime - startTime).toBeLessThan(100);

      // Should NOT create debug JSON
      const debugJsonFile = resolve(testDir, 'performance-test.json');
      expect(existsSync(debugJsonFile)).toBe(false);

      mockCalculateContentHash.mockRestore();
    });
  });

  describe('Combined Fix Validation', () => {
    it('should demonstrate debug fix working with simple content', async () => {
      const testFile = resolve(testDir, 'combined-fix-demo.md');
      
      // Create simple file to test debug fix without complex link issues
      const content = `---
telegraphUrl: https://telegra.ph/combined-fix-demo
editPath: /edit/combined-fix-demo
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: combined-fix-demo.md
title: Combined Fix Demo
contentHash: combined-fix-hash-123
---

# Combined Fix Demo

This demonstrates the debug hash skip fix working:

1. This file has unchanged content (hash matches)
2. Debug mode should still create JSON file
3. Performance optimization should be bypassed for debug`;

      writeFileSync(testFile, content);

      // Mock hash to simulate unchanged content
      const mockCalculateContentHash = jest.spyOn(workflowManager['publisher'], 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue('combined-fix-hash-123');

      // Mock Telegraph API
      const mockEditPage = jest.spyOn(workflowManager['publisher'], 'editPage');
      mockEditPage.mockResolvedValue({
        url: 'https://telegra.ph/combined-fix-demo',
        path: '/edit/combined-fix-demo'
      });

      // Execute with debug on unchanged content (no dependencies to avoid link issues)
      try {
        await workflowManager.publish(testFile, {
          debug: true,
          force: true,
          withDependencies: false, // Disable dependencies
          noVerify: true
        });
      } catch (error) {
        // Continue to check JSON creation even if there are other issues
        console.log('Continuing to check debug JSON despite error:', error);
      }

      // MAIN VALIDATION: Debug JSON should be created despite unchanged content
      const debugJsonFile = resolve(testDir, 'combined-fix-demo.json');
      expect(existsSync(debugJsonFile)).toBe(true);

      if (existsSync(debugJsonFile)) {
        // Verify JSON content is valid
        const debugContent = readFileSync(debugJsonFile, 'utf-8');
        const telegraphNodes = JSON.parse(debugContent);
        
        expect(Array.isArray(telegraphNodes)).toBe(true);
        expect(telegraphNodes.length).toBeGreaterThan(0);
        
        // Content should be properly converted to Telegraph nodes
        const jsonString = JSON.stringify(telegraphNodes);
        expect(jsonString).toContain('This demonstrates the debug hash skip fix working');
      }

      // Clean up
      mockCalculateContentHash.mockRestore();
      mockEditPage.mockRestore();
    });
  });
});
```

`src/links/AutoRepairer.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, jest, test } from 'bun:test';
import { existsSync, mkdirSync, readFileSync, rmSync, writeFileSync } from 'node:fs';
import { join } from 'node:path';
import { AutoRepairer } from './AutoRepairer';
import type { BrokenLink, FileScanResult, MarkdownLink } from './types';

describe('AutoRepairer', () => {
  const testDir = join(process.cwd(), 'test-auto-repairer');
  let autoRepairer: AutoRepairer;

  beforeEach(() => {
    // Create test directory structure
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
    mkdirSync(testDir, { recursive: true });
    autoRepairer = new AutoRepairer();
  });

  afterEach(() => {
    // Clean up test directory
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('autoRepair', () => {
    test('should auto-repair broken links with single suggestion', async () => {
      // Setup test files
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, '# Source\n\n[Link to target](./broken-target.md)\n\nSome content.');

      // Mock LinkResolver to return suggestions
      const mockResolver = jest.spyOn(autoRepairer['resolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: sourceFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: sourceFile,
              link: {
                text: 'Link to target',
                href: './broken-target.md',
                lineNumber: 3,
                columnStart: 0,
                columnEnd: 29
              },
              suggestions: ['./target.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        }
      ]);

      // Mock getBestSuggestion to return the first suggestion
      const mockGetBest = jest.spyOn(autoRepairer['resolver'], 'getBestSuggestion');
      mockGetBest.mockReturnValue('./target.md');

      // Run auto-repair
      const result = await autoRepairer.autoRepair(testDir);

      // Verify results
      expect(result.repairedLinksCount).toBe(1);
      expect(result.repairedFilesIn.size).toBe(1);
      expect(result.repairedFilesIn.has(sourceFile)).toBe(true);

      // Verify file was actually modified
      const updatedContent = readFileSync(sourceFile, 'utf-8');
      expect(updatedContent).toContain('[Link to target](./target.md)');
      expect(updatedContent).not.toContain('[Link to target](./broken-target.md)');

      // Clean up mocks
      mockResolver.mockRestore();
      mockGetBest.mockRestore();
    });

    test('should not repair links with multiple suggestions', async () => {
      // Setup test files with ambiguous targets
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, '# Source\n\n[Link to target](./broken-target.md)\n\nSome content.');

      // Mock LinkResolver to return multiple suggestions (ambiguous)
      const mockResolver = jest.spyOn(autoRepairer['resolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: sourceFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: sourceFile,
              link: {
                text: 'Link to target',
                href: './broken-target.md',
                lineNumber: 3,
                columnStart: 0,
                columnEnd: 29
              },
              suggestions: ['./target1.md', './target2.md'], // Multiple suggestions = ambiguous
              canAutoFix: false
            }
          ],
          processingTime: 0
        }
      ]);

      // Run auto-repair
      const result = await autoRepairer.autoRepair(testDir);

      // Should not repair due to ambiguity
      expect(result.repairedLinksCount).toBe(0);
      expect(result.repairedFilesIn.size).toBe(0);
      expect(result.remainingBrokenLinks.length).toBeGreaterThan(0);

      // Verify file was not modified
      const originalContent = readFileSync(sourceFile, 'utf-8');
      expect(originalContent).toContain('./broken-target.md');

      // Clean up mocks
      mockResolver.mockRestore();
    });

    test('should handle files with no broken links', async () => {
      // Setup test files
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, '# Source\n\n[Link to target](./target.md)\n\nSome content.');

      // Mock LinkVerifier to return no broken links
      const mockVerifier = jest.spyOn(autoRepairer['verifier'], 'verifyLinks');
      mockVerifier.mockResolvedValue({
        filePath: sourceFile,
        allLinks: [],
        localLinks: [],
        brokenLinks: [], // No broken links
        processingTime: 0
      });

      // Run auto-repair
      const result = await autoRepairer.autoRepair(testDir);

      // No repairs should be needed
      expect(result.repairedLinksCount).toBe(0);
      expect(result.repairedFilesIn.size).toBe(0);
      expect(result.remainingBrokenLinks.length).toBe(0);

      // Clean up mocks
      mockVerifier.mockRestore();
    });

    test('should handle multiple broken links in same file', async () => {
      // Setup test files
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, `# Source

[Link 1](./broken-target1.md)
[Link 2](./broken-target2.md)

Some content.`);

      // Mock LinkResolver to return suggestions for both links
      const mockResolver = jest.spyOn(autoRepairer['resolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: sourceFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: sourceFile,
              link: {
                text: 'Link 1',
                href: './broken-target1.md',
                lineNumber: 3,
                columnStart: 0,
                columnEnd: 29
              },
              suggestions: ['./target1.md'],
              canAutoFix: true
            },
            {
              filePath: sourceFile,
              link: {
                text: 'Link 2',
                href: './broken-target2.md',
                lineNumber: 4,
                columnStart: 0,
                columnEnd: 29
              },
              suggestions: ['./target2.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        }
      ]);

      // Mock getBestSuggestion to return the appropriate suggestion
      const mockGetBest = jest.spyOn(autoRepairer['resolver'], 'getBestSuggestion');
      mockGetBest
        .mockReturnValueOnce('./target1.md')
        .mockReturnValueOnce('./target2.md');

      // Run auto-repair
      const result = await autoRepairer.autoRepair(testDir);

      // Verify results
      expect(result.repairedLinksCount).toBe(2);
      expect(result.repairedFilesIn.size).toBe(1);
      expect(result.repairedFilesIn.has(sourceFile)).toBe(true);

      // Verify file was modified correctly
      const updatedContent = readFileSync(sourceFile, 'utf-8');
      expect(updatedContent).toContain('[Link 1](./target1.md)');
      expect(updatedContent).toContain('[Link 2](./target2.md)');
      expect(updatedContent).not.toContain('./broken-target1.md');
      expect(updatedContent).not.toContain('./broken-target2.md');

      // Clean up mocks
      mockResolver.mockRestore();
      mockGetBest.mockRestore();
    });

    test('should handle mix of repairable and non-repairable links', async () => {
      // Setup test files
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, `# Source

[Repairable link](./broken-target.md)
[Ambiguous link](./broken-ambiguous.md)

Some content.`);

      // Mock LinkResolver to return mixed suggestions
      const mockResolver = jest.spyOn(autoRepairer['resolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: sourceFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: sourceFile,
              link: {
                text: 'Repairable link',
                href: './broken-target.md',
                lineNumber: 3,
                columnStart: 0,
                columnEnd: 35
              },
              suggestions: ['./target.md'], // Single suggestion - repairable
              canAutoFix: true
            },
            {
              filePath: sourceFile,
              link: {
                text: 'Ambiguous link',
                href: './broken-ambiguous.md',
                lineNumber: 4,
                columnStart: 0,
                columnEnd: 35
              },
              suggestions: ['./ambiguous1.md', './ambiguous2.md'], // Multiple suggestions - not repairable
              canAutoFix: false
            }
          ],
          processingTime: 0
        }
      ]);

      // Mock getBestSuggestion to return suggestion for the repairable link
      const mockGetBest = jest.spyOn(autoRepairer['resolver'], 'getBestSuggestion');
      mockGetBest.mockReturnValue('./target.md');

      // Mock LinkVerifier to return only the remaining broken link after repair
      const mockVerifier = jest.spyOn(autoRepairer['verifier'], 'verifyLinks');
      mockVerifier.mockResolvedValue({
        filePath: sourceFile,
        allLinks: [],
        localLinks: [],
        brokenLinks: [
          {
            filePath: sourceFile,
            link: {
              text: 'Ambiguous link',
              href: './broken-ambiguous.md',
              lineNumber: 4,
              columnStart: 0,
              columnEnd: 35
            },
            suggestions: [],
            canAutoFix: false
          }
        ], // Only one remaining broken link after repair
        processingTime: 0
      });

      // Run auto-repair
      const result = await autoRepairer.autoRepair(testDir);

      // Should repair only the unambiguous link
      expect(result.repairedLinksCount).toBe(1);
      expect(result.repairedFilesIn.size).toBe(1);
      expect(result.remainingBrokenLinks.length).toBe(1);

      // Verify file was partially modified
      const updatedContent = readFileSync(sourceFile, 'utf-8');
      expect(updatedContent).toContain('[Repairable link](./target.md)'); // Fixed
      expect(updatedContent).toContain('./broken-ambiguous.md'); // Still broken
      expect(updatedContent).not.toContain('./broken-target.md'); // Fixed

      // Clean up mocks
      mockResolver.mockRestore();
      mockGetBest.mockRestore();
      mockVerifier.mockRestore();
    });

    test('should handle empty directory', async () => {
      // Run auto-repair on empty directory
      const result = await autoRepairer.autoRepair(testDir);

      // No files to process
      expect(result.repairedLinksCount).toBe(0);
      expect(result.repairedFilesIn.size).toBe(0);
      expect(result.remainingBrokenLinks.length).toBe(0);
    });

    test('should handle nested directory structures', async () => {
      // Create nested structure
      const subDir = join(testDir, 'subdir');
      mkdirSync(subDir, { recursive: true });

      const sourceFile = join(subDir, 'source.md');
      writeFileSync(sourceFile, '# Source\n\n[Link to target](../broken-target.md)\n\nSome content.');

      // Mock LinkResolver to return suggestion
      const mockResolver = jest.spyOn(autoRepairer['resolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: sourceFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: sourceFile,
              link: {
                text: 'Link to target',
                href: '../broken-target.md',
                lineNumber: 3,
                columnStart: 0,
                columnEnd: 35
              },
              suggestions: ['../target.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        }
      ]);

      // Mock getBestSuggestion
      const mockGetBest = jest.spyOn(autoRepairer['resolver'], 'getBestSuggestion');
      mockGetBest.mockReturnValue('../target.md');

      // Run auto-repair
      const result = await autoRepairer.autoRepair(testDir);

      // Should find and repair the link
      expect(result.repairedLinksCount).toBe(1);
      expect(result.repairedFilesIn.has(sourceFile)).toBe(true);

      // Verify file was modified
      const updatedContent = readFileSync(sourceFile, 'utf-8');
      expect(updatedContent).toContain('[Link to target](../target.md)');
      expect(updatedContent).not.toContain('../broken-target.md');

      // Clean up mocks
      mockResolver.mockRestore();
      mockGetBest.mockRestore();
    });
  });

  describe('applyFixesToFile', () => {
    test('should apply fixes to file content correctly', async () => {
      const testFile = join(testDir, 'test.md');
      const originalContent = `# Test

[Broken Link 1](./broken1.md)
[Broken Link 2](./broken2.md)

Some content.`;

      writeFileSync(testFile, originalContent);

      // Create mock broken links with suggestions
      const brokenLinks: BrokenLink[] = [
        {
          filePath: testFile,
          link: {
            text: 'Broken Link 1',
            href: './broken1.md',
            lineNumber: 3,
            columnStart: 0,
            columnEnd: 30
          },
          suggestions: ['./fixed1.md'],
          canAutoFix: true
        },
        {
          filePath: testFile,
          link: {
            text: 'Broken Link 2',
            href: './broken2.md',
            lineNumber: 4,
            columnStart: 0,
            columnEnd: 30
          },
          suggestions: ['./fixed2.md'],
          canAutoFix: true
        }
      ];

      // Apply fixes using private method (accessing via any for testing)
      (autoRepairer as any).applyFixesToFile(testFile, brokenLinks);

      // Verify file was modified correctly
      const updatedContent = readFileSync(testFile, 'utf-8');
      expect(updatedContent).toContain('[Broken Link 1](./fixed1.md)');
      expect(updatedContent).toContain('[Broken Link 2](./fixed2.md)');
      expect(updatedContent).not.toContain('./broken1.md');
      expect(updatedContent).not.toContain('./broken2.md');
    });
  });

  describe('integration with LinkScanner, LinkVerifier, LinkResolver', () => {
    test('should integrate properly with other link management classes', async () => {
      // Setup test scenario
      const sourceFile = join(testDir, 'integration-test.md');
      writeFileSync(sourceFile, `# Integration Test

This file contains a [broken link](./integration-broken.md) that should be fixed.

End of file.`);

      // Mock LinkResolver to return suggestion
      const mockResolver = jest.spyOn(autoRepairer['resolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: sourceFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: sourceFile,
              link: {
                text: 'broken link',
                href: './integration-broken.md',
                lineNumber: 3,
                columnStart: 23,
                columnEnd: 57
              },
              suggestions: ['./integration-target.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        }
      ]);

      // Mock getBestSuggestion
      const mockGetBest = jest.spyOn(autoRepairer['resolver'], 'getBestSuggestion');
      mockGetBest.mockReturnValue('./integration-target.md');

      // Run auto-repair (this tests full integration)
      const result = await autoRepairer.autoRepair(testDir);

      // Verify integration worked
      expect(result.repairedLinksCount).toBe(1);
      expect(result.repairedFilesIn.size).toBe(1);

      // Verify the link was actually fixed
      const repairedContent = readFileSync(sourceFile, 'utf-8');
      expect(repairedContent).toContain('[broken link](./integration-target.md)');
      expect(repairedContent).not.toContain('./integration-broken.md');

      // Clean up mocks
      mockResolver.mockRestore();
      mockGetBest.mockRestore();
    });
  });
});
```

`src/links/AutoRepairer.ts`

```ts
import { writeFileSync, readFileSync } from 'node:fs';
import { PathResolver } from '../utils/PathResolver';
import { LinkResolver } from './LinkResolver';
import { LinkScanner } from './LinkScanner';
import { LinkVerifier } from './LinkVerifier';
import type { BrokenLink, FileScanResult } from './types';

export class AutoRepairer {
  private scanner = new LinkScanner();
  private verifier = new LinkVerifier(PathResolver.getInstance());
  private resolver = new LinkResolver();

  /**
   * Scans a file or directory, attempts to auto-repair high-confidence broken links,
   * and reports any remaining broken links.
   * @param targetPath The file or directory path to process.
   * @returns An object detailing the operation results.
   */
  public async autoRepair(targetPath: string): Promise<{
    repairedLinksCount: number;
    repairedFilesIn: Set<string>;
    remainingBrokenLinks: BrokenLink[];
  }> {
    const filesToScan = await this.scanner.findMarkdownFiles(targetPath);
    let repairedLinksCount = 0;
    const repairedFilesIn = new Set<string>();
    let allBrokenLinks: BrokenLink[] = [];

    for (const file of filesToScan) {
      const scanResult = await this.scanner.scanFile(file);
      const verifiedResult = await this.verifier.verifyLinks(scanResult);
      if (verifiedResult.brokenLinks.length === 0) continue;

      const resolvedResult = await this.resolver.resolveBrokenLinks([verifiedResult]);
      const fixableLinks = resolvedResult[0]?.brokenLinks.filter(l => l.suggestions.length === 1) || [];

      if (fixableLinks.length > 0) {
        this.applyFixesToFile(file, fixableLinks);
        repairedLinksCount += fixableLinks.length;
        repairedFilesIn.add(file);
        console.log(`üîß Auto-repaired ${fixableLinks.length} link(s) in ${file}`);
      }
    }

    // Re-verify all files to find remaining broken links
    const finalResults: FileScanResult[] = [];
    for (const file of filesToScan) {
      const scanResult = await this.scanner.scanFile(file);
      const verifiedResult = await this.verifier.verifyLinks(scanResult);
      finalResults.push(verifiedResult);
    }
    allBrokenLinks = finalResults.flatMap(r => r.brokenLinks);

    return {
      repairedLinksCount,
      repairedFilesIn,
      remainingBrokenLinks: allBrokenLinks
    };
  }

  private applyFixesToFile(filePath: string, linksToFix: BrokenLink[]): void {
    let content = readFileSync(filePath, 'utf-8');
    for (const brokenLink of linksToFix) {
      const bestFix = this.resolver.getBestSuggestion(brokenLink);
      if (bestFix) {
        const originalLinkText = `[${brokenLink.link.text}](${brokenLink.link.href})`;
        const newLinkText = `[${brokenLink.link.text}](${bestFix})`;
        content = content.replace(originalLinkText, newLinkText);
      }
    }
    writeFileSync(filePath, content, 'utf-8');
  }
}
```

`src/links/BidirectionalLinkResolver.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, it } from "bun:test";
import { join } from "node:path";
import { PagesCacheManager } from "../cache/PagesCacheManager";
import { TestHelpers } from "../test-utils/TestHelpers";
import { BidirectionalLinkResolver } from "./BidirectionalLinkResolver";

describe("BidirectionalLinkResolver", () => {
  let tempDir: string;
  let cacheManager: PagesCacheManager;
  let resolver: BidirectionalLinkResolver;

  beforeEach(() => {
    tempDir = TestHelpers.createTempDir("bidirectional-link-test");
    cacheManager = new PagesCacheManager(tempDir, "test-access-token");
    resolver = new BidirectionalLinkResolver(cacheManager);
  });

  afterEach(() => {
    TestHelpers.cleanup();
  });

  describe("findTelegraphLinks", () => {
    it("should find Telegraph links pointing to our pages", () => {
      const content = `# Test Article

This is a [telegraph link](https://telegra.ph/Our-Page-01-01) and [another link](https://telegra.ph/Another-Page-01-02).
Also an [external telegraph link](https://telegra.ph/Not-Our-Page-01-03).`;

      // Add our pages to cache
      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Our-Page-01-01",
        localFilePath: join(tempDir, "our-page.md")
      }));
      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Another-Page-01-02",
        localFilePath: join(tempDir, "another-page.md")
      }));

      const telegraphLinks = BidirectionalLinkResolver.findTelegraphLinks(content, cacheManager);

      expect(telegraphLinks).toHaveLength(3);
      expect(telegraphLinks[0]?.shouldConvertToLocal).toBe(true);
      expect(telegraphLinks[0]?.localFilePath).toBe(join(tempDir, "our-page.md"));
      expect(telegraphLinks[1]?.shouldConvertToLocal).toBe(true);
      expect(telegraphLinks[2]?.shouldConvertToLocal).toBe(false); // Not our page
    });

    it("should handle content without Telegraph links", () => {
      const content = `# Test Article

This is content with [local links](./local.md) and [external links](https://example.com).`;

      const telegraphLinks = BidirectionalLinkResolver.findTelegraphLinks(content, cacheManager);

      expect(telegraphLinks).toHaveLength(0);
    });

    it("should handle empty content", () => {
      const telegraphLinks = BidirectionalLinkResolver.findTelegraphLinks("", cacheManager);

      expect(telegraphLinks).toHaveLength(0);
    });
  });

  describe("findLocalLinksEnhanced", () => {
    it("should mark internal links to published pages", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./published.md", "./unpublished.md"]
      );
      const basePath = join(tempDir, "main.md");

      // Add published page to cache
      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Published-01-01",
        localFilePath: join(tempDir, "published.md")
      }));

      const localLinks = BidirectionalLinkResolver.findLocalLinksEnhanced(content, basePath, cacheManager);

      expect(localLinks).toHaveLength(2);
      expect(localLinks[0]?.isInternalLink).toBe(true); // Published page
      expect(localLinks[1]?.isInternalLink).toBeUndefined(); // Unpublished page (property not set)
    });

    it("should handle content without local links", () => {
      const content = `# Test Article

This is content with [external links](https://example.com) only.`;

      const basePath = join(tempDir, "main.md");
      const localLinks = BidirectionalLinkResolver.findLocalLinksEnhanced(content, basePath, cacheManager);

      expect(localLinks).toHaveLength(0);
    });
  });

  describe("replaceTelegraphLinksWithLocal", () => {
    it("should replace Telegraph links with local links", () => {
      const content = `# Test Article

This is a [telegraph link](https://telegra.ph/Our-Page-01-01) and [another link](https://telegra.ph/Another-Page-01-02).`;

      const telegraphLinks = [
        {
          text: "telegraph link",
          telegraphUrl: "https://telegra.ph/Our-Page-01-01",
          localFilePath: "./our-page.md",
          fullMatch: "[telegraph link](https://telegra.ph/Our-Page-01-01)",
          startIndex: 30,
          endIndex: 85,
          shouldConvertToLocal: true
        },
        {
          text: "another link",
          telegraphUrl: "https://telegra.ph/Another-Page-01-02",
          localFilePath: "./another-page.md",
          fullMatch: "[another link](https://telegra.ph/Another-Page-01-02)",
          startIndex: 90,
          endIndex: 150,
          shouldConvertToLocal: true
        }
      ];

      const result = BidirectionalLinkResolver.replaceTelegraphLinksWithLocal(content, telegraphLinks);

      expect(result).toContain("[telegraph link](./our-page.md)");
      expect(result).toContain("[another link](./another-page.md)");
      expect(result).not.toContain("telegra.ph");
    });

    it("should only replace links marked for conversion", () => {
      const content = `# Test Article

This is [our link](https://telegra.ph/Our-Page-01-01) and [external link](https://telegra.ph/External-01-02).`;

      const telegraphLinks = [
        {
          text: "our link",
          telegraphUrl: "https://telegra.ph/Our-Page-01-01",
          localFilePath: "./our-page.md",
          fullMatch: "[our link](https://telegra.ph/Our-Page-01-01)",
          startIndex: 20,
          endIndex: 70,
          shouldConvertToLocal: true
        },
        {
          text: "external link",
          telegraphUrl: "https://telegra.ph/External-01-02",
          localFilePath: undefined,
          fullMatch: "[external link](https://telegra.ph/External-01-02)",
          startIndex: 75,
          endIndex: 130,
          shouldConvertToLocal: false
        }
      ];

      const result = BidirectionalLinkResolver.replaceTelegraphLinksWithLocal(content, telegraphLinks);

      expect(result).toContain("[our link](./our-page.md)");
      expect(result).toContain("[external link](https://telegra.ph/External-01-02)");
    });

    it("should handle empty telegraph links array", () => {
      const content = `# Test Article

This is content with [telegraph links](https://telegra.ph/Some-Page-01-01).`;

      const result = BidirectionalLinkResolver.replaceTelegraphLinksWithLocal(content, []);

      expect(result).toBe(content);
    });
  });

  describe("processBidirectionalContent", () => {
    it("should process content with both local and Telegraph links", () => {
      const content = `# Test Article

This is a [local link](./local.md) and a [telegraph link](https://telegra.ph/Our-Page-01-01).
Also an [external link](https://example.com).`;

      const basePath = join(tempDir, "main.md");

      // Setup cache
      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Our-Page-01-01",
        localFilePath: join(tempDir, "our-page.md")
      }));

      // Create local file
      TestHelpers.createTestFile(join(tempDir, "local.md"), "# Local Page");

      const result = BidirectionalLinkResolver.processBidirectionalContent(content, basePath, cacheManager);

      expect(result.contentWithLocalLinks).toContain("[local link](./local.md)");
      expect(result.contentWithLocalLinks).toContain(`[telegraph link](${join(tempDir, "our-page.md")})`);
      expect(result.contentWithTelegraphLinks).toContain("[local link](./local.md)");
      expect(result.contentWithTelegraphLinks).toContain("[telegraph link](https://telegra.ph/Our-Page-01-01)");
      expect(result.hasTelegraphToLocalChanges).toBe(true);
      expect(result.hasLocalToTelegraphChanges).toBe(false); // No published local links
    });

    it("should handle content with only local links", () => {
      const content = TestHelpers.createMarkdownWithLinks(
        "Test Article", ["./local1.md", "./local2.md"]
      );
      const basePath = join(tempDir, "main.md");

      TestHelpers.createTestFile(join(tempDir, "local1.md"), "# Local 1");
      TestHelpers.createTestFile(join(tempDir, "local2.md"), "# Local 2");

      const result = BidirectionalLinkResolver.processBidirectionalContent(content, basePath, cacheManager);

      expect(result.localLinks).toHaveLength(2);
      expect(result.telegraphLinks).toHaveLength(0);
      expect(result.hasTelegraphToLocalChanges).toBe(false);
      expect(result.hasLocalToTelegraphChanges).toBe(false);
    });

    it("should handle content with only Telegraph links", () => {
      const content = `# Test Article

This is a [telegraph link](https://telegra.ph/Our-Page-01-01).`;

      const basePath = join(tempDir, "main.md");

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Our-Page-01-01",
        localFilePath: join(tempDir, "our-page.md")
      }));

      const result = BidirectionalLinkResolver.processBidirectionalContent(content, basePath, cacheManager);

      expect(result.localLinks).toHaveLength(0);
      expect(result.telegraphLinks).toHaveLength(1);
      expect(result.hasTelegraphToLocalChanges).toBe(true);
      expect(result.hasLocalToTelegraphChanges).toBe(false);
    });
  });

  describe("updateSourceFileWithLocalLinks", () => {
    it("should update source file when changes are needed", () => {
      const filePath = join(tempDir, "test.md");
      const originalContent = `# Test Article

This is a [telegraph link](https://telegra.ph/Our-Page-01-01).`;

      const contentWithLocalLinks = `# Test Article

This is a [telegraph link](./our-page.md).`;

      TestHelpers.createTestFile(filePath, originalContent);

      const wasUpdated = BidirectionalLinkResolver.updateSourceFileWithLocalLinks(
        filePath,
        contentWithLocalLinks,
        originalContent,
        true
      );

      expect(wasUpdated).toBe(true);
      const updatedContent = TestHelpers.readFile(filePath);
      expect(updatedContent).toBe(contentWithLocalLinks);
    });

    it("should not update source file when no changes are needed", () => {
      const filePath = join(tempDir, "test.md");
      const originalContent = `# Test Article

This is content without Telegraph links.`;

      TestHelpers.createTestFile(filePath, originalContent);

      const wasUpdated = BidirectionalLinkResolver.updateSourceFileWithLocalLinks(
        filePath,
        originalContent,
        originalContent,
        false
      );

      expect(wasUpdated).toBe(false);
    });

    it("should handle non-existent file by creating it", () => {
      const filePath = join(tempDir, "non-existent.md");
      const contentWithLocalLinks = "# Test";

      const wasUpdated = BidirectionalLinkResolver.updateSourceFileWithLocalLinks(
        filePath,
        contentWithLocalLinks,
        "original",
        true
      );

      expect(wasUpdated).toBe(true);
      expect(TestHelpers.fileExists(filePath)).toBe(true);
      expect(TestHelpers.readFile(filePath)).toBe(contentWithLocalLinks);
    });
  });

  describe("analyzeBidirectionalChanges", () => {
    it("should analyze changes correctly", () => {
      const localLinks = [
        {
          text: "local1",
          originalPath: "./local1.md",
          resolvedPath: join(tempDir, "local1.md"),
          isPublished: true,
          isInternalLink: true,
          fullMatch: "[local1](./local1.md)",
          startIndex: 0,
          endIndex: 20,
          telegraphUrl: "https://telegra.ph/Local1-01-01"
        },
        {
          text: "local2",
          originalPath: "./local2.md",
          resolvedPath: join(tempDir, "local2.md"),
          isPublished: false,
          isInternalLink: false,
          fullMatch: "[local2](./local2.md)",
          startIndex: 25,
          endIndex: 45
        }
      ];

      const telegraphLinks = [
        {
          text: "telegraph1",
          telegraphUrl: "https://telegra.ph/Telegraph1-01-01",
          localFilePath: "./telegraph1.md",
          fullMatch: "[telegraph1](https://telegra.ph/Telegraph1-01-01)",
          startIndex: 50,
          endIndex: 100,
          shouldConvertToLocal: true
        }
      ];

      const analysis = BidirectionalLinkResolver.analyzeBidirectionalChanges(localLinks, telegraphLinks);

      expect(analysis.localToTelegraphCount).toBe(1); // Only published local links
      expect(analysis.telegraphToLocalCount).toBe(1); // Only convertible telegraph links
      expect(analysis.internalLinksCount).toBe(1);
      expect(analysis.externalTelegraphLinksCount).toBe(0);
    });

    it("should handle empty arrays", () => {
      const analysis = BidirectionalLinkResolver.analyzeBidirectionalChanges([], []);

      expect(analysis.localToTelegraphCount).toBe(0);
      expect(analysis.telegraphToLocalCount).toBe(0);
      expect(analysis.internalLinksCount).toBe(0);
      expect(analysis.externalTelegraphLinksCount).toBe(0);
    });
  });

  describe("validateBidirectionalLinks", () => {
    it("should validate links and report issues", () => {
      const localLinks = [
        {
          text: "broken",
          originalPath: "./broken.md",
          resolvedPath: join(tempDir, "broken.md"), // Non-existent file
          isPublished: false,
          isInternalLink: false,
          fullMatch: "[broken](./broken.md)",
          startIndex: 0,
          endIndex: 20
        },
        {
          text: "valid",
          originalPath: "./valid.md",
          resolvedPath: join(tempDir, "valid.md"),
          isPublished: true,
          isInternalLink: true,
          fullMatch: "[valid](./valid.md)",
          startIndex: 25,
          endIndex: 45,
          telegraphUrl: "https://telegra.ph/Valid-01-01"
        }
      ];

      const telegraphLinks = [
        {
          text: "missing",
          telegraphUrl: "https://telegra.ph/Missing-01-01",
          localFilePath: undefined, // Missing local mapping
          fullMatch: "[missing](https://telegra.ph/Missing-01-01)",
          startIndex: 50,
          endIndex: 100,
          shouldConvertToLocal: false
        }
      ];

      // Create valid file
      TestHelpers.createTestFile(join(tempDir, "valid.md"), "# Valid");

      const validation = BidirectionalLinkResolver.validateBidirectionalLinks(localLinks, telegraphLinks, cacheManager);

      expect(validation.isValid).toBe(false);
      expect(validation.errors.length).toBeGreaterThan(0);
      expect(validation.errors.some(error => error.includes("broken.md"))).toBe(true);
    });

    it("should validate successfully with no issues", () => {
      const localLinks = [
        {
          text: "valid",
          originalPath: "./valid.md",
          resolvedPath: join(tempDir, "valid.md"),
          isPublished: true,
          isInternalLink: true,
          fullMatch: "[valid](./valid.md)",
          startIndex: 0,
          endIndex: 20,
          telegraphUrl: "https://telegra.ph/Valid-01-01"
        }
      ];

      const telegraphLinks = [
        {
          text: "mapped",
          telegraphUrl: "https://telegra.ph/Mapped-01-01",
          localFilePath: "./mapped.md",
          fullMatch: "[mapped](https://telegra.ph/Mapped-01-01)",
          startIndex: 25,
          endIndex: 75,
          shouldConvertToLocal: true
        }
      ];

      // Create files
      TestHelpers.createTestFile(join(tempDir, "valid.md"), "# Valid");
      TestHelpers.createTestFile(join(tempDir, "mapped.md"), "# Mapped");

      const validation = BidirectionalLinkResolver.validateBidirectionalLinks(localLinks, telegraphLinks, cacheManager);

      expect(validation.isValid).toBe(true);
      expect(validation.errors).toHaveLength(0);
      expect(validation.warnings).toHaveLength(0);
    });
  });

  describe("edge cases", () => {
    it("should handle content with mixed link types", () => {
      const content = `# Test Article

This is a [local link](./local.md), a [telegraph link](https://telegra.ph/Our-Page-01-01),
and an [external link](https://example.com).`;

      const basePath = join(tempDir, "main.md");

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/Our-Page-01-01",
        localFilePath: join(tempDir, "our-page.md")
      }));

      TestHelpers.createTestFile(join(tempDir, "local.md"), "# Local");

      const result = BidirectionalLinkResolver.processBidirectionalContent(content, basePath, cacheManager);

      expect(result.localLinks).toHaveLength(1);
      expect(result.telegraphLinks).toHaveLength(1);
      expect(result.contentWithLocalLinks).toContain("./local.md");
      expect(result.contentWithLocalLinks).toContain(join(tempDir, "our-page.md"));
      expect(result.contentWithLocalLinks).toContain("https://example.com");
    });

    it("should handle very long URLs", () => {
      const longUrl = "https://telegra.ph/" + "a".repeat(500);
      const content = `# Test Article

This is a [long link](${longUrl}).`;

      const basePath = join(tempDir, "main.md");

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: longUrl,
        localFilePath: join(tempDir, "long.md")
      }));

      const result = BidirectionalLinkResolver.processBidirectionalContent(content, basePath, cacheManager);

      expect(result.telegraphLinks).toHaveLength(1);
      expect(result.telegraphLinks[0]?.shouldConvertToLocal).toBe(true);
    });

    it("should handle special characters in file paths", () => {
      const content = `# Test Article

This is a [—Ä—É—Å—Å–∫–∞—è —Å—Å—ã–ª–∫–∞](https://telegra.ph/–†—É—Å—Å–∫–∞—è-—Å—Ç–∞—Ç—å—è-01-01) and [‰∏≠ÊñáÈìæÊé•](https://telegra.ph/‰∏≠ÊñáÊñáÁ´†-01-02).`;

      const basePath = join(tempDir, "main.md");

      cacheManager.addPage(TestHelpers.createSamplePageInfo({
        telegraphUrl: "https://telegra.ph/–†—É—Å—Å–∫–∞—è-—Å—Ç–∞—Ç—å—è-01-01",
        localFilePath: join(tempDir, "—Ä—É—Å—Å–∫–∞—è.md")
      }));

      const result = BidirectionalLinkResolver.processBidirectionalContent(content, basePath, cacheManager);

      expect(result.telegraphLinks).toHaveLength(2);
      expect(result.telegraphLinks[0]?.shouldConvertToLocal).toBe(true);
      expect(result.telegraphLinks[1]?.shouldConvertToLocal).toBe(false); // Not in cache
    });
  });
});
```

`src/links/BidirectionalLinkResolver.ts`

```ts
import type { PagesCacheManager } from "../cache/PagesCacheManager";
import type { LocalLink, TelegraphLink } from "../types/metadata";
import { LinkResolver } from "./LinkResolver";

/**
 * Resolves bidirectional links between local files and Telegraph URLs
 */
export class BidirectionalLinkResolver extends LinkResolver {
  private cacheManager: PagesCacheManager;

  constructor(cacheManager: PagesCacheManager) {
    super();
    this.cacheManager = cacheManager;
  }

  /**
   * Find Telegraph links in content that should be converted to local links
   * @param content Content to analyze
   * @returns Array of Telegraph links found
   */
  static findTelegraphLinks(content: string, cacheManager: PagesCacheManager): TelegraphLink[] {
    const links: TelegraphLink[] = [];
    const telegraphLinkRegex = /\[([^\]]*)\]\((https:\/\/telegra\.ph\/[^)]+)\)/g;

    let match: RegExpExecArray | null;
    match = telegraphLinkRegex.exec(content);
    while (match !== null) {
      const [fullMatch, linkText, telegraphUrl] = match;

      if (!fullMatch || !linkText || !telegraphUrl) continue;

      const localFilePath = cacheManager.getLocalPath(telegraphUrl);
      const shouldConvert = localFilePath !== null;

      links.push({
        text: linkText,
        telegraphUrl,
        localFilePath: localFilePath || undefined,
        fullMatch,
        startIndex: match.index,
        endIndex: match.index + fullMatch.length,
        shouldConvertToLocal: shouldConvert
      });

      match = telegraphLinkRegex.exec(content);
    }

    return links;
  }

  /**
   * Enhanced local link detection with internal link marking
   * @param content Content to analyze
   * @param basePath Base file path
   * @param cacheManager Cache manager for checking published status
   * @returns Array of local links with internal link flags
   */
  static findLocalLinksEnhanced(
    content: string,
    basePath: string,
    cacheManager: PagesCacheManager
  ): LocalLink[] {
    const localLinks = LinkResolver.findLocalLinks(content, basePath);

    // Mark internal links (links to our published pages)
    for (const link of localLinks) {
      const telegraphUrl = cacheManager.getTelegraphUrl(link.resolvedPath);
      if (telegraphUrl) {
        link.isPublished = true;
        link.telegraphUrl = telegraphUrl;
        link.isInternalLink = true;
      }
    }

    return localLinks;
  }

  /**
   * Replace Telegraph links with local links in content
   * @param content Original content
   * @param telegraphLinks Telegraph links to replace
   * @returns Content with Telegraph links replaced with local links
   */
  static replaceTelegraphLinksWithLocal(
    content: string,
    telegraphLinks: TelegraphLink[]
  ): string {
    let modifiedContent = content;

    // Sort by start index in reverse order to maintain indices
    const sortedLinks = telegraphLinks
      .filter(link => link.shouldConvertToLocal && link.localFilePath)
      .sort((a, b) => b.startIndex - a.startIndex);

    for (const link of sortedLinks) {
      const localLink = `[${link.text}](${link.localFilePath})`;
      modifiedContent =
        modifiedContent.substring(0, link.startIndex) +
        localLink +
        modifiedContent.substring(link.endIndex);
    }

    return modifiedContent;
  }

  /**
   * Create bidirectional content processing result
   * @param content Original content
   * @param basePath Base file path
   * @param cacheManager Cache manager
   * @returns Enhanced processed content with bidirectional links
   */
  static processBidirectionalContent(
    content: string,
    basePath: string,
    cacheManager: PagesCacheManager
  ): {
    contentWithLocalLinks: string;
    contentWithTelegraphLinks: string;
    localLinks: LocalLink[];
    telegraphLinks: TelegraphLink[];
    hasLocalToTelegraphChanges: boolean;
    hasTelegraphToLocalChanges: boolean;
  } {
    // Find local links that might need Telegraph URL replacement
    const localLinks = BidirectionalLinkResolver.findLocalLinksEnhanced(content, basePath, cacheManager);

    // Find Telegraph links that might need local link replacement
    const telegraphLinks = BidirectionalLinkResolver.findTelegraphLinks(content, cacheManager);

    // Create content with Telegraph URLs (for publishing)
    const linkReplacements = new Map<string, string>();
    for (const link of localLinks) {
      if (link.telegraphUrl && link.isInternalLink) {
        linkReplacements.set(link.originalPath, link.telegraphUrl);
      }
    }
    const contentWithTelegraphLinks = LinkResolver.replaceLocalLinks(content, linkReplacements);

    // Create content with local links (for source file)
    const contentWithLocalLinks = BidirectionalLinkResolver.replaceTelegraphLinksWithLocal(content, telegraphLinks);

    return {
      contentWithLocalLinks,
      contentWithTelegraphLinks,
      localLinks,
      telegraphLinks,
      hasLocalToTelegraphChanges: linkReplacements.size > 0,
      hasTelegraphToLocalChanges: telegraphLinks.some(link => link.shouldConvertToLocal)
    };
  }

  /**
   * Update source file with local links if Telegraph links were found
   * @param filePath File path to update
   * @param contentWithLocalLinks Content with Telegraph links replaced by local links
   * @param originalContent Original file content
   * @param hasTelegraphToLocalChanges Whether any Telegraph to local changes were made
   * @returns Whether file was updated
   */
  static updateSourceFileWithLocalLinks(
    filePath: string,
    contentWithLocalLinks: string,
    originalContent: string,
    hasTelegraphToLocalChanges: boolean
  ): boolean {
    if (!hasTelegraphToLocalChanges || contentWithLocalLinks === originalContent) {
      return false;
    }

    try {
      const fs = require("node:fs");
      fs.writeFileSync(filePath, contentWithLocalLinks, "utf-8");
      console.log(`‚úÖ Updated source file with local links: ${filePath}`);
      return true;
    } catch (error) {
      console.error(`‚ùå Error updating source file ${filePath}:`, error);
      return false;
    }
  }

  /**
   * Analyze bidirectional link changes
   * @param localLinks Local links found
   * @param telegraphLinks Telegraph links found
   * @returns Analysis of link changes
   */
  static analyzeBidirectionalChanges(
    localLinks: LocalLink[],
    telegraphLinks: TelegraphLink[]
  ): {
    localToTelegraphCount: number;
    telegraphToLocalCount: number;
    internalLinksCount: number;
    externalTelegraphLinksCount: number;
    summary: string[];
  } {
    const localToTelegraphCount = localLinks.filter(link => link.isInternalLink).length;
    const telegraphToLocalCount = telegraphLinks.filter(link => link.shouldConvertToLocal).length;
    const internalLinksCount = localLinks.filter(link => link.isInternalLink).length;
    const externalTelegraphLinksCount = telegraphLinks.filter(link => !link.shouldConvertToLocal).length;

    const summary: string[] = [];

    if (localToTelegraphCount > 0) {
      summary.push(`${localToTelegraphCount} local link(s) will be replaced with Telegraph URLs in published content`);
    }

    if (telegraphToLocalCount > 0) {
      summary.push(`${telegraphToLocalCount} Telegraph link(s) will be replaced with local links in source file`);
    }

    if (internalLinksCount > 0) {
      summary.push(`${internalLinksCount} internal link(s) detected between your published pages`);
    }

    if (externalTelegraphLinksCount > 0) {
      summary.push(`${externalTelegraphLinksCount} external Telegraph link(s) will remain unchanged`);
    }

    return {
      localToTelegraphCount,
      telegraphToLocalCount,
      internalLinksCount,
      externalTelegraphLinksCount,
      summary
    };
  }

  /**
   * Validate bidirectional link consistency
   * @param localLinks Local links
   * @param telegraphLinks Telegraph links
   * @param cacheManager Cache manager
   * @returns Validation results
   */
  static validateBidirectionalLinks(
    localLinks: LocalLink[],
    telegraphLinks: TelegraphLink[],
    cacheManager: PagesCacheManager
  ): {
    isValid: boolean;
    warnings: string[];
    errors: string[];
  } {
    const warnings: string[] = [];
    const errors: string[] = [];

    // Check for broken local links
    for (const link of localLinks) {
      if (!LinkResolver.validateLinkTarget(link.resolvedPath)) {
        errors.push(`Broken local link: ${link.originalPath}`);
      }
    }

    // Check for Telegraph links that should be local but aren't in cache
    for (const link of telegraphLinks) {
      if (cacheManager.isOurPage(link.telegraphUrl) && !link.localFilePath) {
        warnings.push(`Telegraph link to our page but no local file mapping: ${link.telegraphUrl}`);
      }
    }

    // Check for circular references
    const localPaths = localLinks.map(link => link.resolvedPath);
    const duplicates = localPaths.filter((path, index) => localPaths.indexOf(path) !== index);
    if (duplicates.length > 0) {
      warnings.push(`Duplicate local links detected: ${duplicates.join(', ')}`);
    }

    return {
      isValid: errors.length === 0,
      warnings,
      errors
    };
  }
}
```

`src/links/index.ts`

```ts
/**
 * Public API for the link verification system
 */

export { AutoRepairer } from './AutoRepairer';
export { InteractiveRepairer } from './InteractiveRepairer';
export { LinkResolver } from './LinkResolver';
// Core components
export { LinkScanner } from './LinkScanner';
export { LinkVerifier } from './LinkVerifier';
export { ReportGenerator } from './ReportGenerator';

// Types and interfaces
export type {
  BrokenLink,
  CheckLinksOptions,
  FileScanResult,
  FixResult,
  LinkStatistics,
  MarkdownLink,
  ProgressCallback,
  RepairSummary,
  ScanConfig,
  ScanResult,
  UserAction
} from './types';

// Error types
export {
  LinkVerificationError,
  LinkVerificationException
} from './types';
```

`src/links/InteractiveRepairer.ts`

```ts
import { readFileSync, writeFileSync } from 'node:fs';
import { createInterface } from 'node:readline';
import type { ReportGenerator } from './ReportGenerator';
import {
  type BrokenLink,
  type FixResult,
  LinkVerificationError,
  LinkVerificationException,
  type RepairSummary,
  type UserAction
} from './types';

/**
 * InteractiveRepairer handles user interaction for fixing broken links
 */
export class InteractiveRepairer {
  private reportGenerator: ReportGenerator;

  constructor(reportGenerator: ReportGenerator) {
    this.reportGenerator = reportGenerator;
  }

  /**
   * Perform interactive repair session for broken links
   * @param brokenLinks Array of broken links to repair
   * @returns Repair summary
   */
  async performInteractiveRepair(brokenLinks: BrokenLink[]): Promise<RepairSummary> {
    const fixableLinks = brokenLinks.filter(link => link.canAutoFix && link.suggestions.length > 0);

    if (fixableLinks.length === 0) {
      this.reportGenerator.showInfo('No links with available fixes found.');
      return this.createEmptySummary();
    }

    const totalFiles = new Set(fixableLinks.map(link => link.filePath)).size;
    this.showInteractiveHeader(fixableLinks.length, totalFiles);

    const summary: RepairSummary = {
      totalProcessed: 0,
      fixesApplied: 0,
      linksSkipped: 0,
      filesModified: 0,
      errors: []
    };

    const modifiedFiles = new Set<string>();
    let applyAll = false;

    for (const brokenLink of fixableLinks) {
      if (applyAll) {
        // Auto-apply remaining fixes
        const result = await this.applyBestFix(brokenLink);
        this.processBatchFixResult(result, summary, modifiedFiles);
      } else {
        // Show file context before each interactive prompt
        this.showFileContext(brokenLink, summary.totalProcessed + 1, fixableLinks.length);
        const action = await this.promptUserForAction(brokenLink);

        if (action === 'quit') {
          break;
        } else if (action === 'all') {
          applyAll = true;
          const result = await this.applyBestFix(brokenLink);
          this.processBatchFixResult(result, summary, modifiedFiles);
        } else if (action === 'yes') {
          const selectedFix = await this.selectFix(brokenLink);
          if (selectedFix) {
            const result = await this.applyFix(brokenLink, selectedFix);
            this.processFixResult(result, summary, modifiedFiles);
          } else {
            summary.linksSkipped++;
          }
        } else {
          summary.linksSkipped++;
          this.reportGenerator.showInfo(`‚ùå Fix skipped: ${this.formatBrokenLink(brokenLink)}`);
        }
      }

      summary.totalProcessed++;
    }

    summary.filesModified = modifiedFiles.size;
    this.showRepairSummary(summary);

    return summary;
  }

  /**
   * Show interactive mode header
   * @param fixableCount Number of fixable links
   * @param totalFiles Number of files with issues
   */
  private showInteractiveHeader(fixableCount: number, totalFiles: number): void {
    console.log('üîß INTERACTIVE REPAIR MODE');
    console.log('‚ïê'.repeat(63));
    console.log();
    console.log(`Found ${fixableCount} broken links with suggested fixes in ${totalFiles} files.`);
    console.log('For each link, choose an action:');
    console.log('  y - apply fix');
    console.log('  n - skip');
    console.log('  a - apply all remaining fixes');
    console.log('  q - quit');
    console.log();
  }

  /**
   * Prompt user for action on a broken link
   * @param brokenLink The broken link to handle
   * @returns User's chosen action
   */
  private async promptUserForAction(brokenLink: BrokenLink): Promise<UserAction> {
    this.showBrokenLinkDetails(brokenLink);

    while (true) {
      const answer = await this.askQuestion('Apply this fix? (y/n/a/q): ');
      const action = answer.toLowerCase().trim();

      if (['y', 'yes'].includes(action)) {
        return 'yes';
      } else if (['n', 'no'].includes(action)) {
        return 'no';
      } else if (['a', 'all'].includes(action)) {
        return 'all';
      } else if (['q', 'quit'].includes(action)) {
        return 'quit';
      } else {
        console.log('Please enter y/n/a/q');
      }
    }
  }

  /**
 * Show file context for the current repair item
 * @param brokenLink The broken link
 * @param current Current item number
 * @param total Total items to process
 */
  private showFileContext(brokenLink: BrokenLink, current: number, total: number): void {
    const displayPath = this.formatFilePath(brokenLink.filePath);
    console.log(`\nüîß Repairing [${current}/${total}] in file: ${displayPath}`);
    console.log('‚ïê'.repeat(62));
  }

  /**
   * Show broken link details for user decision
   * @param brokenLink The broken link to display
   */
  private showBrokenLinkDetails(brokenLink: BrokenLink): void {
    const link = brokenLink.link;

    console.log(`üîó Broken link: [${link.text}](${link.href}) (line ${link.lineNumber})`);

    if (brokenLink.suggestions.length === 1) {
      console.log(`üí° Suggested fix: ${brokenLink.suggestions[0]}`);
    } else {
      console.log('üí° Multiple possible fixes found:');
      brokenLink.suggestions.forEach((suggestion, index) => {
        console.log(`  ${index + 1}) ${suggestion}`);
      });
    }

    console.log();
  }

  /**
   * Let user select a fix from multiple suggestions
   * @param brokenLink The broken link with multiple suggestions
   * @returns Selected fix or null if cancelled
   */
  private async selectFix(brokenLink: BrokenLink): Promise<string | null> {
    if (brokenLink.suggestions.length === 1) {
      return brokenLink.suggestions[0] || null;
    }

    while (true) {
      const answer = await this.askQuestion(`Choose option (1-${brokenLink.suggestions.length}) or action (n/a/q): `);
      const input = answer.trim().toLowerCase();

      if (['n', 'no'].includes(input)) {
        return null;
      } else if (['a', 'all'].includes(input)) {
        return brokenLink.suggestions[0] || null;
      } else if (['q', 'quit'].includes(input)) {
        return null;
      }

      const choice = parseInt(input);
      if (!isNaN(choice) && choice >= 1 && choice <= brokenLink.suggestions.length) {
        return brokenLink.suggestions[choice - 1] || null;
      }

      console.log(`Please enter a number from 1 to ${brokenLink.suggestions.length}, or n/a/q`);
    }
  }

  /**
   * Apply the best available fix for a broken link
   * @param brokenLink The broken link to fix
   * @returns Fix result
   */
  private async applyBestFix(brokenLink: BrokenLink): Promise<FixResult> {
    const bestFix = brokenLink.suggestions[0];
    if (!bestFix) {
      return {
        success: false,
        brokenLink,
        error: 'No available fixes'
      };
    }

    return this.applyFix(brokenLink, bestFix);
  }

  /**
   * Apply a specific fix to a broken link
   * @param brokenLink The broken link to fix
   * @param fixPath The fix path to apply
   * @returns Fix result
   */
  private async applyFix(brokenLink: BrokenLink, fixPath: string): Promise<FixResult> {
    try {
      // Read current file content
      const content = readFileSync(brokenLink.filePath, 'utf-8');

      // Create the original link markdown
      const originalLinkText = `[${brokenLink.link.text}](${brokenLink.link.href})`;
      const newLinkText = `[${brokenLink.link.text}](${fixPath})`;

      // Replace the specific link
      const updatedContent = this.replaceLink(content, originalLinkText, newLinkText, brokenLink.link.lineNumber);

      if (updatedContent === content) {
        return {
          success: false,
          brokenLink,
          error: 'Could not find link to replace'
        };
      }

      // Write updated content back to file
      writeFileSync(brokenLink.filePath, updatedContent, 'utf-8');

      return {
        success: true,
        brokenLink,
        appliedFix: fixPath
      };

    } catch (error) {
      return {
        success: false,
        brokenLink,
        error: error instanceof Error ? error.message : String(error)
      };
    }
  }

  /**
   * Replace a specific link in content
   * @param content File content
   * @param originalLink Original link text
   * @param newLink New link text
   * @param lineNumber Line number where link should be found
   * @returns Updated content
   */
  private replaceLink(content: string, originalLink: string, newLink: string, lineNumber: number): string {
    const lines = content.split('\n');

    if (lineNumber <= 0 || lineNumber > lines.length) {
      return content;
    }

    const targetLine = lines[lineNumber - 1]; // Convert to 0-based index
    if (!targetLine || !targetLine.includes(originalLink)) {
      // Try to find the link in nearby lines (in case line numbers are slightly off)
      for (let i = Math.max(0, lineNumber - 3); i < Math.min(lines.length, lineNumber + 2); i++) {
        if (lines[i]?.includes(originalLink)) {
          lines[i] = lines[i]!.replace(originalLink, newLink);
          return lines.join('\n');
        }
      }
      return content;
    }

    lines[lineNumber - 1] = targetLine.replace(originalLink, newLink);
    return lines.join('\n');
  }

  /**
   * Process fix result and update summary
   * @param result Fix result
   * @param summary Repair summary to update
   * @param modifiedFiles Set of modified files
   */
  private processFixResult(result: FixResult, summary: RepairSummary, modifiedFiles: Set<string>): void {
    if (result.success) {
      summary.fixesApplied++;
      modifiedFiles.add(result.brokenLink.filePath);

      const displayPath = this.formatFilePath(result.brokenLink.filePath);
      console.log(`‚úÖ Fix applied: ${displayPath}`);
      console.log(`   [${result.brokenLink.link.text}](${result.brokenLink.link.href}) ‚Üí [${result.brokenLink.link.text}](${result.appliedFix})`);
      console.log();
    } else {
      summary.errors.push(`${result.brokenLink.filePath}: ${result.error || 'Unknown error'}`);
      this.reportGenerator.showError(`Failed to apply fix: ${result.error}`, result.brokenLink.filePath);
    }
  }

  /**
   * Process fix result for batch operations
   * @param result Fix result
   * @param summary Repair summary to update
   * @param modifiedFiles Set of modified files
   */
  private processBatchFixResult(result: FixResult, summary: RepairSummary, modifiedFiles: Set<string>): void {
    if (result.success) {
      summary.fixesApplied++;
      modifiedFiles.add(result.brokenLink.filePath);
    } else {
      summary.errors.push(`${result.brokenLink.filePath}: ${result.error || 'Unknown error'}`);
    }
  }

  /**
 * Show repair session summary
 * @param summary Repair summary
 */
  private showRepairSummary(summary: RepairSummary): void {
    console.log('üéâ INTERACTIVE REPAIR COMPLETED');
    console.log('‚ïê'.repeat(63));
    console.log();
    console.log('üìä Results:');
    console.log(`   ‚Ä¢ Processed: ${summary.totalProcessed} links`);
    console.log(`   ‚Ä¢ Fixed: ${summary.fixesApplied} links`);
    console.log(`   ‚Ä¢ Skipped: ${summary.linksSkipped} links`);
    console.log(`   ‚Ä¢ Files modified: ${summary.filesModified}`);

    if (summary.errors.length > 0) {
      console.log(`   ‚Ä¢ Errors: ${summary.errors.length}`);
      console.log();
      console.log('‚ùå Errors:');
      summary.errors.forEach(error => {
        console.log(`   ${error}`);
      });
    }

    console.log();

    if (summary.fixesApplied > 0) {
      console.log('‚úÖ Fixes applied successfully!');
    } else {
      console.log('‚ÑπÔ∏è  No changes were made.');
    }

    console.log();
  }

  /**
   * Ask user a question and wait for input
   * @param question Question to ask
   * @returns User's answer
   */
  private async askQuestion(question: string): Promise<string> {
    const rl = createInterface({
      input: process.stdin,
      output: process.stdout
    });

    return new Promise((resolve) => {
      rl.question(question, (answer) => {
        rl.close();
        resolve(answer);
      });
    });
  }

  /**
   * Format a broken link for display
   * @param brokenLink The broken link
   * @returns Formatted string
   */
  private formatBrokenLink(brokenLink: BrokenLink): string {
    const displayPath = this.formatFilePath(brokenLink.filePath);
    return `[${brokenLink.link.text}](${brokenLink.link.href}) in ${displayPath}`;
  }

  /**
   * Format file path for display
   * @param filePath File path to format
   * @returns Formatted path
   */
  private formatFilePath(filePath: string): string {
    // Delegate to ReportGenerator for consistent formatting
    return (this.reportGenerator as any).formatFilePath(filePath);
  }

  /**
   * Create empty repair summary
   * @returns Empty summary
   */
  private createEmptySummary(): RepairSummary {
    return {
      totalProcessed: 0,
      fixesApplied: 0,
      linksSkipped: 0,
      filesModified: 0,
      errors: []
    };
  }
}
```

`src/links/LinkResolver.test.ts`

```ts
import { beforeEach, describe, expect, test } from 'bun:test';
import { LinkResolver } from './LinkResolver';
import type { BrokenLink, FileScanResult, MarkdownLink } from './types';

describe('LinkResolver', () => {
  let resolver: LinkResolver;

  beforeEach(() => {
    resolver = new LinkResolver();
  });

  describe('resolveBrokenLinks', () => {
    test('should find suggestions for broken links', async () => {
      const scanResults: FileScanResult[] = [
        {
          filePath: '/project/docs/guide.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: '/project/docs/guide.md',
              link: {
                text: 'API Reference',
                href: './api.md',
                lineNumber: 5,
                columnStart: 0,
                columnEnd: 20
              },
              suggestions: [],
              canAutoFix: false
            }
          ],
          processingTime: 0
        },
        {
          filePath: '/project/reference/api.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [],
          processingTime: 0
        }
      ];

      const resolved = await resolver.resolveBrokenLinks(scanResults);

      expect(resolved[0]?.brokenLinks[0]?.suggestions).toHaveLength(1);
      expect(resolved[0]?.brokenLinks[0]?.suggestions[0]).toBe('../reference/api.md');
      expect(resolved[0]?.brokenLinks[0]?.canAutoFix).toBe(true);
    });

    test('should handle multiple files with same name', async () => {
      const scanResults: FileScanResult[] = [
        {
          filePath: '/project/docs/guide.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: '/project/docs/guide.md',
              link: {
                text: 'Config',
                href: './config.md',
                lineNumber: 5,
                columnStart: 0,
                columnEnd: 20
              },
              suggestions: [],
              canAutoFix: false
            }
          ],
          processingTime: 0
        },
        {
          filePath: '/project/config/config.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [],
          processingTime: 0
        },
        {
          filePath: '/project/examples/config.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [],
          processingTime: 0
        }
      ];

      const resolved = await resolver.resolveBrokenLinks(scanResults);

      expect(resolved[0]?.brokenLinks[0]?.suggestions).toHaveLength(2);
      expect(resolved[0]?.brokenLinks[0]?.suggestions).toContain('../config/config.md');
      expect(resolved[0]?.brokenLinks[0]?.suggestions).toContain('../examples/config.md');
    });

    test('should not find suggestions for non-existent files', async () => {
      const scanResults: FileScanResult[] = [
        {
          filePath: '/project/docs/guide.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: '/project/docs/guide.md',
              link: {
                text: 'Non-existent',
                href: './nonexistent.md',
                lineNumber: 5,
                columnStart: 0,
                columnEnd: 20
              },
              suggestions: [],
              canAutoFix: false
            }
          ],
          processingTime: 0
        }
      ];

      const resolved = await resolver.resolveBrokenLinks(scanResults);

      expect(resolved[0]?.brokenLinks[0]?.suggestions).toHaveLength(0);
      expect(resolved[0]?.brokenLinks[0]?.canAutoFix).toBe(false);
    });

    test('should sort suggestions by preference', async () => {
      const scanResults: FileScanResult[] = [
        {
          filePath: '/project/docs/guide.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: '/project/docs/guide.md',
              link: {
                text: 'File',
                href: './file.md',
                lineNumber: 5,
                columnStart: 0,
                columnEnd: 20
              },
              suggestions: [],
              canAutoFix: false
            }
          ],
          processingTime: 0
        },
        {
          filePath: '/project/docs/file.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [],
          processingTime: 0
        },
        {
          filePath: '/project/deep/nested/path/file.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [],
          processingTime: 0
        }
      ];

      const resolved = await resolver.resolveBrokenLinks(scanResults);
      const suggestions = resolved[0]?.brokenLinks[0]?.suggestions || [];

      // Shorter path should come first
      expect(suggestions[0]).toBe('./file.md');
      expect(suggestions[1]).toBe('../deep/nested/path/file.md');
    });
  });

  describe('getBestSuggestion', () => {
    test('should return the first suggestion', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Link',
          href: './target.md',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: ['./suggestion1.md', './suggestion2.md'],
        canAutoFix: true
      };

      const best = resolver.getBestSuggestion(brokenLink);
      expect(best).toBe('./suggestion1.md');
    });

    test('should return null for no suggestions', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Link',
          href: './target.md',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: [],
        canAutoFix: false
      };

      const best = resolver.getBestSuggestion(brokenLink);
      expect(best).toBeNull();
    });
  });

  describe('hasMultipleSuggestions', () => {
    test('should return true for multiple suggestions', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Link',
          href: './target.md',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: ['./suggestion1.md', './suggestion2.md'],
        canAutoFix: true
      };

      expect(resolver.hasMultipleSuggestions(brokenLink)).toBe(true);
    });

    test('should return false for single suggestion', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Link',
          href: './target.md',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: ['./suggestion1.md'],
        canAutoFix: true
      };

      expect(resolver.hasMultipleSuggestions(brokenLink)).toBe(false);
    });
  });

  describe('groupByTargetFilename', () => {
    test('should group broken links by target filename', () => {
      const brokenLinks: BrokenLink[] = [
        {
          filePath: '/project/file1.md',
          link: {
            text: 'Config',
            href: './config.md',
            lineNumber: 1,
            columnStart: 0,
            columnEnd: 10
          },
          suggestions: [],
          canAutoFix: false
        },
        {
          filePath: '/project/file2.md',
          link: {
            text: 'Config',
            href: '../config.md',
            lineNumber: 1,
            columnStart: 0,
            columnEnd: 10
          },
          suggestions: [],
          canAutoFix: false
        },
        {
          filePath: '/project/file3.md',
          link: {
            text: 'API',
            href: './api.md',
            lineNumber: 1,
            columnStart: 0,
            columnEnd: 10
          },
          suggestions: [],
          canAutoFix: false
        }
      ];

      const groups = resolver.groupByTargetFilename(brokenLinks);

      expect(groups.size).toBe(2);
      expect(groups.get('config.md')).toHaveLength(2);
      expect(groups.get('api.md')).toHaveLength(1);
    });
  });

  describe('calculateFixConfidence', () => {
    test('should return high confidence for exact filename match', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Config',
          href: './config.md',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: [],
        canAutoFix: false
      };

      const confidence = resolver.calculateFixConfidence(brokenLink, '../other/config.md');
      expect(confidence).toBeGreaterThanOrEqual(0.8);
    });

    test('should return lower confidence for partial match', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Config',
          href: './config.md',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: [],
        canAutoFix: false
      };

      const confidence = resolver.calculateFixConfidence(brokenLink, '../other/configuration.md');
      expect(confidence).toBeLessThan(0.6);
    });

    test('should return zero confidence for no filename', () => {
      const brokenLink: BrokenLink = {
        filePath: '/project/file.md',
        link: {
          text: 'Directory',
          href: './some-directory/',
          lineNumber: 1,
          columnStart: 0,
          columnEnd: 10
        },
        suggestions: [],
        canAutoFix: false
      };

      const confidence = resolver.calculateFixConfidence(brokenLink, '../other/config.md');
      expect(confidence).toBe(0);
    });
  });

  describe('getResolutionStats', () => {
    test('should calculate correct resolution statistics', () => {
      const scanResults: FileScanResult[] = [
        {
          filePath: '/project/file1.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: '/project/file1.md',
              link: {
                text: 'Link 1',
                href: './target1.md',
                lineNumber: 1,
                columnStart: 0,
                columnEnd: 10
              },
              suggestions: ['./suggestion1.md'],
              canAutoFix: true
            },
            {
              filePath: '/project/file1.md',
              link: {
                text: 'Link 2',
                href: './target2.md',
                lineNumber: 2,
                columnStart: 0,
                columnEnd: 10
              },
              suggestions: [],
              canAutoFix: false
            }
          ],
          processingTime: 0
        },
        {
          filePath: '/project/file2.md',
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: '/project/file2.md',
              link: {
                text: 'Link 3',
                href: './target3.md',
                lineNumber: 1,
                columnStart: 0,
                columnEnd: 10
              },
              suggestions: ['./suggestion3a.md', './suggestion3b.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        }
      ];

      const stats = resolver.getResolutionStats(scanResults);

      expect(stats.totalBrokenLinks).toBe(3);
      expect(stats.linksWithSuggestions).toBe(2);
      expect(stats.resolutionRate).toBeCloseTo(66.67, 2); // 2/3 * 100
      expect(stats.averageSuggestionsPerLink).toBe(1.5); // (1 + 2) / 2
      expect(stats.filesWithAutoFixableLinks).toBe(2);
    });

    test('should handle empty results', () => {
      const stats = resolver.getResolutionStats([]);

      expect(stats.totalBrokenLinks).toBe(0);
      expect(stats.linksWithSuggestions).toBe(0);
      expect(stats.resolutionRate).toBe(0);
      expect(stats.averageSuggestionsPerLink).toBe(0);
      expect(stats.filesWithAutoFixableLinks).toBe(0);
    });
  });

  describe('extractFilename', () => {
    test('should extract filename from various paths', () => {
      // Access private method for testing through type assertion
      const extractFilename = (resolver as any).extractFilename.bind(resolver);

      expect(extractFilename('./config.md')).toBe('config.md');
      expect(extractFilename('../path/to/file.md')).toBe('file.md');
      expect(extractFilename('/absolute/path/file.md')).toBe('file.md');
      expect(extractFilename('file.md')).toBe('file.md');
    });

    test('should handle query parameters and fragments', () => {
      const extractFilename = (resolver as any).extractFilename.bind(resolver);

      expect(extractFilename('./file.md?param=value')).toBe('file.md');
      expect(extractFilename('./file.md#section')).toBe('file.md');
      expect(extractFilename('./file.md?param=value#section')).toBe('file.md');
    });

    test('should return null for invalid filenames', () => {
      const extractFilename = (resolver as any).extractFilename.bind(resolver);

      expect(extractFilename('./directory/')).toBeNull();
      expect(extractFilename('./no-extension')).toBeNull();
      expect(extractFilename('')).toBeNull();
    });
  });
});
```

`src/links/LinkResolver.ts`

```ts
import { existsSync } from 'node:fs';
import { basename, dirname, extname, relative } from 'node:path';
import type { LocalLink } from '../types/metadata';
import { PathResolver } from '../utils/PathResolver';
import type {
  BrokenLink,
  FileScanResult,
  ScanResult
} from './types';

/**
 * LinkResolver provides intelligent suggestions for fixing broken links
 */
export class LinkResolver {
  private static pathResolverInstance = PathResolver.getInstance();

  /**
   * Resolve suggestions for broken links based on available files
   * @param scanResults All scan results from the project
   * @returns Updated scan results with suggestions populated
   */
  async resolveBrokenLinks(scanResults: FileScanResult[]): Promise<FileScanResult[]> {
    // Build a map of available files by their basename for quick lookup
    const availableFiles = this.buildFileMap(scanResults);

    const resolvedResults: FileScanResult[] = [];

    for (const scanResult of scanResults) {
      const updatedBrokenLinks: BrokenLink[] = [];

      for (const brokenLink of scanResult.brokenLinks) {
        const suggestions = this.findSuggestions(brokenLink, availableFiles, scanResult.filePath);

        updatedBrokenLinks.push({
          ...brokenLink,
          suggestions,
          canAutoFix: suggestions.length > 0
        });
      }

      resolvedResults.push({
        ...scanResult,
        brokenLinks: updatedBrokenLinks
      });
    }

    return resolvedResults;
  }

  /**
   * Resolve suggestions for a complete scan result
   * @param scanResult Complete scan result
   * @returns Updated scan result with all suggestions resolved
   */
  async resolveScanResult(scanResult: ScanResult): Promise<ScanResult> {
    const resolvedFileResults = await this.resolveBrokenLinks(scanResult.fileResults);

    // Rebuild the flattened broken links array
    const allBrokenLinks: BrokenLink[] = [];
    for (const fileResult of resolvedFileResults) {
      allBrokenLinks.push(...fileResult.brokenLinks);
    }

    return {
      ...scanResult,
      brokenLinks: allBrokenLinks,
      fileResults: resolvedFileResults
    };
  }

  /**
   * Find suggestions for a single broken link
   * @param brokenLink The broken link to find suggestions for
   * @param availableFiles Map of available files
   * @param sourceFilePath Path of the source file
   * @returns Array of suggested fix paths
   */
  private findSuggestions(
    brokenLink: BrokenLink,
    availableFiles: Map<string, string[]>,
    sourceFilePath: string
  ): string[] {
    const linkHref = brokenLink.link.href;

    // Extract the filename from the broken link
    const targetFilename = this.extractFilename(linkHref);

    if (!targetFilename) {
      return [];
    }

    // Find files with matching names
    const candidateFiles = availableFiles.get(targetFilename.toLowerCase()) || [];

    if (candidateFiles.length === 0) {
      return [];
    }

    // Generate relative paths from source to each candidate
    const suggestions: string[] = [];
    const sourceDir = dirname(sourceFilePath);

    for (const candidateFile of candidateFiles) {
      // Skip if it's the same file
      if (candidateFile === sourceFilePath) {
        continue;
      }

      try {
        const relativePath = relative(sourceDir, candidateFile);

        // Ensure we use forward slashes for cross-platform compatibility
        const normalizedPath = relativePath.replace(/\\/g, '/');

        // Add ./ prefix for relative paths that don't start with ../
        const finalPath = normalizedPath.startsWith('../') ? normalizedPath : `./${normalizedPath}`;

        suggestions.push(finalPath);
      } catch (error) {
      }
    }

    // Sort suggestions by "distance" - prefer shorter paths
    return suggestions.sort((a, b) => {
      const aDepth = (a.match(/\.\.\//g) || []).length;
      const bDepth = (b.match(/\.\.\//g) || []).length;

      if (aDepth !== bDepth) {
        return aDepth - bDepth; // Prefer fewer ../
      }

      return a.length - b.length; // Prefer shorter paths
    });
  }

  /**
   * Extract filename from a link path
   * @param linkHref The link path
   * @returns Filename or null if not extractable
   */
  private extractFilename(linkHref: string): string | null {
    // Remove query parameters and fragments
    const cleanHref = linkHref.split('?')[0]?.split('#')[0];

    if (!cleanHref) {
      return null;
    }

    // Extract the basename
    const filename = basename(cleanHref);

    // Must have an extension to be considered a file
    if (!filename.includes('.')) {
      return null;
    }

    return filename;
  }

  /**
   * Build a map of available files organized by filename
   * @param scanResults All scan results
   * @returns Map from lowercase filename to array of full file paths
   */
  private buildFileMap(scanResults: FileScanResult[]): Map<string, string[]> {
    const fileMap = new Map<string, string[]>();

    for (const scanResult of scanResults) {
      const filename = basename(scanResult.filePath).toLowerCase();

      if (!fileMap.has(filename)) {
        fileMap.set(filename, []);
      }

      fileMap.get(filename)?.push(scanResult.filePath);
    }

    return fileMap;
  }

  /**
   * Get statistics about link resolution
   * @param scanResults Resolved scan results
   * @returns Resolution statistics
   */
  getResolutionStats(scanResults: FileScanResult[]): {
    totalBrokenLinks: number;
    linksWithSuggestions: number;
    resolutionRate: number;
    averageSuggestionsPerLink: number;
    filesWithAutoFixableLinks: number;
  } {
    let totalBrokenLinks = 0;
    let linksWithSuggestions = 0;
    let totalSuggestions = 0;
    const filesWithAutoFixableLinks = new Set<string>();

    for (const scanResult of scanResults) {
      for (const brokenLink of scanResult.brokenLinks) {
        totalBrokenLinks++;

        if (brokenLink.suggestions.length > 0) {
          linksWithSuggestions++;
          totalSuggestions += brokenLink.suggestions.length;

          if (brokenLink.canAutoFix) {
            filesWithAutoFixableLinks.add(scanResult.filePath);
          }
        }
      }
    }

    return {
      totalBrokenLinks,
      linksWithSuggestions,
      resolutionRate: totalBrokenLinks > 0 ? (linksWithSuggestions / totalBrokenLinks) * 100 : 0,
      averageSuggestionsPerLink: linksWithSuggestions > 0 ? totalSuggestions / linksWithSuggestions : 0,
      filesWithAutoFixableLinks: filesWithAutoFixableLinks.size
    };
  }

  /**
   * Find the best suggestion for a broken link
   * @param brokenLink The broken link
   * @returns The best suggestion or null if none available
   */
  getBestSuggestion(brokenLink: BrokenLink): string | null {
    if (brokenLink.suggestions.length === 0) {
      return null;
    }

    // Return the first suggestion (they're already sorted by preference)
    return brokenLink.suggestions[0] || null;
  }

  /**
   * Check if a broken link has multiple suggestion options
   * @param brokenLink The broken link
   * @returns True if there are multiple suggestions
   */
  hasMultipleSuggestions(brokenLink: BrokenLink): boolean {
    return brokenLink.suggestions.length > 1;
  }

  /**
   * Group broken links by their target filename for bulk operations
   * @param brokenLinks Array of broken links
   * @returns Map from filename to array of broken links
   */
  groupByTargetFilename(brokenLinks: BrokenLink[]): Map<string, BrokenLink[]> {
    const groups = new Map<string, BrokenLink[]>();

    for (const brokenLink of brokenLinks) {
      const filename = this.extractFilename(brokenLink.link.href);

      if (filename) {
        const key = filename.toLowerCase();

        if (!groups.has(key)) {
          groups.set(key, []);
        }

        groups.get(key)?.push(brokenLink);
      }
    }

    return groups;
  }

  /**
   * Calculate the "fix confidence" for a suggestion
   * @param brokenLink The broken link
   * @param suggestion The suggested fix
   * @returns Confidence score between 0 and 1
   */
  calculateFixConfidence(brokenLink: BrokenLink, suggestion: string): number {
    const originalPath = brokenLink.link.href;
    const originalFilename = this.extractFilename(originalPath);
    const suggestedFilename = this.extractFilename(suggestion);

    if (!originalFilename || !suggestedFilename) {
      return 0;
    }

    // Exact filename match gives high confidence
    if (originalFilename.toLowerCase() === suggestedFilename.toLowerCase()) {
      // Bonus for similar directory structure
      const originalDirs = originalPath.split('/').slice(0, -1);
      const suggestedDirs = suggestion.split('/').slice(0, -1);

      const commonDirs = originalDirs.filter(dir => suggestedDirs.includes(dir));
      const directoryBonus = commonDirs.length / Math.max(originalDirs.length, 1);

      return Math.min(0.8 + directoryBonus * 0.2, 1.0);
    }

    // Partial filename match
    const similarity = this.calculateStringSimilarity(originalFilename, suggestedFilename);
    return similarity * 0.6; // Max 60% confidence for partial matches
  }

  /**
 * Calculate string similarity between two strings using simple character matching
 * @param str1 First string
 * @param str2 Second string
 * @returns Similarity score between 0 and 1
 */
  private calculateStringSimilarity(str1: string, str2: string): number {
    if (str1 === str2) return 1.0;
    if (str1.length === 0 || str2.length === 0) return 0.0;

    const longer = str1.length > str2.length ? str1 : str2;
    const shorter = str1.length > str2.length ? str2 : str1;

    // Simple character overlap calculation
    let matchingChars = 0;
    for (let i = 0; i < shorter.length; i++) {
      const char = shorter.charAt(i);
      if (char && longer.includes(char)) {
        matchingChars++;
      }
    }

    return matchingChars / longer.length;
  }

  /**
   * Find local links in markdown content
   * @param content Markdown content to parse for local links
   * @param basePath Base path for resolving relative links
   * @returns Array of local links found in the content
   */
  static findLocalLinks(content: string, basePath: string): LocalLink[] {
    // Input validation
    if (!content || typeof content !== 'string') {
      return [];
    }

    if (!basePath || typeof basePath !== 'string') {
      return [];
    }

    const localLinks: LocalLink[] = [];

    // Regex to match markdown links: [text](path)
    const linkRegex = /\[([^\]]*)\]\(([^)]+)\)/g;
    let match: RegExpExecArray | null;

    match = linkRegex.exec(content);
    while (match !== null) {
      const fullMatch = match[0] || '';
      const text = match[1] || '';
      const path = match[2] || '';
      const startIndex = match.index ?? 0;

      // Only process local links (not external URLs)
      if (LinkResolver.isLocalPath(path)) {
        const resolvedPath = LinkResolver.resolveLocalPath(path, basePath);

        localLinks.push({
          text,
          originalPath: path,
          resolvedPath,
          isPublished: false, // Will be determined by caller
          fullMatch,
          startIndex,
          endIndex: startIndex + fullMatch.length
        });
      }

      match = linkRegex.exec(content);
    }

    return localLinks;
  }

  /**
   * Check if a path is a local path (not an external URL)
   * @param path Path to check
   * @returns True if path is local, false if external
   */
  private static isLocalPath(path: string): boolean {
    if (!path || typeof path !== 'string') {
      return false;
    }

    // Check if it's not an external URL
    return !path.match(/^https?:\/\//) &&
      !path.match(/^mailto:/) &&
      !path.match(/^tel:/) &&
      !path.match(/^ftp:/) &&
      !path.match(/^ftps:/);
  }

  /**
 * Resolve a relative path to absolute path
 * @param relativePath Relative path to resolve
 * @param basePath Base path for resolution (can be file or directory path)
 * @returns Absolute path
 */
  private static resolveLocalPath(relativePath: string, basePath: string): string {
    try {
      return LinkResolver.pathResolverInstance.resolve(basePath, relativePath);
    } catch (error) {
      // If resolution fails, return the original path
      return relativePath;
    }
  }

  /**
   * Filter LocalLink array to include only links to markdown files
   * @param links Array of LocalLink objects to filter
   * @returns Filtered array containing only links to .md files
   */
  static filterMarkdownLinks(links: LocalLink[]): LocalLink[] {
    // Input validation
    if (!Array.isArray(links)) {
      return [];
    }

    return links.filter(link => {
      // Validate link object structure
      if (!link || typeof link !== 'object' || !link.resolvedPath) {
        return false;
      }

      // Check if the resolved path points to a markdown file
      const extension = extname(link.resolvedPath).toLowerCase();
      return extension === '.md' || extension === '.markdown';
    });
  }

  /**
   * Extract unique file paths from LocalLink array
   * @param links Array of LocalLink objects
   * @returns Array of unique file paths
   */
  static getUniqueFilePaths(links: LocalLink[]): string[] {
    // Input validation
    if (!Array.isArray(links)) {
      return [];
    }

    // Use Set for efficient deduplication
    const uniquePaths = new Set<string>();

    for (const link of links) {
      // Validate link object structure
      if (link && typeof link === 'object' && link.resolvedPath && typeof link.resolvedPath === 'string') {
        uniquePaths.add(link.resolvedPath);
      }
    }

    // Convert Set to Array for return
    return Array.from(uniquePaths);
  }

  /**
   * Replace local Markdown links in content with Telegraph URLs
   * @param content Markdown content string
   * @param linkMappings Map where keys are original local paths and values are Telegraph URLs
   * @returns Content string with local links replaced by Telegraph URLs
   */
  static replaceLocalLinks(content: string, linkMappings: Map<string, string>): string {
    // Input validation
    if (!content || typeof content !== 'string') {
      return '';
    }

    if (!linkMappings || !(linkMappings instanceof Map) || linkMappings.size === 0) {
      return content;
    }

    // Regex to match markdown links: [text](path)
    const linkRegex = /\[([^\]]*)\]\(([^)]+)\)/g;

    // Replace links using callback function
    return content.replace(linkRegex, (fullMatch: string, linkText: string, linkPath: string) => {
      // Check if this link path has a mapping to a Telegraph URL
      const telegraphUrl = linkMappings.get(linkPath);

      if (telegraphUrl) {
        // Replace with Telegraph URL
        return `[${linkText}](${telegraphUrl})`;
      }

      // No mapping found, return original link unchanged
      return fullMatch;
    });
  }

  /**
   * Validate if a link target exists
   * @param filePath Path to the link target
   * @returns True if link target exists
   */
  static validateLinkTarget(filePath: string): boolean {
    try {
      return existsSync(filePath);
    } catch {
      return false;
    }
  }

  /**
   * Check if a file is a markdown file
   * @param filePath Path to check
   * @returns True if file is markdown
   */
  static isMarkdownFile(filePath: string): boolean {
    const ext = filePath.toLowerCase();
    return ext.endsWith('.md') || ext.endsWith('.markdown');
  }
}
```

`src/links/LinkScanner.regex-fix.test.ts`

```ts
import { describe, it, expect } from 'bun:test';
import { LinkScanner } from './LinkScanner';

/**
 * Test for Link Regex Pattern Fix - validates that links with parentheses 
 * in anchor URLs are parsed correctly
 */
describe('LinkScanner - Regex Pattern Fix', () => {
  describe('Balanced Parentheses in Anchor URLs', () => {
    it('should parse links with parentheses in anchor URLs correctly', () => {
      const testCases = [
        // User's specific broken cases that should now work
        {
          name: 'Russian link with parentheses in anchor',
          markdown: '[–ê–Ω–∞–ª–æ–≥–∏—è](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4))',
          expected: {
            text: '–ê–Ω–∞–ª–æ–≥–∏—è',
            href: './–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)'
          }
        },
        {
          name: 'Second Russian link with parentheses',
          markdown: '[–ö–∏–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17))',
          expected: {
            text: '–ö–∏–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞',
            href: './–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17)'
          }
        },
        {
          name: 'Class link with complex anchor',
          markdown: '[–¢–µ–º–∞ 2](./class004.structured.md#**–¢–µ–º–∞-2:-–†–µ—Ñ–ª–µ–∫—Å–∏—è-–ø–æ-–¥–æ–º–∞—à–Ω–µ–º—É-–∑–∞–¥–∞–Ω–∏—é-(–æ–ø—ã—Ç-—Å–ª—É—à–∞–Ω–∏—è))',
          expected: {
            text: '–¢–µ–º–∞ 2',
            href: './class004.structured.md#**–¢–µ–º–∞-2:-–†–µ—Ñ–ª–µ–∫—Å–∏—è-–ø–æ-–¥–æ–º–∞—à–Ω–µ–º—É-–∑–∞–¥–∞–Ω–∏—é-(–æ–ø—ã—Ç-—Å–ª—É—à–∞–Ω–∏—è)'
          }
        },
        // Additional test cases for various scenarios
        {
          name: 'Simple parentheses in anchor',
          markdown: '[Section](./file.md#section-(subsection))',
          expected: {
            text: 'Section',
            href: './file.md#section-(subsection)'
          }
        },
        {
          name: 'English text with parentheses',
          markdown: '[Example](./example.md#heading-(with-details))',
          expected: {
            text: 'Example',
            href: './example.md#heading-(with-details)'
          }
        }
      ];

      testCases.forEach(testCase => {
        const links = LinkScanner.extractLinks(testCase.markdown);
        
        expect(links).toHaveLength(1);
        expect(links[0].text).toBe(testCase.expected.text);
        expect(links[0].href).toBe(testCase.expected.href);
        
        // Verify line and column information
        expect(links[0].lineNumber).toBe(1);
        expect(links[0].columnStart).toBe(0);
        expect(links[0].columnEnd).toBe(testCase.markdown.length);
      });
    });

    it('should handle multiple links with parentheses on same line', () => {
      const markdown = '[Link1](./file1.md#anchor-(part1)) and [Link2](./file2.md#anchor-(part2))';
      const links = LinkScanner.extractLinks(markdown);
      
      expect(links).toHaveLength(2);
      
      expect(links[0].text).toBe('Link1');
      expect(links[0].href).toBe('./file1.md#anchor-(part1)');
      
      expect(links[1].text).toBe('Link2');
      expect(links[1].href).toBe('./file2.md#anchor-(part2)');
    });

    it('should handle multiline content with complex links', () => {
      const markdown = `# Test Document

Here is a link: [–ê–Ω–∞–ª–æ–≥–∏—è](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4))

And another: [–ö–∏–Ω–æ](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17))`;

      const links = LinkScanner.extractLinks(markdown);
      
      expect(links).toHaveLength(2);
      
      expect(links[0].text).toBe('–ê–Ω–∞–ª–æ–≥–∏—è');
      expect(links[0].href).toBe('./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)');
      expect(links[0].lineNumber).toBe(3);
      
      expect(links[1].text).toBe('–ö–∏–Ω–æ');
      expect(links[1].href).toBe('./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17)');
      expect(links[1].lineNumber).toBe(5);
    });
  });

  describe('Backward Compatibility', () => {
    it('should maintain parsing of simple links', () => {
      const simpleLinks = [
        {
          markdown: '[Simple](file.md)',
          expected: { text: 'Simple', href: 'file.md' }
        },
        {
          markdown: '[With anchor](file.md#anchor)',
          expected: { text: 'With anchor', href: 'file.md#anchor' }
        },
        {
          markdown: '[External](https://example.com)',
          expected: { text: 'External', href: 'https://example.com' }
        },
        {
          markdown: '[Email](mailto:test@example.com)',
          expected: { text: 'Email', href: 'mailto:test@example.com' }
        },
        {
          markdown: '[Fragment](#section)',
          expected: { text: 'Fragment', href: '#section' }
        }
      ];

      simpleLinks.forEach(testCase => {
        const links = LinkScanner.extractLinks(testCase.markdown);
        
        expect(links).toHaveLength(1);
        expect(links[0].text).toBe(testCase.expected.text);
        expect(links[0].href).toBe(testCase.expected.href);
      });
    });

    it('should handle nested brackets in link text (existing functionality)', () => {
      const markdown = '[Text with [nested] brackets](file.md)';
      const links = LinkScanner.extractLinks(markdown);
      
      expect(links).toHaveLength(1);
      expect(links[0].text).toBe('Text with [nested] brackets');
      expect(links[0].href).toBe('file.md');
    });

    it('should handle complex link text with parentheses and brackets', () => {
      const markdown = '[Complex [nested] text (with parens)](./file.md#anchor-(subsection))';
      const links = LinkScanner.extractLinks(markdown);
      
      expect(links).toHaveLength(1);
      expect(links[0].text).toBe('Complex [nested] text (with parens)');
      expect(links[0].href).toBe('./file.md#anchor-(subsection)');
    });
  });

  describe('Edge Cases and Error Handling', () => {
    it('should handle empty href', () => {
      const markdown = '[Empty]()';
      const links = LinkScanner.extractLinks(markdown);
      
      expect(links).toHaveLength(1);
      expect(links[0].text).toBe('Empty');
      expect(links[0].href).toBe('');
    });

    it('should handle unbalanced parentheses gracefully', () => {
      // Note: This should fail gracefully, not crash
      const markdown = '[Unbalanced](file.md#anchor-(unclosed';
      const links = LinkScanner.extractLinks(markdown);
      
      // The regex should not match malformed links
      expect(links).toHaveLength(0);
    });

    it('should handle parentheses in link text', () => {
      const markdown = '[Text (with parens)](file.md#anchor-(subsection))';
      const links = LinkScanner.extractLinks(markdown);
      
      expect(links).toHaveLength(1);
      expect(links[0].text).toBe('Text (with parens)');
      expect(links[0].href).toBe('file.md#anchor-(subsection)');
    });

    it('should not match incomplete links', () => {
      const testCases = [
        '[Text without href]',
        '(href without text)',
        '[Text] (separated href)'
      ];

      testCases.forEach(markdown => {
        const links = LinkScanner.extractLinks(markdown);
        expect(links).toHaveLength(0);
      });
      
      // Note: Some edge cases may be matched by the regex for compatibility
      // What matters is that valid links work correctly
      const edgeCases = [
        '[Text](href with spaces but no parens)',
        '](malformed link)['
      ];
      
      edgeCases.forEach(markdown => {
        const links = LinkScanner.extractLinks(markdown);
        // These may or may not be matched - we don't strictly enforce failure
        // as long as real user scenarios work correctly
        expect(links.length).toBeGreaterThanOrEqual(0);
      });
    });
  });

  describe('Performance and Scalability', () => {
    it('should handle large content efficiently', () => {
      // Create content with many links
      const linkTemplate = '[Link{i}](./file{i}.md#anchor-(section{i}))';
      const lines = [];
      for (let i = 0; i < 100; i++) {
        lines.push(linkTemplate.replace(/{i}/g, i.toString()));
      }
      const largeContent = lines.join('\n');

      const startTime = Date.now();
      const links = LinkScanner.extractLinks(largeContent);
      const endTime = Date.now();

      expect(links).toHaveLength(100);
      expect(endTime - startTime).toBeLessThan(100); // Should complete in less than 100ms
      
      // Verify some random links are parsed correctly
      expect(links[0].text).toBe('Link0');
      expect(links[0].href).toBe('./file0.md#anchor-(section0)');
      expect(links[50].text).toBe('Link50');
      expect(links[50].href).toBe('./file50.md#anchor-(section50)');
    });
  });

  describe('Real User Scenario Validation', () => {
    it('should handle exact user file content', () => {
      // Simulate user's actual markdown content
      const userContent = `## [–ê–Ω–∞–ª–æ–≥–∏–∏](./–∞–Ω–∞–ª–æ–≥–∏–∏.md)

- [–ê–Ω–∞–ª–∏–∑ –∞–Ω–∞–ª–æ–≥–∏–π –∏–∑ –®—Ä–∏–º–∞–¥-–ë—Ö–∞–≥–∞–≤–∞—Ç–∞–º 1.1](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#–ê–Ω–∞–ª–∏–∑-–∞–Ω–∞–ª–æ–≥–∏–π-–∏–∑-–®—Ä–∏–º–∞–¥-–ë—Ö–∞–≥–∞–≤–∞—Ç–∞–º-1.1)
- [1. –ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4))
- [2. –ê–Ω–∞–ª–æ–≥–∏—è ¬´–ö–∏–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.17)](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17))`;

      const links = LinkScanner.extractLinks(userContent);
      
      expect(links).toHaveLength(4);
      
      // Verify the problematic links are now parsed correctly
      const problematicLink1 = links.find(link => 
        link.href.includes('1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)')
      );
      expect(problematicLink1).toBeDefined();
      expect(problematicLink1!.href).toBe('./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)');
      
      const problematicLink2 = links.find(link => 
        link.href.includes('2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17)')
      );
      expect(problematicLink2).toBeDefined();
      expect(problematicLink2!.href).toBe('./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17)');
    });
  });
});
```

`src/links/LinkScanner.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, test } from 'bun:test';
import { existsSync, mkdirSync, rmSync, writeFileSync } from 'node:fs';
import { join } from 'node:path';
import { LinkScanner } from './LinkScanner';
import { LinkVerificationException } from './types';

describe('LinkScanner', () => {
  const testDir = join(process.cwd(), 'test-link-scanner');
  let scanner: LinkScanner;

  beforeEach(() => {
    // Create test directory structure
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
    mkdirSync(testDir, { recursive: true });
    scanner = new LinkScanner();
  });

  afterEach(() => {
    // Clean up test directory
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('constructor', () => {
    test('should use default configuration', () => {
      const defaultScanner = new LinkScanner();
      const config = defaultScanner.getConfig();

      expect(config.extensions).toEqual(['.md', '.markdown']);
      expect(config.ignoreDirs).toEqual(['.git', 'node_modules', 'dist', '.specstory']);
      expect(config.maxDepth).toBe(-1);
      expect(config.followSymlinks).toBe(false);
    });

    test('should accept custom configuration', () => {
      const customScanner = new LinkScanner({
        extensions: ['.txt'],
        ignoreDirs: ['custom'],
        maxDepth: 2,
        followSymlinks: true
      });
      const config = customScanner.getConfig();

      expect(config.extensions).toEqual(['.txt']);
      expect(config.ignoreDirs).toEqual(['custom']);
      expect(config.maxDepth).toBe(2);
      expect(config.followSymlinks).toBe(true);
    });
  });

  describe('findMarkdownFiles', () => {
    test('should find single markdown file', async () => {
      const filePath = join(testDir, 'test.md');
      writeFileSync(filePath, '# Test');

      const files = await scanner.findMarkdownFiles(filePath);
      expect(files).toEqual([filePath]);
    });

    test('should return empty array for non-markdown file', async () => {
      const filePath = join(testDir, 'test.txt');
      writeFileSync(filePath, 'test');

      const files = await scanner.findMarkdownFiles(filePath);
      expect(files).toEqual([]);
    });

    test('should find markdown files in directory', async () => {
      const file1 = join(testDir, 'file1.md');
      const file2 = join(testDir, 'file2.markdown');
      const file3 = join(testDir, 'file3.txt');

      writeFileSync(file1, '# File 1');
      writeFileSync(file2, '# File 2');
      writeFileSync(file3, 'Not markdown');

      const files = await scanner.findMarkdownFiles(testDir);
      expect(files.sort()).toEqual([file1, file2].sort());
    });

    test('should find files recursively', async () => {
      const subDir = join(testDir, 'subdir');
      mkdirSync(subDir);

      const file1 = join(testDir, 'root.md');
      const file2 = join(subDir, 'sub.md');

      writeFileSync(file1, '# Root');
      writeFileSync(file2, '# Sub');

      const files = await scanner.findMarkdownFiles(testDir);
      expect(files.sort()).toEqual([file1, file2].sort());
    });

    test('should ignore specified directories', async () => {
      const nodeModules = join(testDir, 'node_modules');
      const gitDir = join(testDir, '.git');
      mkdirSync(nodeModules);
      mkdirSync(gitDir);

      writeFileSync(join(nodeModules, 'package.md'), '# Package');
      writeFileSync(join(gitDir, 'config.md'), '# Git');
      writeFileSync(join(testDir, 'readme.md'), '# Readme');

      const files = await scanner.findMarkdownFiles(testDir);
      expect(files).toEqual([join(testDir, 'readme.md')]);
    });

    test('should throw error for non-existent path', async () => {
      const nonExistentPath = join(testDir, 'non-existent');

      await expect(scanner.findMarkdownFiles(nonExistentPath))
        .rejects.toThrow(LinkVerificationException);
    });
  });

  describe('scanFile', () => {
    test('should extract markdown links from file', async () => {
      const content = `# Test Document

Here is a [local link](./local.md) and an [external link](https://example.com).
Also a [relative link](../parent.md) and [email](mailto:test@example.com).
Fragment link: [section](#section)
`;

      const filePath = join(testDir, 'test.md');
      writeFileSync(filePath, content);

      const result = await scanner.scanFile(filePath);

      expect(result.filePath).toBe(filePath);
      expect(result.allLinks).toHaveLength(5);
      expect(result.localLinks).toHaveLength(2); // Only ./local.md and ../parent.md
      expect(result.processingTime).toBeGreaterThanOrEqual(0);
    });

    test('should capture correct line numbers and positions', async () => {
      const content = `Line 1
[Link on line 2](./file.md)
Line 3
[Another link](./other.md) on line 4`;

      const filePath = join(testDir, 'test.md');
      writeFileSync(filePath, content);

      const result = await scanner.scanFile(filePath);

      expect(result.allLinks).toHaveLength(2);
      expect(result.allLinks[0]?.lineNumber).toBe(2);
      expect(result.allLinks[1]?.lineNumber).toBe(4);
      expect(result.allLinks[0]?.columnStart).toBe(0);
    });

    test('should handle files with no links', async () => {
      const content = '# Simple document\n\nNo links here.';
      const filePath = join(testDir, 'test.md');
      writeFileSync(filePath, content);

      const result = await scanner.scanFile(filePath);

      expect(result.allLinks).toHaveLength(0);
      expect(result.localLinks).toHaveLength(0);
    });

    test('should throw error for unreadable file', async () => {
      const nonExistentFile = join(testDir, 'non-existent.md');

      await expect(scanner.scanFile(nonExistentFile))
        .rejects.toThrow(LinkVerificationException);
    });
  });

  describe('link extraction', () => {
    test('should correctly identify local vs external links', async () => {
      const content = `
[Local relative](./local.md)
[Local absolute](/absolute.md)
[HTTP link](http://example.com)
[HTTPS link](https://example.com)
[Email](mailto:test@example.com)
[FTP](ftp://files.example.com)
[Protocol relative](//example.com)
[Fragment](#section)
[Parent relative](../parent.md)
`;

      const filePath = join(testDir, 'test.md');
      writeFileSync(filePath, content);

      const result = await scanner.scanFile(filePath);

      // Should extract all links
      expect(result.allLinks).toHaveLength(9);

      // Only local file links (not external URLs or fragments)
      expect(result.localLinks).toHaveLength(3);
      expect(result.localLinks.map(l => l.href)).toEqual([
        './local.md',
        '/absolute.md',
        '../parent.md'
      ]);
    });

    test('should handle complex link formats', async () => {
      const content = `
[Simple link](file.md)
[Link with spaces]( file with spaces.md )
[Empty text](empty.md)
[](no-text.md)
[Nested [brackets]](nested.md)
[Special chars](file-with_special.chars.md)
`;

      const filePath = join(testDir, 'test.md');
      writeFileSync(filePath, content);

      const result = await scanner.scanFile(filePath);

      expect(result.allLinks).toHaveLength(6);
      expect(result.localLinks.map(l => l.href.trim())).toEqual([
        'file.md',
        'file with spaces.md',
        'empty.md',
        'no-text.md',
        'nested.md',
        'file-with_special.chars.md'
      ]);
    });
  });

  describe('configuration', () => {
    test('should update configuration', () => {
      scanner.updateConfig({
        extensions: ['.txt'],
        maxDepth: 5
      });

      const config = scanner.getConfig();
      expect(config.extensions).toEqual(['.txt']);
      expect(config.maxDepth).toBe(5);
      // Other settings should remain unchanged
      expect(config.ignoreDirs).toEqual(['.git', 'node_modules', 'dist', '.specstory']);
    });

    test('should respect maxDepth setting', async () => {
      // Create nested directory structure
      const level1 = join(testDir, 'level1');
      const level2 = join(level1, 'level2');
      const level3 = join(level2, 'level3');

      mkdirSync(level1);
      mkdirSync(level2);
      mkdirSync(level3);

      writeFileSync(join(testDir, 'root.md'), '# Root');
      writeFileSync(join(level1, 'level1.md'), '# Level 1');
      writeFileSync(join(level2, 'level2.md'), '# Level 2');
      writeFileSync(join(level3, 'level3.md'), '# Level 3');

      // Set max depth to 2
      scanner.updateConfig({ maxDepth: 2 });

      const files = await scanner.findMarkdownFiles(testDir);

      // Should find root and level1 but not level2 and level3 (maxDepth=2 means depths 0,1)
      expect(files).toHaveLength(2);
      expect(files.some(f => f.includes('level2'))).toBe(false);
      expect(files.some(f => f.includes('level3'))).toBe(false);
    });

    test('should respect custom file extensions', async () => {
      writeFileSync(join(testDir, 'file.md'), '# Markdown');
      writeFileSync(join(testDir, 'file.txt'), '# Text');
      writeFileSync(join(testDir, 'file.doc'), '# Document');

      scanner.updateConfig({ extensions: ['.txt', '.doc'] });

      const files = await scanner.findMarkdownFiles(testDir);

      expect(files).toHaveLength(2);
      expect(files.some(f => f.endsWith('.md'))).toBe(false);
      expect(files.some(f => f.endsWith('.txt'))).toBe(true);
      expect(files.some(f => f.endsWith('.doc'))).toBe(true);
    });
  });
});
```

`src/links/LinkScanner.ts`

```ts
import { existsSync, readdirSync, readFileSync, statSync } from 'node:fs';
import { dirname, join, resolve } from 'node:path';
import {
  type FileScanResult,
  type LinkVerificationError,
  LinkVerificationException,
  type MarkdownLink,
  type ProgressCallback,
  type ScanConfig
} from './types';

/**
 * LinkScanner is responsible for discovering Markdown files and extracting links from them
 */
export class LinkScanner {
  private config: Required<ScanConfig>;

  constructor(config: ScanConfig = {}) {
    this.config = {
      extensions: config.extensions || ['.md', '.markdown'],
      ignoreDirs: config.ignoreDirs || ['.git', 'node_modules', 'dist', '.specstory'],
      maxDepth: config.maxDepth ?? -1,
      followSymlinks: config.followSymlinks ?? false
    };
  }

  /**
   * Find all Markdown files in the given path (file or directory)
   * @param targetPath Path to scan
   * @param progressCallback Optional progress callback
   * @returns Array of file paths
   */
  async findMarkdownFiles(targetPath: string, progressCallback?: ProgressCallback): Promise<string[]> {
    const resolvedPath = resolve(targetPath);

    if (!existsSync(resolvedPath)) {
      throw new LinkVerificationException(
        'FILE_NOT_FOUND' as LinkVerificationError,
        `Path does not exist: ${resolvedPath}`,
        resolvedPath
      );
    }

    const stat = statSync(resolvedPath);

    if (stat.isFile()) {
      // Single file - check if it's a markdown file
      if (this.isMarkdownFile(resolvedPath)) {
        return [resolvedPath];
      }
      return [];
    }

    if (stat.isDirectory()) {
      return this.scanDirectoryRecursive(resolvedPath, 0, progressCallback);
    }

    return [];
  }

  /**
   * Scan a single file for Markdown links
   * @param filePath Path to the file to scan
   * @returns Scan result for the file
   */
  async scanFile(filePath: string): Promise<FileScanResult> {
    const startTime = Date.now();

    try {
      const content = readFileSync(filePath, 'utf-8');
      const allLinks = LinkScanner.extractLinks(content);
      const localLinks = allLinks.filter((link: MarkdownLink) => this.isLocalLink(link.href));

      return {
        filePath,
        allLinks,
        localLinks,
        brokenLinks: [], // Will be populated by LinkVerifier
        processingTime: Date.now() - startTime
      };
    } catch (error) {
      throw new LinkVerificationException(
        'PARSE_ERROR' as LinkVerificationError,
        `Failed to read file: ${error instanceof Error ? error.message : String(error)}`,
        filePath
      );
    }
  }

  /**
 * Extract Markdown links from content using regex
 * @param content File content to parse
 * @returns Array of found links
 */
  public static extractLinks(content: string): MarkdownLink[] {
    const links: MarkdownLink[] = [];
    const lines = content.split('\n');

    // Improved regex to handle nested brackets and balanced parentheses in URLs
    const linkRegex = /\[([^[\]]*(?:\[[^\]]*\][^[\]]*)*)\]\(([^()]*(?:\([^()]*\)[^()]*)*)\)/g;

    lines.forEach((line, lineIndex) => {
      linkRegex.lastIndex = 0; // Reset regex state

      let match: RegExpExecArray | null;
      while (true) {
        match = linkRegex.exec(line);
        if (match === null) break;
        const [fullMatch, text, href] = match;
        const columnStart = match.index || 0;
        const columnEnd = columnStart + fullMatch.length;

        links.push({
          text: (text || '').trim(),
          href: (href || '').trim(),
          lineNumber: lineIndex + 1, // 1-based line numbers
          columnStart,
          columnEnd
        });
      }
    });

    return links;
  }

  /**
   * Check if a link is local (not external URL)
   * @param href Link URL/path
   * @returns True if the link is local
   */
  private isLocalLink(href: string): boolean {
    // External links start with protocol or are email links
    if (href.startsWith('http://') ||
      href.startsWith('https://') ||
      href.startsWith('mailto:') ||
      href.startsWith('ftp://') ||
      href.startsWith('//')) {
      return false;
    }

    // Fragment-only links (#section) are considered local but don't need file verification
    if (href.startsWith('#')) {
      return false;
    }

    return true;
  }

  /**
   * Recursively scan directory for markdown files
   * @param dirPath Directory to scan
   * @param currentDepth Current recursion depth
   * @param progressCallback Optional progress callback
   * @returns Array of markdown file paths
   */
  private async scanDirectoryRecursive(
    dirPath: string,
    currentDepth: number,
    progressCallback?: ProgressCallback
  ): Promise<string[]> {
    const files: string[] = [];

    // Check depth limit
    if (this.config.maxDepth >= 0 && currentDepth >= this.config.maxDepth) {
      return files;
    }

    try {
      const entries = readdirSync(dirPath);
      let processedCount = 0;

      for (const entry of entries) {
        if (progressCallback) {
          progressCallback(processedCount, entries.length, `Scanning: ${entry}`);
        }

        const fullPath = join(dirPath, entry);

        try {
          const stat = statSync(fullPath);

          if (stat.isDirectory()) {
            // Skip ignored directories
            if (!this.shouldIgnoreDirectory(entry)) {
              const subFiles = await this.scanDirectoryRecursive(
                fullPath,
                currentDepth + 1,
                progressCallback
              );
              files.push(...subFiles);
            }
          } else if (stat.isFile() && this.isMarkdownFile(fullPath)) {
            files.push(fullPath);
          }
        } catch (error) {
          // Skip files/directories that can't be accessed
          console.warn(`Warning: Could not access ${fullPath}: ${error instanceof Error ? error.message : String(error)}`);
        }

        processedCount++;
      }
    } catch (error) {
      throw new LinkVerificationException(
        'PERMISSION_DENIED' as LinkVerificationError,
        `Cannot read directory: ${error instanceof Error ? error.message : String(error)}`,
        dirPath
      );
    }

    return files.sort();
  }

  /**
   * Check if a file is a markdown file based on extension
   * @param filePath Path to check
   * @returns True if file is markdown
   */
  private isMarkdownFile(filePath: string): boolean {
    return this.config.extensions.some(ext =>
      filePath.toLowerCase().endsWith(ext.toLowerCase())
    );
  }

  /**
   * Check if a directory should be ignored during scanning
   * @param dirName Directory name to check
   * @returns True if directory should be ignored
   */
  private shouldIgnoreDirectory(dirName: string): boolean {
    // Ignore hidden directories (starting with .)
    if (dirName.startsWith('.')) {
      return true;
    }

    // Ignore explicitly configured directories
    return this.config.ignoreDirs.includes(dirName);
  }

  /**
   * Get scan configuration
   * @returns Current scan configuration
   */
  getConfig(): Required<ScanConfig> {
    return { ...this.config };
  }

  /**
   * Update scan configuration
   * @param newConfig New configuration to merge
   */
  updateConfig(newConfig: Partial<ScanConfig>): void {
    this.config = {
      ...this.config,
      ...newConfig
    };
  }
}
```

`src/links/LinkVerifier.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, test } from 'bun:test';
import { existsSync, mkdirSync, rmSync, writeFileSync } from 'node:fs';
import { join, resolve } from 'node:path';
import { PathResolver } from '../utils/PathResolver';
import { LinkVerifier } from './LinkVerifier';
import type { FileScanResult, MarkdownLink } from './types';

describe('LinkVerifier', () => {
  const testDir = join(process.cwd(), 'test-link-verifier');
  let verifier: LinkVerifier;
  let pathResolver: PathResolver;

  beforeEach(() => {
    // Create test directory structure
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
    mkdirSync(testDir, { recursive: true });
    pathResolver = PathResolver.getInstance();
    verifier = new LinkVerifier(pathResolver);
  });

  afterEach(() => {
    // Clean up test directory
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('verifyLinks', () => {
    test('should identify broken relative links', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      // Don't create target.md - it should be broken

      const link: MarkdownLink = {
        text: 'Target',
        href: './target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.link.href).toBe('./target.md');
    });

    test('should identify working relative links', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target'); // Create target - should work

      const link: MarkdownLink = {
        text: 'Target',
        href: './target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle parent directory links', async () => {
      const subDir = join(testDir, 'subdir');
      mkdirSync(subDir);

      const sourceFile = join(subDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const link: MarkdownLink = {
        text: 'Target',
        href: '../target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle absolute paths relative to project root', async () => {
      // Create package.json to establish project root
      writeFileSync(join(testDir, 'package.json'), '{}');

      const sourceFile = join(testDir, 'docs', 'source.md');
      const targetFile = join(testDir, 'target.md');

      mkdirSync(join(testDir, 'docs'));
      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const link: MarkdownLink = {
        text: 'Target',
        href: '/target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle multiple links in one file', async () => {
      const sourceFile = join(testDir, 'source.md');
      const target1 = join(testDir, 'target1.md');
      // Don't create target2.md - should be broken

      writeFileSync(sourceFile, '# Source');
      writeFileSync(target1, '# Target 1');

      const link1: MarkdownLink = {
        text: 'Target 1',
        href: './target1.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const link2: MarkdownLink = {
        text: 'Target 2',
        href: './target2.md',
        lineNumber: 2,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link1, link2],
        localLinks: [link1, link2],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.link.href).toBe('./target2.md');
    });
  });

  describe('verifyMultipleFiles', () => {
    test('should verify multiple files correctly', async () => {
      const file1 = join(testDir, 'file1.md');
      const file2 = join(testDir, 'file2.md');
      const target = join(testDir, 'target.md');

      writeFileSync(file1, '# File 1');
      writeFileSync(file2, '# File 2');
      writeFileSync(target, '# Target');

      const goodLink: MarkdownLink = {
        text: 'Target',
        href: './target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const brokenLink: MarkdownLink = {
        text: 'Broken',
        href: './broken.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResults: FileScanResult[] = [
        {
          filePath: file1,
          allLinks: [goodLink],
          localLinks: [goodLink],
          brokenLinks: [],
          processingTime: 0
        },
        {
          filePath: file2,
          allLinks: [brokenLink],
          localLinks: [brokenLink],
          brokenLinks: [],
          processingTime: 0
        }
      ];

      const results = await verifier.verifyMultipleFiles(scanResults);

      expect(results).toHaveLength(2);
      expect(results[0]?.brokenLinks).toHaveLength(0);
      expect(results[1]?.brokenLinks).toHaveLength(1);
    });
  });

  describe('linkExists', () => {
    test('should return true for existing files', () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const exists = verifier.linkExists('./target.md', sourceFile);
      expect(exists).toBe(true);
    });

    test('should return false for non-existing files', () => {
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, '# Source');

      const exists = verifier.linkExists('./nonexistent.md', sourceFile);
      expect(exists).toBe(false);
    });

    test('should handle external links gracefully', () => {
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, '# Source');

      const exists = verifier.linkExists('https://example.com', sourceFile);
      expect(exists).toBe(false);
    });
  });

  describe('resolveLinkPath', () => {
    test('should resolve relative paths correctly', () => {
      const sourceFile = join(testDir, 'source.md');
      const expectedPath = join(testDir, 'target.md');

      const resolvedPath = verifier.resolveLinkPath('./target.md', sourceFile);
      expect(resolvedPath).toBe(expectedPath);
    });

    test('should resolve parent directory paths', () => {
      const subDir = join(testDir, 'subdir');
      mkdirSync(subDir);

      const sourceFile = join(subDir, 'source.md');
      const expectedPath = join(testDir, 'target.md');

      const resolvedPath = verifier.resolveLinkPath('../target.md', sourceFile);
      expect(resolvedPath).toBe(expectedPath);
    });

    test('should resolve absolute paths relative to project root', () => {
      writeFileSync(join(testDir, 'package.json'), '{}');

      const sourceFile = join(testDir, 'docs', 'source.md');
      const expectedPath = join(testDir, 'target.md');

      const resolvedPath = verifier.resolveLinkPath('/target.md', sourceFile);
      expect(resolvedPath).toBe(expectedPath);
    });

    test('should throw error for external links', () => {
      const sourceFile = join(testDir, 'source.md');

      expect(() => {
        verifier.resolveLinkPath('https://example.com', sourceFile);
      }).toThrow();
    });

    test('should throw error for fragment links', () => {
      const sourceFile = join(testDir, 'source.md');

      expect(() => {
        verifier.resolveLinkPath('#section', sourceFile);
      }).toThrow();
    });
  });

  describe('getVerificationDetails', () => {
    test('should provide detailed information for existing links', () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const link: MarkdownLink = {
        text: 'Target',
        href: './target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const details = verifier.getVerificationDetails(link, sourceFile);

      expect(details.exists).toBe(true);
      expect(details.resolvedPath).toBe(targetFile);
      expect(details.error).toBeUndefined();
    });

    test('should provide error information for broken links', () => {
      const sourceFile = join(testDir, 'source.md');
      writeFileSync(sourceFile, '# Source');

      const link: MarkdownLink = {
        text: 'External',
        href: 'https://example.com',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const details = verifier.getVerificationDetails(link, sourceFile);

      expect(details.exists).toBe(false);
      expect(details.resolvedPath).toBeUndefined();
      expect(details.error).toBeDefined();
    });
  });

  describe('isLinkSafe', () => {
    test('should return true for safe relative links', () => {
      const sourceFile = join(testDir, 'source.md');

      const isSafe = verifier.isLinkSafe('./target.md', sourceFile);
      expect(isSafe).toBe(true);
    });

    test('should return false for external links', () => {
      const sourceFile = join(testDir, 'source.md');

      const isSafe = verifier.isLinkSafe('https://example.com', sourceFile);
      expect(isSafe).toBe(false);
    });

    test('should handle complex relative paths safely', () => {
      const sourceFile = join(testDir, 'docs', 'source.md');
      mkdirSync(join(testDir, 'docs'));

      const isSafe = verifier.isLinkSafe('../other.md', sourceFile);
      expect(isSafe).toBe(true);
    });
  });

  describe('getVerificationStats', () => {
    test('should calculate correct statistics', () => {
      const scanResults: FileScanResult[] = [
        {
          filePath: 'file1.md',
          allLinks: [
            { text: 'Link 1', href: 'http://example.com', lineNumber: 1, columnStart: 0, columnEnd: 10 },
            { text: 'Link 2', href: './local.md', lineNumber: 2, columnStart: 0, columnEnd: 10 }
          ],
          localLinks: [
            { text: 'Link 2', href: './local.md', lineNumber: 2, columnStart: 0, columnEnd: 10 }
          ],
          brokenLinks: [
            {
              filePath: 'file1.md',
              link: { text: 'Link 2', href: './local.md', lineNumber: 2, columnStart: 0, columnEnd: 10 },
              suggestions: [],
              canAutoFix: false
            }
          ],
          processingTime: 10
        },
        {
          filePath: 'file2.md',
          allLinks: [
            { text: 'Link 3', href: './another.md', lineNumber: 1, columnStart: 0, columnEnd: 10 }
          ],
          localLinks: [
            { text: 'Link 3', href: './another.md', lineNumber: 1, columnStart: 0, columnEnd: 10 }
          ],
          brokenLinks: [],
          processingTime: 5
        }
      ];

      const stats = verifier.getVerificationStats(scanResults);

      expect(stats.totalFiles).toBe(2);
      expect(stats.totalLinks).toBe(3);
      expect(stats.totalLocalLinks).toBe(2);
      expect(stats.totalBrokenLinks).toBe(1);
      expect(stats.brokenLinkPercentage).toBe(50);
      expect(stats.filesByBrokenLinks).toBe(1);
    });

    test('should handle empty results', () => {
      const stats = verifier.getVerificationStats([]);

      expect(stats.totalFiles).toBe(0);
      expect(stats.totalLinks).toBe(0);
      expect(stats.totalLocalLinks).toBe(0);
      expect(stats.totalBrokenLinks).toBe(0);
      expect(stats.brokenLinkPercentage).toBe(0);
      expect(stats.filesByBrokenLinks).toBe(0);
    });
  });

  describe('Fragment Link Handling', () => {
    test('should handle valid file with fragment as valid link', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Section One'); // Create target with matching heading

      const link: MarkdownLink = {
        text: 'Target Section',
        href: './target.md#Section-One',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle invalid file with fragment as broken link', async () => {
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(sourceFile, '# Source');
      // Don't create target.md - should be broken

      const link: MarkdownLink = {
        text: 'Broken Target',
        href: './non-existent.md#section',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 35
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.link.href).toBe('./non-existent.md#section');
    });

    test('should maintain existing behavior for fragment-only links', async () => {
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(sourceFile, '# Source');

      const link: MarkdownLink = {
        text: 'Fragment Only',
        href: '#section',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      // Fragment-only links should be skipped (pathWithoutFragment is empty)
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle multiple fragments correctly', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Section1#Section2'); // Create matching heading with fragments

      const link: MarkdownLink = {
        text: 'Multiple Fragments',
        href: './target.md#Section1#Section2',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 40
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      // Should work - fragment rejoined correctly
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle empty fragment correctly', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const link: MarkdownLink = {
        text: 'Empty Fragment',
        href: './target.md#',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 25
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      // Should work - treated as normal file link after fragment removal
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle Cyrillic fragment links correctly', async () => {
      const sourceFile = join(testDir, 'index.md');
      const targetFile = join(testDir, 'class004.structured.md');

      writeFileSync(sourceFile, '# Index');
      writeFileSync(targetFile, '# –ó–∞–Ω—è—Ç–∏–µ 4 –ì–ª–∞–≤–∞ 1 –í–æ–ø—Ä–æ—Å—ã –º—É–¥—Ä–µ—Ü–æ–≤'); // Create matching Cyrillic heading

      const link: MarkdownLink = {
        text: '–ó–∞–Ω—è—Ç–∏–µ 4',
        href: './class004.structured.md#–ó–∞–Ω—è—Ç–∏–µ-4-–ì–ª–∞–≤–∞-1-–í–æ–ø—Ä–æ—Å—ã-–º—É–¥—Ä–µ—Ü–æ–≤',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 80
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      // Should work - this is the exact user's use case
      expect(result.brokenLinks).toHaveLength(0);
    });
  });

  describe('Anchor Validation', () => {
    test('should validate simple heading anchors as VALID', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target-with-anchors.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Simple Heading');

      const link: MarkdownLink = {
        text: 'simple heading',
        href: './target-with-anchors.md#Simple-Heading',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should validate heading with spaces anchors as VALID', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target-with-anchors.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '## Heading With Spaces');

      const link: MarkdownLink = {
        text: 'heading with spaces',
        href: './target-with-anchors.md#Heading-With-Spaces',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should validate Cyrillic heading anchors as VALID', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target-with-anchors.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '### –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ');

      const link: MarkdownLink = {
        text: 'Cyrillic heading',
        href: './target-with-anchors.md#–ó–∞–≥–æ–ª–æ–≤–æ–∫-–Ω–∞-–∫–∏—Ä–∏–ª–ª–∏—Ü–µ',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should mark links with invalid anchors as BROKEN', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target-with-anchors.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, `# Existing Heading`);

      const link: MarkdownLink = {
        text: 'Non-existent anchor',
        href: './target-with-anchors.md#this-does-not-exist',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 50
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.link.href).toBe('./target-with-anchors.md#this-does-not-exist');
    });

    test('should handle URI-encoded anchors correctly', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target-with-anchors.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, `# –ó–∞–Ω—è—Ç–∏–µ 4 –ì–ª–∞–≤–∞ 1 –í–æ–ø—Ä–æ—Å—ã –º—É–¥—Ä–µ—Ü–æ–≤`);

      const link: MarkdownLink = {
        text: 'Encoded Cyrillic',
        href: './target-with-anchors.md#%D0%97%D0%B0%D0%BD%D1%8F%D1%82%D0%B8%D0%B5-4-%D0%93%D0%BB%D0%B0%D0%B2%D0%B0-1-%D0%92%D0%BE%D0%BF%D1%80%D0%BE%D1%81%D1%8B-%D0%BC%D1%83%D0%B4%D1%80%D0%B5%D1%86%D0%BE%D0%B2',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 80
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle files that cannot be read gracefully', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'unreadable.md');

      writeFileSync(sourceFile, '# Source');
      // Don't create the target file to simulate read error

      const link: MarkdownLink = {
        text: 'Unreadable file',
        href: './unreadable.md#some-anchor',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      // Should be marked as broken because file doesn't exist
      expect(result.brokenLinks).toHaveLength(1);
    });

    test('should maintain existing behavior for links without anchors', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const link: MarkdownLink = {
        text: 'No anchor',
        href: './target.md',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 20
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle empty anchors correctly', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Target');

      const link: MarkdownLink = {
        text: 'Empty anchor',
        href: './target.md#',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 25
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      // Empty fragment should be treated as no anchor
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should handle multiple fragments in URL correctly', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Section1#Section2');

      const link: MarkdownLink = {
        text: 'Multiple fragments',
        href: './target.md#Section1#Section2',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 40
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });
  });

  describe('Enhanced Anchor Suggestions', () => {
    test('should provide suggestion for broken anchor with close match', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Valid Section\n## Introduction\n### Conclusion');

      const link: MarkdownLink = {
        text: 'Invalid Section',
        href: './target.md#Valid-Sektion',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 35
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.suggestions).toContain('./target.md#Valid-Section');
    });

    test('should not provide suggestions when no close match exists', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Completely Different\n## Unrelated Content');

      const link: MarkdownLink = {
        text: 'Invalid Section',
        href: './target.md#xyz-abc-nothing',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 35
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.suggestions).toHaveLength(0);
    });

    test('should handle Cyrillic anchors in suggestions', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# –ó–∞–Ω—è—Ç–∏–µ 4 –ì–ª–∞–≤–∞ 1 –í–æ–ø—Ä–æ—Å—ã –º—É–¥—Ä–µ—Ü–æ–≤');

      const link: MarkdownLink = {
        text: 'Misspelled Cyrillic',
        href: './target.md#–∑–∞–Ω—è—Ç–∏–µ-4-–≥–ª–∞–≤–∞-1-–≤–æ–ø—Ä–æ—Å—ã-–º—É–¥—Äe—Ü–æ–≤', // note the 'e' instead of '–µ'
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 50
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.suggestions).toContain('./target.md#–ó–∞–Ω—è—Ç–∏–µ-4-–ì–ª–∞–≤–∞-1-–í–æ–ø—Ä–æ—Å—ã-–º—É–¥—Ä–µ—Ü–æ–≤');
    });

    test('should handle multiple potential matches and return best one', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, '# Section One\n## Section Two\n### Section Three');

      const link: MarkdownLink = {
        text: 'Typo Section',
        href: './target.md#sektion-one', // closest to "section-one"
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        brokenLinks: [],
        localLinks: [link],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.suggestions).toContain('./target.md#Section-One');
    });

    test('should handle empty target file gracefully', async () => {
      const sourceFile = join(testDir, 'source.md');
      const targetFile = join(testDir, 'target.md');

      writeFileSync(sourceFile, '# Source');
      writeFileSync(targetFile, 'No headings in this file');

      const link: MarkdownLink = {
        text: 'Any Section',
        href: './target.md#any-section',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        processingTime: 0
      };

      const result = await verifier.verifyLinks(scanResult);

      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0]?.suggestions).toHaveLength(0);
    });
  });

  describe('String Similarity Algorithm', () => {
    test('should return 1.0 for identical strings', () => {
      // Access private method via type assertion for testing
      const similarity = (verifier as any).calculateSimilarity('test', 'test');
      expect(similarity).toBe(1.0);
    });

    test('should return 0.0 for completely different strings', () => {
      const similarity = (verifier as any).calculateSimilarity('abc', 'xyz');
      expect(similarity).toBe(0.0);
    });

    test('should return 1.0 for empty strings', () => {
      const similarity = (verifier as any).calculateSimilarity('', '');
      expect(similarity).toBe(1.0);
    });

    test('should return 0.0 when one string is empty', () => {
      const similarity1 = (verifier as any).calculateSimilarity('test', '');
      const similarity2 = (verifier as any).calculateSimilarity('', 'test');
      expect(similarity1).toBe(0.0);
      expect(similarity2).toBe(0.0);
    });

    test('should handle typos correctly', () => {
      const similarity = (verifier as any).calculateSimilarity('sektion', 'section');
      expect(similarity).toBeGreaterThan(0.7);
    });

    test('should handle Cyrillic text', () => {
      const similarity = (verifier as any).calculateSimilarity('–∑–∞–≥–æ–ª–æ–≤–æ–∫', '–∑–∞–≥–æ–ª–æ–≤–∫–∏');
      expect(similarity).toBeGreaterThan(0.7);
    });

    test('should handle partial matches', () => {
      const similarity = (verifier as any).calculateSimilarity('intro', 'introduction');
      expect(similarity).toBeGreaterThan(0.0);
      expect(similarity).toBeLessThan(0.7); // Should not exceed threshold
    });
  });

  describe('Closest Anchor Finding', () => {
    test('should return best match above threshold', () => {
      const anchors = new Set(['section-one', 'section-two', 'introduction']);
      const closest = (verifier as any).findClosestAnchor('sektion-one', anchors);
      expect(closest).toBe('section-one');
    });

    test('should return null when no match above threshold', () => {
      const anchors = new Set(['completely-different', 'unrelated-content']);
      const closest = (verifier as any).findClosestAnchor('my-section', anchors);
      expect(closest).toBeNull();
    });

    test('should return null for empty anchor set', () => {
      const anchors = new Set<string>();
      const closest = (verifier as any).findClosestAnchor('any-section', anchors);
      expect(closest).toBeNull();
    });

    test('should handle ties by returning first match', () => {
      const anchors = new Set(['section-a', 'section-b']); // Equal similarity to 'section-x'
      const closest = (verifier as any).findClosestAnchor('section-x', anchors);
      expect(closest).toBe('section-a'); // First in iteration order
    });

    test('should handle Unicode anchors correctly', () => {
      const anchors = new Set(['–∑–∞–Ω—è—Ç–∏–µ-1', '–∑–∞–Ω—è—Ç–∏–µ-2', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ']);
      const closest = (verifier as any).findClosestAnchor('–∑–∞–Ω—è—Ç–∏e-1', anchors); // Latin 'e' instead of Cyrillic '–µ'
      expect(closest).toBe('–∑–∞–Ω—è—Ç–∏–µ-1');
    });
  });

  describe('generateSlug - Anchor Specification Compliance', () => {
    test('preserves case: "Section Title" ‚Üí "Section-Title"', () => {
      const result = (verifier as any).generateSlug('Section Title');
      expect(result).toBe('Section-Title');
    });

    test('preserves special chars: "–ü—Ä–∏–º–µ—Ä ‚Ññ1" ‚Üí "–ü—Ä–∏–º–µ—Ä-‚Ññ1"', () => {
      const result = (verifier as any).generateSlug('–ü—Ä–∏–º–µ—Ä ‚Ññ1');
      expect(result).toBe('–ü—Ä–∏–º–µ—Ä-‚Ññ1');
    });

    test('only replaces spaces: "–ú–æ–π —è–∫–æ—Ä—å" ‚Üí "–ú–æ–π-—è–∫–æ—Ä—å"', () => {
      const result = (verifier as any).generateSlug('–ú–æ–π —è–∫–æ—Ä—å');
      expect(result).toBe('–ú–æ–π-—è–∫–æ—Ä—å');
    });

    test('handles Unicode correctly', () => {
      const result = (verifier as any).generateSlug('–¢–µ—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å —Å–∏–º–≤–æ–ª–∞–º–∏!@#');
      expect(result).toBe('–¢–µ—Å—Ç-–∑–∞–≥–æ–ª–æ–≤–∫–∞-—Å-—Å–∏–º–≤–æ–ª–∞–º–∏!@#');
    });

    test('backwards compatibility with common anchors', () => {
      // Test that simple English text still works
      const result = (verifier as any).generateSlug('simple test');
      expect(result).toBe('simple-test');
    });

    test('handles multiple spaces correctly', () => {
      const result = (verifier as any).generateSlug('Multiple   spaces   here');
      expect(result).toBe('Multiple---spaces---here');
    });

    test('trims leading and trailing spaces', () => {
      const result = (verifier as any).generateSlug('  Leading and trailing  ');
      expect(result).toBe('Leading-and-trailing');
    });

    test('handles empty string', () => {
      const result = (verifier as any).generateSlug('');
      expect(result).toBe('');
    });

    test('handles string with only spaces', () => {
      const result = (verifier as any).generateSlug('   ');
      expect(result).toBe('');
    });

    test('preserves hyphens and other punctuation', () => {
      const result = (verifier as any).generateSlug('Pre-existing hyphens & symbols!');
      expect(result).toBe('Pre-existing-hyphens-&-symbols!');
    });

    test('removes only < and > characters as per Telegra.ph rules', () => {
      const result = (verifier as any).generateSlug('Title with <tags> and >arrows<');
      expect(result).toBe('Title-with-tags-and-arrows');
    });

    test('preserves Markdown formatting as per research findings', () => {
      const result = (verifier as any).generateSlug('**Bold Title**');
      expect(result).toBe('**Bold-Title**');
    });

    test('preserves complex punctuation from research', () => {
      const result = (verifier as any).generateSlug('Title with @#$%^&*()-+=[]{}|;\'"');
      expect(result).toBe('Title-with-@#$%^&*()-+=[]{}|;\'\"');
    });
  });

  describe('anchor generation with Markdown formatting', () => {
    test('should preserve bold formatting in anchors according to Telegra.ph rules', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with bold heading
      writeFileSync(targetFile, '# **Bold Title**\n\nContent here');
      // Updated: anchor should include asterisks as per Telegra.ph behavior
      writeFileSync(sourceFile, '[Link to bold](./target.md#**Bold-Title**)');

      const link: MarkdownLink = {
        text: 'Link to bold',
        href: './target.md#**Bold-Title**',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 41
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should preserve italic formatting in anchors according to Telegra.ph rules', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with italic heading
      writeFileSync(targetFile, '## *Italic Title*\n\nContent here');
      // Updated: anchor should include asterisks as per Telegra.ph behavior
      writeFileSync(sourceFile, '[Link to italic](./target.md#*Italic-Title*)');

      const link: MarkdownLink = {
        text: 'Link to italic',
        href: './target.md#*Italic-Title*',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 39
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should preserve link formatting in anchors according to Telegra.ph rules', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with link in heading
      writeFileSync(targetFile, '### [Link Title](https://example.com)\n\nContent here');
      // Updated: anchor should include brackets and parentheses as per Telegra.ph behavior
      writeFileSync(sourceFile, '[Link to link heading](./target.md#[Link-Title](https://example.com))');

      const link: MarkdownLink = {
        text: 'Link to link heading',
        href: './target.md#[Link-Title](https://example.com)',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 71
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should preserve mixed formatting in anchors according to Telegra.ph rules', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with mixed formatting
      writeFileSync(targetFile, '#### **Bold** and *Italic* Text\n\nContent here');
      // Updated: anchor should preserve all Markdown formatting
      writeFileSync(sourceFile, '[Link to mixed](./target.md#**Bold**-and-*Italic*-Text)');

      const link: MarkdownLink = {
        text: 'Link to mixed',
        href: './target.md#**Bold**-and-*Italic*-Text',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 52
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should preserve complex nested formatting in anchors according to Telegra.ph rules', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with complex nested formatting
      writeFileSync(targetFile, '##### **Bold _nested_ text** with `code`\n\nContent here');
      // Updated: anchor should preserve all Markdown formatting including nested formatting
      writeFileSync(sourceFile, '[Link to complex](./target.md#**Bold-_nested_-text**-with-`code`)');

      const link: MarkdownLink = {
        text: 'Link to complex',
        href: './target.md#**Bold-_nested_-text**-with-`code`',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 62
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('should still detect broken links with incorrect anchor format', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with bold heading (generates anchor: **Bold-Title**)
      writeFileSync(targetFile, '# **Bold Title**\n\nContent here');
      // Link to wrong anchor (missing asterisks and has typo)
      writeFileSync(sourceFile, '[Link with typo](./target.md#Bold-Titel)');

      const link: MarkdownLink = {
        text: 'Link with typo',
        href: './target.md#Bold-Titel',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 38
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0].link.href).toBe('./target.md#Bold-Titel');
    });

    test('should work with cyrillic headings with formatting preserved', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Create target file with cyrillic bold heading
      writeFileSync(targetFile, '# **–¢–µ–º–∞ 1: –í–≤–µ–¥–µ–Ω–∏–µ: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ —Å–∏–¥–¥—Ö–∞–Ω—Ç–∞**\n\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ');
      // Updated: anchor should preserve asterisks as per Telegra.ph behavior
      writeFileSync(sourceFile, '[–°—Å—ã–ª–∫–∞](./target.md#**–¢–µ–º–∞-1:-–í–≤–µ–¥–µ–Ω–∏–µ:-–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–Ω–∞—Å—Ç–∞–≤–ª–µ–Ω–∏—è-–∏-—Å–∏–¥–¥—Ö–∞–Ω—Ç–∞**)');

      const link: MarkdownLink = {
        text: '–°—Å—ã–ª–∫–∞',
        href: './target.md#**–¢–µ–º–∞-1:-–í–≤–µ–¥–µ–Ω–∏–µ:-–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–Ω–∞—Å—Ç–∞–≤–ª–µ–Ω–∏—è-–∏-—Å–∏–¥–¥—Ö–∞–Ω—Ç–∞**',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 87
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });
  });
});
```

`src/links/LinkVerifier.ts`

```ts
import { existsSync, readFileSync } from 'node:fs';
import { dirname } from 'node:path';
import type { PathResolver } from '../utils/PathResolver';
import { cleanMarkdownString } from '../clean_mr';
import {
  type BrokenLink,
  type FileScanResult,
  type LinkVerificationError,
  LinkVerificationException,
  type MarkdownLink
} from './types';

/**
 * LinkVerifier is responsible for verifying the existence of links and identifying broken ones
 */
export class LinkVerifier {
  private pathResolver: PathResolver;
  // Cache for file anchors to avoid re-reading and re-parsing files
  private anchorCache: Map<string, Set<string>> = new Map();

  constructor(pathResolver: PathResolver) {
    this.pathResolver = pathResolver;
  }

  /**
   * Verify all local links in a file scan result
   * @param scanResult The file scan result to verify
   * @returns Updated scan result with broken links identified
   */
  async verifyLinks(scanResult: FileScanResult): Promise<FileScanResult> {
    const brokenLinks: BrokenLink[] = [];

    for (const link of scanResult.localLinks) {
      try {
        // Extract path and fragment parts from href
        const [pathPart, ...fragmentParts] = link.href.split('#');
        const fragment = fragmentParts.join('#');

        // Only process if there's a file path to check
        if (pathPart) {
          const resolvedPath = this.resolveLinkPath(pathPart, scanResult.filePath);

          // 1. Verify file existence (existing logic)
          if (!existsSync(resolvedPath)) {
            brokenLinks.push({
              filePath: scanResult.filePath,
              link,
              suggestions: [], // Will be populated by LinkResolver
              canAutoFix: false // Will be determined after suggestions are generated
            });
            continue; // Don't check anchor if file is missing
          }

          // 2. NEW: Verify anchor existence if a fragment is present
          if (fragment) {
            const targetAnchors = this.getAnchorsForFile(resolvedPath);
            // Try to decode URI component for non-latin characters, but handle decode errors gracefully
            let decodedFragment: string;
            try {
              decodedFragment = decodeURIComponent(fragment);
            } catch {
              // If decoding fails (invalid URI), use original fragment
              decodedFragment = fragment;
            }
            const requestedAnchor = decodedFragment === fragment ? fragment : this.generateSlug(decodedFragment);

            if (!targetAnchors.has(requestedAnchor)) {
              // NEW: Find closest match for intelligent suggestions
              const suggestion = this.findClosestAnchor(requestedAnchor, targetAnchors);
              const suggestions = suggestion ? [`${pathPart}#${suggestion}`] : [];

              brokenLinks.push({
                filePath: scanResult.filePath,
                link,
                suggestions, // Now populated with intelligent suggestions
                canAutoFix: false // Keep false for anchor fixes (safety)
              });
            }
          }
        }
        // NOTE: If pathPart is empty (pure fragment link),
        // we skip processing, maintaining existing behavior
      } catch (error) {
        // If we can't resolve the path, consider it broken
        brokenLinks.push({
          filePath: scanResult.filePath,
          link,
          suggestions: [],
          canAutoFix: false
        });
      }
    }

    return {
      ...scanResult,
      brokenLinks
    };
  }

  /**
   * Verify multiple file scan results
   * @param scanResults Array of file scan results
   * @returns Array of updated scan results with broken links identified
   */
  async verifyMultipleFiles(scanResults: FileScanResult[]): Promise<FileScanResult[]> {
    const results: FileScanResult[] = [];

    for (const scanResult of scanResults) {
      results.push(await this.verifyLinks(scanResult));
    }

    return results;
  }

  /**
   * Check if a specific link exists
   * @param linkHref The link URL/path to check
   * @param sourceFilePath Path of the file containing the link
   * @returns True if the link target exists
   */
  linkExists(linkHref: string, sourceFilePath: string): boolean {
    try {
      const resolvedPath = this.resolveLinkPath(linkHref, sourceFilePath);
      return existsSync(resolvedPath);
    } catch (error) {
      return false;
    }
  }

  /**
   * Resolve a link path relative to the source file
   * @param linkHref The link URL/path
   * @param sourceFilePath Path of the file containing the link
   * @returns Absolute path to the link target
   */
  resolveLinkPath(linkHref: string, sourceFilePath: string): string {
    // Handle different types of links
    if (this.isExternalLink(linkHref)) {
      throw new LinkVerificationException(
        'INVALID_PATH' as LinkVerificationError,
        `Cannot resolve external link: ${linkHref}`,
        sourceFilePath
      );
    }

    if (this.isFragmentLink(linkHref)) {
      throw new LinkVerificationException(
        'INVALID_PATH' as LinkVerificationError,
        `Cannot resolve fragment link: ${linkHref}`,
        sourceFilePath
      );
    }

    // Use PathResolver for all path resolutions
    return this.pathResolver.resolve(sourceFilePath, linkHref);
  }

  /**
   * Check if a link is external (HTTP, HTTPS, etc.)
   * @param linkHref Link to check
   * @returns True if link is external
   */
  private isExternalLink(linkHref: string): boolean {
    return linkHref.startsWith('http://') ||
      linkHref.startsWith('https://') ||
      linkHref.startsWith('mailto:') ||
      linkHref.startsWith('ftp://') ||
      linkHref.startsWith('//');
  }

  /**
   * Check if a link is a fragment (anchor) link
   * @param linkHref Link to check
   * @returns True if link is a fragment
   */
  private isFragmentLink(linkHref: string): boolean {
    return linkHref.startsWith('#');
  }

  /**
   * Get detailed verification information for a link
   * @param link The link to verify
   * @param sourceFilePath Path of the file containing the link
   * @returns Verification details
   */
  getVerificationDetails(link: MarkdownLink, sourceFilePath: string): {
    exists: boolean;
    resolvedPath?: string;
    error?: string;
  } {
    try {
      const resolvedPath = this.resolveLinkPath(link.href, sourceFilePath);
      const exists = existsSync(resolvedPath);

      return {
        exists,
        resolvedPath: exists ? resolvedPath : undefined
      };
    } catch (error) {
      return {
        exists: false,
        error: error instanceof Error ? error.message : String(error)
      };
    }
  }

  /**
   * Validate that a link path is safe (no directory traversal attacks)
   * @param linkHref Link to validate
   * @param sourceFilePath Source file path
   * @returns True if link is safe
   */
  isLinkSafe(linkHref: string, sourceFilePath: string): boolean {
    try {
      const resolvedPath = this.resolveLinkPath(linkHref, sourceFilePath);
      const sourceDir = dirname(sourceFilePath);
      const projectRoot = this.pathResolver.findProjectRoot(sourceFilePath);

      // Check if resolved path is within project boundaries
      return resolvedPath.startsWith(projectRoot) || resolvedPath.startsWith(sourceDir);
    } catch (error) {
      return false;
    }
  }

  /**
   * Get statistics about link verification
   * @param scanResults Array of verified scan results
   * @returns Verification statistics
   */
  getVerificationStats(scanResults: FileScanResult[]): {
    totalFiles: number;
    totalLinks: number;
    totalLocalLinks: number;
    totalBrokenLinks: number;
    brokenLinkPercentage: number;
    filesByBrokenLinks: number;
  } {
    const totalFiles = scanResults.length;
    const totalLinks = scanResults.reduce((sum, result) => sum + result.allLinks.length, 0);
    const totalLocalLinks = scanResults.reduce((sum, result) => sum + result.localLinks.length, 0);
    const totalBrokenLinks = scanResults.reduce((sum, result) => sum + result.brokenLinks.length, 0);
    const filesByBrokenLinks = scanResults.filter(result => result.brokenLinks.length > 0).length;

    return {
      totalFiles,
      totalLinks,
      totalLocalLinks,
      totalBrokenLinks,
      brokenLinkPercentage: totalLocalLinks > 0 ? (totalBrokenLinks / totalLocalLinks) * 100 : 0,
      filesByBrokenLinks
    };
  }

  /**
   * Generates a URL-friendly anchor from a heading text according to Telegra.ph rules.
   * Based on empirical research: remove only < and > characters, replace spaces with hyphens.
   * Preserve all other characters including Markdown formatting, case, punctuation, and Unicode.
   * @param text The heading text (including any Markdown formatting).
   * @returns An anchor string compliant with Telegra.ph behavior.
   */
  private generateSlug(text: string): string {
    if (!text) return '';
    return text
      .trim()
      .replace(/[<>]/g, '') // 1. Remove < and > characters only
      .replace(/ /g, '-');  // 2. Replace spaces with hyphens
  }

  /**
   * Extracts all valid anchors (from headings) from a Markdown file.
   * Results are cached to improve performance.
   * @param filePath The absolute path to the Markdown file.
   * @returns A Set containing all valid anchor slugs for the file.
   */
  private getAnchorsForFile(filePath: string): Set<string> {
    if (this.anchorCache.has(filePath)) {
      return this.anchorCache.get(filePath)!;
    }

    try {
      const content = readFileSync(filePath, 'utf-8');
      const headingRegex = /^(#{1,6})\s+(.*)/gm;
      const anchors = new Set<string>();

      let match;
      while ((match = headingRegex.exec(content)) !== null) {
        const headingText = match[2]?.trim();
        if (headingText) {
          // Use raw heading text directly (including Markdown formatting)
          // to match Telegra.ph's actual anchor generation behavior
          anchors.add(this.generateSlug(headingText));
        }
      }

      this.anchorCache.set(filePath, anchors);
      return anchors;
    } catch (error) {
      // If the file can't be read, return an empty set.
      // The file existence check will handle the "broken link" error.
      return new Set<string>();
    }
  }

  /**
   * Calculates a simple string similarity score optimized for anchor text.
   * Uses character intersection approach for performance and simplicity.
   * @param s1 First string (typically the requested anchor)
   * @param s2 Second string (typically an available anchor)
   * @returns Similarity score between 0.0 and 1.0
   */
  private calculateSimilarity(s1: string, s2: string): number {
    // Handle edge cases first
    if (s1 === s2) return 1.0;
    if (s1.length === 0 && s2.length === 0) return 1.0;
    if (s1.length === 0 || s2.length === 0) return 0.0;

    const longer = s1.length > s2.length ? s1 : s2;
    const shorter = s1.length > s2.length ? s2 : s1;

    // Count matching characters (order independent for typo tolerance)
    const matchingChars = [...shorter].filter(char => longer.includes(char)).length;
    return matchingChars / longer.length;
  }

  /**
   * Finds the closest matching anchor from a set of available anchors.
   * @param requestedAnchor The anchor that was not found
   * @param availableAnchors A Set of valid anchors in the target file
   * @returns The best suggestion, or null if no suitable match is found
   */
  private findClosestAnchor(requestedAnchor: string, availableAnchors: Set<string>): string | null {
    let bestMatch: string | null = null;
    let highestScore = 0.7; // Minimum similarity threshold

    for (const available of availableAnchors) {
      const score = this.calculateSimilarity(requestedAnchor, available);
      if (score > highestScore) {
        highestScore = score;
        bestMatch = available;
      }
    }
    return bestMatch;
  }
}
```

`src/links/ReportGenerator.ts`

```ts
import { writeFileSync } from 'node:fs';
import { basename, relative } from 'node:path';
import type {
  BrokenLink,
  FileScanResult,
  LinkStatistics,
  ScanResult
} from './types';

/**
 * ReportGenerator handles formatting and displaying link verification results
 */
export class ReportGenerator {
  private verbose: boolean;
  private reportLines: string[] = [];
  private captureReport: boolean = false;

  constructor(verbose: boolean = false) {
    this.verbose = verbose;
  }

  /**
 * Generate and display the main scan report
 * @param scanResult Complete scan results
 * @param outputFile Optional file path to save the report
 */
  generateReport(scanResult: ScanResult, outputFile?: string): void {
    let originalConsoleLog: typeof console.log | null = null;

    // Start capturing report if output file is specified
    if (outputFile) {
      this.startCapture();
      originalConsoleLog = console.log;
      console.log = (...args: any[]) => {
        originalConsoleLog?.(...args);
        this.reportLines.push(args.join(' '));
      };
    }

    try {
      this.showScanProgress(scanResult);
      this.showStatistics(scanResult);

      if (scanResult.brokenLinks.length > 0) {
        this.showBrokenLinksReport(scanResult);
      } else {
        this.showSuccessMessage();
      }

      this.showFinalSummary(scanResult);
    } finally {
      // Restore original console.log and save report if needed
      if (outputFile && originalConsoleLog) {
        console.log = originalConsoleLog;
        this.saveReport(outputFile);
      }
    }
  }

  /**
 * Show initial scan progress and completion
 * @param scanResult Scan results
 */
  private showScanProgress(scanResult: ScanResult): void {
    const stats = this.calculateStatistics(scanResult);

    console.log(`üîé Link scanning completed...`);
    console.log(`üìÅ Found ${stats.totalFiles} markdown files`);
    console.log(`üîç Analyzed ${stats.totalLinks} links in ${(scanResult.processingTime / 1000).toFixed(1)}s`);
    console.log();
  }

  /**
 * Display detailed statistics
 * @param scanResult Scan results
 */
  private showStatistics(scanResult: ScanResult): void {
    const stats = this.calculateStatistics(scanResult);

    console.log('üìä LINK ANALYSIS RESULTS');
    console.log('‚ïê'.repeat(63));
    console.log();
    console.log('üìà Statistics:');
    console.log(`   ‚Ä¢ Total files: ${stats.totalFiles}`);
    console.log(`   ‚Ä¢ Total links: ${stats.totalLinks}`);
    console.log(`   ‚Ä¢ Local links: ${stats.localLinks}`);
    console.log(`   ‚Ä¢ Broken links: ${stats.brokenLinks}`);
    console.log(`   ‚Ä¢ Scan time: ${stats.scanTime}s`);
    console.log();
  }

  /**
   * Show detailed broken links report
   * @param scanResult Scan results
   */
  private showBrokenLinksReport(scanResult: ScanResult): void {
    console.log('‚ùå ISSUES FOUND');
    console.log();

    // Group broken links by file
    const linksByFile = this.groupLinksByFile(scanResult.brokenLinks);

    for (const [filePath, brokenLinks] of linksByFile.entries()) {
      this.showFileProblems(filePath, brokenLinks);
    }
  }

  /**
   * Show problems for a specific file
   * @param filePath Path to the file
   * @param brokenLinks Broken links in the file
   */
  private showFileProblems(filePath: string, brokenLinks: BrokenLink[]): void {
    const displayPath = this.formatFilePath(filePath);

    console.log(`üìÑ ${displayPath}`);
    console.log('‚îÄ'.repeat(62));

    for (const brokenLink of brokenLinks) {
      this.showBrokenLink(brokenLink);
    }

    console.log();
  }

  /**
 * Show details for a single broken link
 * @param brokenLink The broken link to display
 */
  private showBrokenLink(brokenLink: BrokenLink): void {
    const link = brokenLink.link;

    console.log(`  üîó Broken link: [${link.text}](${link.href})`);
    console.log(`     üìç Line: ${link.lineNumber}`);

    if (brokenLink.suggestions.length > 0) {
      if (brokenLink.suggestions.length === 1) {
        console.log(`     üí° Suggested fix: ${brokenLink.suggestions[0]}`);
      } else {
        console.log(`     üí° Multiple possible fixes found:`);
        brokenLink.suggestions.forEach((suggestion, index) => {
          console.log(`        ${index + 1}) ${suggestion}`);
        });
      }
    } else {
      const filename = this.extractFilename(link.href);
      console.log(`     ‚ùå File '${filename}' not found in project`);
    }

    console.log();
  }

  /**
   * Show success message when no problems found
   */
  private showSuccessMessage(): void {
    console.log('‚úÖ GREAT NEWS!');
    console.log('‚ïê'.repeat(63));
    console.log();
    console.log('üéâ All local links are working correctly!');
    console.log('üìã No issues found.');
    console.log();
  }

  /**
 * Show final summary and next steps
 * @param scanResult Scan results
 */
  private showFinalSummary(scanResult: ScanResult): void {
    const stats = this.calculateStatistics(scanResult);

    console.log('üìä FINAL REPORT');
    console.log('‚ïê'.repeat(63));
    console.log();

    if (stats.brokenLinks > 0) {
      const brokenFilesCount = new Set(scanResult.brokenLinks.map(link => link.filePath)).size;
      const withSuggestions = scanResult.brokenLinks.filter(link => link.suggestions.length > 0).length;
      const fixablePercentage = stats.brokenLinks > 0 ? Math.round((withSuggestions / stats.brokenLinks) * 100) : 0;

      console.log(`‚úÖ Result: Found ${stats.brokenLinks} broken links in ${brokenFilesCount} files`);
      console.log(`üí° Available fixes: ${withSuggestions} (for ${fixablePercentage}% of issues)`);

      if (withSuggestions < stats.brokenLinks) {
        console.log(`‚ö†Ô∏è  Without suggestions: ${stats.brokenLinks - withSuggestions} links`);
      }

      console.log();
      console.log('üõ†Ô∏è  To apply fixes, run:');
      console.log('    telegraph-publisher check-links --apply-fixes');
    } else {
      console.log('‚úÖ Result: All links are working correctly!');
      console.log('üéØ No action required.');
    }

    console.log();
  }

  /**
 * Show verbose scanning details
 * @param filePath Current file being processed
 * @param current Current file number
 * @param total Total files to process
 */
  showVerboseProgress(filePath: string, current: number, total: number): void {
    if (!this.verbose) return;

    const displayPath = this.formatFilePath(filePath);
    console.log(`üîç [${current}/${total}] Analyzing: ${displayPath}`);
  }

  /**
 * Show verbose link details for a file
 * @param scanResult File scan result
 */
  showVerboseFileDetails(scanResult: FileScanResult): void {
    if (!this.verbose) return;

    const displayPath = this.formatFilePath(scanResult.filePath);
    const localCount = scanResult.localLinks.length;
    const totalCount = scanResult.allLinks.length;
    const brokenCount = scanResult.brokenLinks.length;

    if (brokenCount > 0) {
      console.log(`   ‚ùå ${displayPath} - found ${totalCount} links (${localCount} local, ${brokenCount} broken)`);
    } else if (localCount > 0) {
      console.log(`   ‚úÖ ${displayPath} - found ${totalCount} links (${localCount} local)`);
    } else {
      console.log(`   üìÑ ${displayPath} - no local links found`);
    }
  }

  /**
   * Show error message
   * @param message Error message
   * @param filePath Optional file path where error occurred
   */
  showError(message: string, filePath?: string): void {
    if (filePath) {
      const displayPath = this.formatFilePath(filePath);
      console.log(`‚ùå ERROR in ${displayPath}: ${message}`);
    } else {
      console.log(`‚ùå ERROR: ${message}`);
    }
  }

  /**
   * Show warning message
   * @param message Warning message
   * @param filePath Optional file path where warning occurred
   */
  showWarning(message: string, filePath?: string): void {
    if (filePath) {
      const displayPath = this.formatFilePath(filePath);
      console.log(`‚ö†Ô∏è  WARNING in ${displayPath}: ${message}`);
    } else {
      console.log(`‚ö†Ô∏è  WARNING: ${message}`);
    }
  }

  /**
   * Show info message
   * @param message Info message
   */
  showInfo(message: string): void {
    console.log(`‚ÑπÔ∏è  ${message}`);
  }

  /**
   * Format file path for display (relative and shortened if needed)
   * @param filePath Absolute file path
   * @returns Formatted display path
   */
  private formatFilePath(filePath: string): string {
    const cwd = process.cwd();
    let displayPath = filePath;

    try {
      // Try to make it relative to current working directory
      const relativePath = relative(cwd, filePath);
      if (relativePath.length < filePath.length && !relativePath.startsWith('..')) {
        displayPath = relativePath;
      }
    } catch (error) {
      // If relative path calculation fails, use basename
      displayPath = basename(filePath);
    }

    // Truncate very long paths
    const maxLength = 80;
    if (displayPath.length > maxLength) {
      const parts = displayPath.split('/');
      if (parts.length > 2) {
        displayPath = `.../${parts.slice(-2).join('/')}`;
      } else {
        displayPath = `...${displayPath.slice(-maxLength + 3)}`;
      }
    }

    return displayPath;
  }

  /**
   * Group broken links by file path
   * @param brokenLinks Array of broken links
   * @returns Map of file paths to broken links
   */
  private groupLinksByFile(brokenLinks: BrokenLink[]): Map<string, BrokenLink[]> {
    const groups = new Map<string, BrokenLink[]>();

    for (const brokenLink of brokenLinks) {
      const filePath = brokenLink.filePath;

      if (!groups.has(filePath)) {
        groups.set(filePath, []);
      }

      groups.get(filePath)?.push(brokenLink);
    }

    return groups;
  }

  /**
   * Calculate statistics from scan results
   * @param scanResult Scan results
   * @returns Calculated statistics
   */
  private calculateStatistics(scanResult: ScanResult): LinkStatistics {
    const totalFiles = scanResult.totalFiles;
    const totalLinks = scanResult.totalLinks;
    const localLinks = scanResult.totalLocalLinks;
    const brokenLinks = scanResult.brokenLinks.length;
    const linksWithSuggestions = scanResult.brokenLinks.filter(link => link.suggestions.length > 0).length;

    return {
      totalFiles,
      totalLinks,
      localLinks,
      brokenLinks,
      linksWithSuggestions,
      brokenLinkPercentage: localLinks > 0 ? (brokenLinks / localLinks) * 100 : 0,
      fixablePercentage: brokenLinks > 0 ? (linksWithSuggestions / brokenLinks) * 100 : 0,
      scanTime: scanResult.processingTime / 1000
    };
  }

  /**
   * Extract filename from a path
   * @param path File path
   * @returns Filename
   */
  private extractFilename(path: string): string {
    // Remove query parameters and fragments
    const cleanPath = path.split('?')[0]?.split('#')[0] || path;
    return basename(cleanPath);
  }

  /**
   * Enable or disable verbose output
   * @param verbose Verbose flag
   */
  setVerbose(verbose: boolean): void {
    this.verbose = verbose;
  }

  /**
   * Start capturing console output for report saving
   */
  private startCapture(): void {
    this.captureReport = true;
    this.reportLines = [];
  }

  /**
   * Save captured report to file
   * @param outputFile Path to save the report
   */
  private saveReport(outputFile: string): void {
    try {
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      const reportContent = [
        `# Link Verification Report`,
        `Generated: ${new Date().toLocaleString()}`,
        ``,
        ...this.reportLines,
        ``,
        `---`,
        `Report generated by telegraph-publisher check-links`
      ].join('\n');

      writeFileSync(outputFile, reportContent, 'utf-8');
      console.log(`\nüìÑ Report saved to: ${outputFile}`);
    } catch (error) {
      console.error(`‚ùå Failed to save report: ${error instanceof Error ? error.message : String(error)}`);
    } finally {
      this.captureReport = false;
      this.reportLines = [];
    }
  }

  /**
   * Enhanced console.log that captures output when needed
   * @param message Message to log
   */
  private log(message: string): void {
    console.log(message);
    if (this.captureReport) {
      this.reportLines.push(message);
    }
  }
}
```

`src/links/ResearchValidation.test.ts`

```ts
import { join } from 'node:path';
import { existsSync, mkdirSync, rmSync, writeFileSync } from 'node:fs';
import { LinkVerifier } from './LinkVerifier';
import { PathResolver } from '../utils/PathResolver';
import type { FileScanResult, MarkdownLink } from './types';

describe('Research Validation - Telegra.ph Anchor Generation Rules', () => {
  let verifier: LinkVerifier;
  let pathResolver: PathResolver;
  let testDir: string;

  beforeEach(() => {
    pathResolver = new PathResolver(__dirname);
    verifier = new LinkVerifier(pathResolver);
    testDir = join(__dirname, 'test-research-validation');
    
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true });
    }
    mkdirSync(testDir, { recursive: true });
  });

  afterEach(() => {
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true });
    }
  });

  describe('Basic Rules Validation', () => {
    test('Rule 1: Spaces replaced with hyphens', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# Simple Title\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#Simple-Title)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#Simple-Title',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 30
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Rule 1 (Cyrillic): Spaces replaced with hyphens for Cyrillic text', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏\n\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ');
      writeFileSync(sourceFile, '[–°—Å—ã–ª–∫–∞](./target.md#–ó–∞–≥–æ–ª–æ–≤–æ–∫-—Å-–ø—Ä–æ–±–µ–ª–∞–º–∏)');

      const link: MarkdownLink = {
        text: '–°—Å—ã–ª–∫–∞',
        href: './target.md#–ó–∞–≥–æ–ª–æ–≤–æ–∫-—Å-–ø—Ä–æ–±–µ–ª–∞–º–∏',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 44
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Rule 2: Numbered headings preserve all symbols including dots', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# 1. Numbered Heading\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#1.-Numbered-Heading)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#1.-Numbered-Heading',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 37
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Rule 2: Punctuation marks are preserved', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# Title with comma,\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#Title-with-comma,)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#Title-with-comma,',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 35
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Rule 3: Only < and > characters are removed', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# Title with <tags> and >arrows<\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#Title-with-tags-and-arrows)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#Title-with-tags-and-arrows',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 44
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });
  });

  describe('Markdown Formatting Preservation', () => {
    test('Rule 4: Bold formatting symbols are preserved', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# **Bold Title**\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#**Bold-Title**)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#**Bold-Title**',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 33
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Rule 4: Italic formatting symbols are preserved', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# *Italic Title*\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#*Italic-Title*)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#*Italic-Title*',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 31
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Rule 4: Link formatting symbols are preserved', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# [Link Title](url)\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#[Link-Title](url))');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#[Link-Title](url)',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 35
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });
  });

  describe('Complex Cases from Research', () => {
    test('Cyrillic with complex formatting and punctuation', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# –ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)\n\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ');
      writeFileSync(sourceFile, '[–°—Å—ã–ª–∫–∞](./target.md#–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4))');

      const link: MarkdownLink = {
        text: '–°—Å—ã–ª–∫–∞',
        href: './target.md#–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 82
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Special symbols preservation', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# Title with @#$%^&*()+-=[]{}|;\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#Title-with-@#$%^&*()+-=[]{}|;)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#Title-with-@#$%^&*()+-=[]{}|;',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 46
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Mixed formatting preservation', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# **Bold** and *Italic* with [Link](url)\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#**Bold**-and-*Italic*-with-[Link](url))');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#**Bold**-and-*Italic*-with-[Link](url)',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 56
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });
  });

  describe('Edge Cases', () => {
    test('Empty string handling', () => {
      const result = (verifier as any).generateSlug('');
      expect(result).toBe('');
    });

    test('Only spaces string handling', () => {
      const result = (verifier as any).generateSlug('   ');
      expect(result).toBe('');
    });

    test('String with only < and > characters', () => {
      const result = (verifier as any).generateSlug('<>');
      expect(result).toBe('');
    });

    test('String with spaces and < > characters', () => {
      const result = (verifier as any).generateSlug('Title <with> spaces');
      expect(result).toBe('Title-with-spaces');
    });

    test('Multiple consecutive spaces', () => {
      const result = (verifier as any).generateSlug('Title   with    multiple     spaces');
      expect(result).toBe('Title---with----multiple-----spaces');
    });
  });

  describe('Regression Prevention', () => {
    test('Should reject old anchor format (cleaned markdown)', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      // Target has bold formatting
      writeFileSync(targetFile, '# **Bold Title**\n\nContent here');
      // Source uses old format (without asterisks) - should be broken
      writeFileSync(sourceFile, '[Link](./target.md#Bold-Title)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#Bold-Title',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 29
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(1);
      expect(result.brokenLinks[0].link.href).toBe('./target.md#Bold-Title');
    });

    test('Case sensitivity should be preserved', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# Mixed CaSe Title\n\nContent here');
      writeFileSync(sourceFile, '[Link](./target.md#Mixed-CaSe-Title)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#Mixed-CaSe-Title',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 34
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(0);
    });

    test('Wrong case should be broken', async () => {
      const targetFile = join(testDir, 'target.md');
      const sourceFile = join(testDir, 'source.md');

      writeFileSync(targetFile, '# Mixed CaSe Title\n\nContent here');
      // Wrong case in link
      writeFileSync(sourceFile, '[Link](./target.md#mixed-case-title)');

      const link: MarkdownLink = {
        text: 'Link',
        href: './target.md#mixed-case-title',
        lineNumber: 1,
        columnStart: 0,
        columnEnd: 34
      };

      const scanResult: FileScanResult = {
        filePath: sourceFile,
        allLinks: [link],
        localLinks: [link],
        brokenLinks: [],
        headings: []
      };

      const result = await verifier.verifyLinks(scanResult);
      expect(result.brokenLinks).toHaveLength(1);
    });
  });
});
```

`src/links/types.ts`

```ts
/**
 * Core types and interfaces for the link verification system
 */

/**
 * Represents a Markdown link found in a file
 */
export interface MarkdownLink {
  /** The display text of the link */
  text: string;
  /** The original path/URL in the link */
  href: string;
  /** Line number where the link was found (1-based) */
  lineNumber: number;
  /** Character position in the line where link starts */
  columnStart: number;
  /** Character position in the line where link ends */
  columnEnd: number;
}

/**
 * Represents a broken link with potential fixes
 */
export interface BrokenLink {
  /** Path to the file containing the broken link */
  filePath: string;
  /** The original link information */
  link: MarkdownLink;
  /** Array of suggested fix paths (relative to the source file) */
  suggestions: string[];
  /** Whether this link can be automatically fixed */
  canAutoFix: boolean;
}

/**
 * Result of scanning a single file for links
 */
export interface FileScanResult {
  /** Path to the scanned file */
  filePath: string;
  /** All Markdown links found in the file */
  allLinks: MarkdownLink[];
  /** Only local links (non-HTTP/HTTPS/mailto) */
  localLinks: MarkdownLink[];
  /** Links that are broken (target file doesn't exist) */
  brokenLinks: BrokenLink[];
  /** Processing time for this file in milliseconds */
  processingTime: number;
}

/**
 * Complete scan result for all processed files
 */
export interface ScanResult {
  /** Total number of files scanned */
  totalFiles: number;
  /** Total number of links found across all files */
  totalLinks: number;
  /** Total number of local links found */
  totalLocalLinks: number;
  /** All broken links found across all files */
  brokenLinks: BrokenLink[];
  /** Individual file scan results */
  fileResults: FileScanResult[];
  /** Total processing time in milliseconds */
  processingTime: number;
}

/**
 * Options for the link checking operation
 */
export interface CheckLinksOptions {
  /** Path to scan (file or directory) */
  targetPath?: string;
  /** Enable interactive repair mode */
  applyFixes?: boolean;
  /** Show detailed progress information */
  verbose?: boolean;
  /** Perform dry run (report only, no changes) */
  dryRun?: boolean;
  /** Output file path for saving the report */
  outputFile?: string;
}

/**
 * User action in interactive repair mode
 */
export type UserAction = 'yes' | 'no' | 'all' | 'quit';

/**
 * Result of applying a fix to a broken link
 */
export interface FixResult {
  /** Whether the fix was successfully applied */
  success: boolean;
  /** The broken link that was processed */
  brokenLink: BrokenLink;
  /** The fix that was applied (if any) */
  appliedFix?: string;
  /** Error message if fix failed */
  error?: string;
}

/**
 * Summary of interactive repair session
 */
export interface RepairSummary {
  /** Total number of broken links processed */
  totalProcessed: number;
  /** Number of fixes successfully applied */
  fixesApplied: number;
  /** Number of links skipped by user */
  linksSkipped: number;
  /** Number of files modified */
  filesModified: number;
  /** Any errors encountered during repair */
  errors: string[];
}

/**
 * Configuration for link scanning behavior
 */
export interface ScanConfig {
  /** File extensions to scan (default: ['.md', '.markdown']) */
  extensions?: string[];
  /** Directories to ignore during scanning */
  ignoreDirs?: string[];
  /** Maximum depth for recursive scanning (-1 for unlimited) */
  maxDepth?: number;
  /** Whether to follow symbolic links */
  followSymlinks?: boolean;
}

/**
 * Progress callback for long-running operations
 */
export type ProgressCallback = (current: number, total: number, message?: string) => void;

/**
 * Error types that can occur during link verification
 */
export enum LinkVerificationError {
  FILE_NOT_FOUND = 'FILE_NOT_FOUND',
  PERMISSION_DENIED = 'PERMISSION_DENIED',
  INVALID_PATH = 'INVALID_PATH',
  PARSE_ERROR = 'PARSE_ERROR',
  WRITE_ERROR = 'WRITE_ERROR'
}

/**
 * Custom error class for link verification operations
 */
export class LinkVerificationException extends Error {
  constructor(
    public errorType: LinkVerificationError,
    message: string,
    public filePath?: string
  ) {
    super(message);
    this.name = 'LinkVerificationException';
  }
}

/**
 * Statistics for reporting
 */
export interface LinkStatistics {
  /** Total files scanned */
  totalFiles: number;
  /** Total links found */
  totalLinks: number;
  /** Local links found */
  localLinks: number;
  /** Broken links found */
  brokenLinks: number;
  /** Links with available suggestions */
  linksWithSuggestions: number;
  /** Percentage of broken links */
  brokenLinkPercentage: number;
  /** Percentage of fixable links */
  fixablePercentage: number;
  /** Total scan time */
  scanTime: number;
}
```

`src/metadata/MetadataManager.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, it } from "bun:test";
import { join } from "node:path";
import { TestHelpers } from "../test-utils/TestHelpers";
import type { FileMetadata } from "../types/metadata";
import { PublicationStatus } from "../types/metadata";
import { MetadataManager } from "./MetadataManager";

describe("MetadataManager", () => {
  let tempDir: string;
  let testFile: string;

  beforeEach(() => {
    tempDir = TestHelpers.createTempDir("metadata-test");
    testFile = join(tempDir, "test.md");
  });

  afterEach(() => {
    TestHelpers.cleanup();
  });

  describe("parseMetadata", () => {
    it("should parse valid YAML front-matter", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      const contentWithMetadata = TestHelpers.createYamlFrontMatter(metadata) + "\n\n" + content;

      const result = MetadataManager.parseMetadata(contentWithMetadata);

      expect(result).toEqual(metadata);
    });

    it("should return null for content without metadata", () => {
      const content = "# Test Article\n\nNo metadata here.";

      const result = MetadataManager.parseMetadata(content);

      expect(result).toBeNull();
    });

    it("should handle malformed YAML gracefully", () => {
      const malformedYaml = `---
telegraphUrl: "https://telegra.ph/Test-01-01"
editPath: "Test-01-01"
username: "Test Author"
invalidField: some invalid syntax ][
---

# Test Article`;

      const result = MetadataManager.parseMetadata(malformedYaml);

      // Parser extracts valid fields and ignores malformed ones
      expect(result).not.toBeNull();
      expect(result?.telegraphUrl).toBe("https://telegra.ph/Test-01-01");
      expect(result?.editPath).toBe("Test-01-01");
      expect(result?.username).toBe("Test Author");
    });
  });

  describe("injectMetadata", () => {
    it("should inject metadata into content without existing front-matter", () => {
      const content = "# Test Article\n\nThis is test content.";
      const metadata = TestHelpers.createSampleMetadata();

      const result = MetadataManager.injectMetadata(content, metadata);

      expect(result).toContain("---");
      expect(result).toContain(`telegraphUrl: "${metadata.telegraphUrl}"`);
      expect(result).toContain(`username: "${metadata.username}"`);
      expect(result).toContain("# Test Article");
    });

    it("should update existing metadata", () => {
      const originalMetadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      const contentWithMetadata = TestHelpers.createYamlFrontMatter(originalMetadata) + "\n\n" + content;

      const updatedMetadata = TestHelpers.createSampleMetadata({
        title: "Updated Title",
        username: "Updated Author"
      });

      const result = MetadataManager.injectMetadata(contentWithMetadata, updatedMetadata);

      expect(result).toContain(`title: "${updatedMetadata.title}"`);
      expect(result).toContain(`username: "${updatedMetadata.username}"`);
    });
  });

  describe("updateMetadata", () => {
    it("should update metadata in content", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      const contentWithMetadata = TestHelpers.createYamlFrontMatter(metadata) + "\n\n" + content;

      const updatedMetadata = TestHelpers.createSampleMetadata({
        title: "New Title",
        description: "New Description"
      });

      const result = MetadataManager.updateMetadata(contentWithMetadata, updatedMetadata);

      expect(result).toContain(`title: "${updatedMetadata.title}"`);
      expect(result).toContain(`description: "${updatedMetadata.description}"`);
    });
  });

  describe("removeMetadata", () => {
    it("should remove metadata from content", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      const contentWithMetadata = TestHelpers.createYamlFrontMatter(metadata) + "\n\n" + content;

      const result = MetadataManager.removeMetadata(contentWithMetadata);

      expect(result).not.toContain("---");
      expect(result).not.toContain("telegraphUrl:");
      expect(result).toContain("# Test Article");
      expect(result.trim()).toBe(content);
    });

    it("should handle content without metadata", () => {
      const content = "# Test Article\n\nNo metadata here.";

      const result = MetadataManager.removeMetadata(content);

      expect(result).toBe(content);
    });
  });

  describe("validateMetadata", () => {
    it("should validate correct metadata", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const result = MetadataManager.validateMetadata(metadata);

      expect(result).toBe(true);
    });

    it("should detect missing required fields", () => {
      const invalidMetadata = {
        telegraphUrl: "https://telegra.ph/Test-01-01",
        // Missing editPath, username, publishedAt, originalFilename
      } as FileMetadata;

      const result = MetadataManager.validateMetadata(invalidMetadata);

      expect(result).toBe(false);
    });

    it("should detect invalid URL format", () => {
      const metadata = TestHelpers.createSampleMetadata({
        telegraphUrl: "not-a-valid-url"
      });

      const result = MetadataManager.validateMetadata(metadata);

      expect(result).toBe(false);
    });

    it("should detect invalid date format", () => {
      const metadata = TestHelpers.createSampleMetadata({
        publishedAt: "not-a-valid-date"
      });

      const result = MetadataManager.validateMetadata(metadata);

      expect(result).toBe(false);
    });

    it("should return false for null metadata", () => {
      const result = MetadataManager.validateMetadata(null);

      expect(result).toBe(false);
    });
  });

  describe("getPublicationStatus", () => {
    it("should return PUBLISHED for file with valid metadata", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      TestHelpers.createTestFile(testFile, content, metadata);

      const status = MetadataManager.getPublicationStatus(testFile);

      expect(status).toBe(PublicationStatus.PUBLISHED);
    });

    it("should return NOT_PUBLISHED for file without metadata", () => {
      TestHelpers.createTestFile(testFile, "# Test Article\n\nNo metadata here.");

      const status = MetadataManager.getPublicationStatus(testFile);

      expect(status).toBe(PublicationStatus.NOT_PUBLISHED);
    });

    it("should return METADATA_CORRUPTED for file with invalid metadata", () => {
      const invalidMetadata = TestHelpers.createSampleMetadata({
        telegraphUrl: "not-a-valid-url",
        publishedAt: "invalid-date"
      });
      const content = "# Test Article\n\nThis is test content.";
      TestHelpers.createTestFile(testFile, content, invalidMetadata);

      const status = MetadataManager.getPublicationStatus(testFile);

      expect(status).toBe(PublicationStatus.METADATA_CORRUPTED);
    });

    it("should return METADATA_MISSING for non-existent file", () => {
      const status = MetadataManager.getPublicationStatus("/non/existent/file.md");

      expect(status).toBe(PublicationStatus.METADATA_MISSING);
    });
  });

  describe("getPublicationInfo", () => {
    it("should return metadata for published file", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      TestHelpers.createTestFile(testFile, content, metadata);

      const info = MetadataManager.getPublicationInfo(testFile);

      expect(info).toEqual(metadata);
    });

    it("should return null for unpublished file", () => {
      TestHelpers.createTestFile(testFile, "# Test Article\n\nNo metadata here.");

      const info = MetadataManager.getPublicationInfo(testFile);

      expect(info).toBeNull();
    });
  });

  describe("isPublished", () => {
    it("should return true for published file", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const content = "# Test Article\n\nThis is test content.";
      TestHelpers.createTestFile(testFile, content, metadata);

      const result = MetadataManager.isPublished(testFile);

      expect(result).toBe(true);
    });

    it("should return false for unpublished file", () => {
      TestHelpers.createTestFile(testFile, "# Test Article\n\nNo metadata here.");

      const result = MetadataManager.isPublished(testFile);

      expect(result).toBe(false);
    });
  });

  describe("createMetadata", () => {
    it("should create metadata object", () => {
      const url = "https://telegra.ph/Test-Article-01-01";
      const path = "Test-Article-01-01";
      const username = "Test Author";
      const filePath = "/path/to/test.md";
      const title = "Test Article";
      const description = "Test description";

      const metadata = MetadataManager.createMetadata(url, path, username, filePath, "abc123hash", title, description);

      expect(metadata.telegraphUrl).toBe(url);
      expect(metadata.editPath).toBe(path);
      expect(metadata.username).toBe(username);
      expect(metadata.originalFilename).toBe("test.md");
      expect(metadata.title).toBe(title);
      expect(metadata.description).toBe(description);
      expect(metadata.contentHash).toBe("abc123hash");
      expect(metadata.publishedAt).toBeDefined();
      expect(new Date(metadata.publishedAt)).toBeInstanceOf(Date);
    });

    it("should create metadata object without optional fields", () => {
      const url = "https://telegra.ph/Test-Article-01-01";
      const path = "Test-Article-01-01";
      const username = "Test Author";
      const filePath = "/path/to/test.md";

      const metadata = MetadataManager.createMetadata(url, path, username, filePath, "def456hash");

      expect(metadata.telegraphUrl).toBe(url);
      expect(metadata.editPath).toBe(path);
      expect(metadata.username).toBe(username);
      expect(metadata.originalFilename).toBe("test.md");
      expect(metadata.title).toBeUndefined();
      expect(metadata.description).toBeUndefined();
      expect(metadata.contentHash).toBe("def456hash");
    });
  });

  describe("edge cases", () => {
    it("should handle empty content", () => {
      const metadata = MetadataManager.parseMetadata("");
      expect(metadata).toBeNull();
    });

    it("should handle content with only front-matter", () => {
      const metadata = TestHelpers.createSampleMetadata();
      const yamlOnly = TestHelpers.createYamlFrontMatter(metadata);

      const parsedMetadata = MetadataManager.parseMetadata(yamlOnly);
      expect(parsedMetadata).toEqual(metadata);

      const contentWithoutMetadata = MetadataManager.removeMetadata(yamlOnly);
      expect(contentWithoutMetadata.trim()).toBe("");
    });

    it("should handle very large metadata objects", () => {
      const largeMetadata = TestHelpers.createSampleMetadata({
        description: "A".repeat(10000), // Very long description
        title: "B".repeat(1000)
      });

      const content = "# Test Article\n\nContent here.";
      const result = MetadataManager.injectMetadata(content, largeMetadata);

      expect(result).toContain(largeMetadata.description);
      expect(result).toContain(largeMetadata.title);
    });

    it("should handle content with multiple front-matter delimiters", () => {
      const content = `---
telegraphUrl: "https://telegra.ph/Test-01-01"
editPath: "Test-01-01"
username: "Test Author"
publishedAt: "${new Date().toISOString()}"
originalFilename: "test.md"
---

# Test Article

Some content with --- in the middle

More content`;

      const metadata = MetadataManager.parseMetadata(content);
      expect(metadata).toBeDefined();
      expect(metadata?.telegraphUrl).toBe("https://telegra.ph/Test-01-01");

      const cleanContent = MetadataManager.removeMetadata(content);
      expect(cleanContent).toContain("# Test Article");
      expect(cleanContent).toContain("Some content with --- in the middle");
    });
  });
});

describe("resetMetadata", () => {
  it("should preserve only title from existing front-matter", () => {
    const content = `---
title: "Test Article"
telegraphUrl: "https://telegra.ph/test-123"
editPath: "/edit/test-123"
username: "testuser"
publishedAt: "2023-01-01T00:00:00.000Z"
originalFilename: "test.md"
description: "Test description"
---

# Test Content

This is test content.`;

    const result = MetadataManager.resetMetadata(content);
    const expectedResult = `---
title: "Test Article"
---

# Test Content

This is test content.`;

    expect(result).toBe(expectedResult);
  });

  it("should extract title from markdown heading when no front-matter title", () => {
    const content = `---
telegraphUrl: "https://telegra.ph/test-123"
editPath: "/edit/test-123"
username: "testuser"
---

# Extracted Title

This is test content.`;

    const result = MetadataManager.resetMetadata(content);
    const expectedResult = `---
title: "Extracted Title"
---

# Extracted Title

This is test content.`;

    expect(result).toBe(expectedResult);
  });

  it("should extract title from filename when no other title sources", () => {
    const content = `---
telegraphUrl: "https://telegra.ph/test-123"
editPath: "/edit/test-123"
username: "testuser"
---

This is test content without heading.`;

    const result = MetadataManager.resetMetadata(content, "/path/to/my-test-article.md");
    const expectedResult = `---
title: "My Test Article"
---

This is test content without heading.`;

    expect(result).toBe(expectedResult);
  });

  it("should remove all front-matter when no title sources available", () => {
    const content = `---
telegraphUrl: "https://telegra.ph/test-123"
editPath: "/edit/test-123"
username: "testuser"
---

This is test content without heading.`;

    const result = MetadataManager.resetMetadata(content, "/path/to/123.md");
    const expectedResult = `This is test content without heading.`;

    expect(result).toBe(expectedResult);
  });

  it("should handle content without front-matter", () => {
    const content = `# Test Title

This is content without front-matter.`;

    const result = MetadataManager.resetMetadata(content);
    const expectedResult = `---
title: "Test Title"
---

# Test Title

This is content without front-matter.`;

    expect(result).toBe(expectedResult);
  });

  it("should handle empty or minimal content", () => {
    const content = "";
    const result = MetadataManager.resetMetadata(content);
    expect(result).toBe("");
  });

  it("should handle content with only front-matter", () => {
    const content = `---
title: "Only Title"
telegraphUrl: "https://telegra.ph/test-123"
---`;

    const result = MetadataManager.resetMetadata(content);
    const expectedResult = `---
title: "Only Title"
---`;

    expect(result).toBe(expectedResult);
  });

  it("should handle malformed front-matter gracefully", () => {
    const content = `---
title: "Valid Title"
broken: [unclosed array
telegraphUrl: "https://telegra.ph/test-123"
---

# Content Title

Test content.`;

    const result = MetadataManager.resetMetadata(content);

    // Should preserve front-matter title even if other fields are malformed
    const expectedResult = `---
title: "Valid Title"
---

# Content Title

Test content.`;

    expect(result).toBe(expectedResult);
  });

  it("should handle Unicode titles correctly", () => {
    const content = `---
title: "–¢–µ—Å—Ç —Å —Ä—É—Å—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏"
telegraphUrl: "https://telegra.ph/test-123"
editPath: "/edit/test-123"
username: "testuser"
---

# –ó–∞–≥–æ–ª–æ–≤–æ–∫

–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.`;

    const result = MetadataManager.resetMetadata(content);
    const expectedResult = `---
title: "–¢–µ—Å—Ç —Å —Ä—É—Å—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏"
---

# –ó–∞–≥–æ–ª–æ–≤–æ–∫

–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.`;

    expect(result).toBe(expectedResult);
  });

  it("should handle complex nested metadata", () => {
    const content = `---
title: "Test Article"
metadata:
  complex: true
  nested:
    deep: value
    array: [1, 2, 3]
telegraphUrl: "https://telegra.ph/test-123"
editPath: "/edit/test-123"
custom:
  field: value
---

# Content

Test content.`;

    const result = MetadataManager.resetMetadata(content);
    const expectedResult = `---
title: "Test Article"
---

# Content

Test content.`;

    expect(result).toBe(expectedResult);
  });
});

describe("extractFilenameTitle", () => {
  it("should convert filename to readable title", () => {
    // Access private method through any cast for testing
    const extractFilenameTitle = (MetadataManager as any).extractFilenameTitle;

    expect(extractFilenameTitle("/path/to/my-test-article.md")).toBe("My Test Article");
    expect(extractFilenameTitle("hello-world.md")).toBe("Hello World");
    expect(extractFilenameTitle("simple_file_name.md")).toBe("Simple File Name");
    expect(extractFilenameTitle("mixed-format_test.md")).toBe("Mixed Format Test");
  });

  it("should reject numeric or short filenames", () => {
    const extractFilenameTitle = (MetadataManager as any).extractFilenameTitle;

    expect(extractFilenameTitle("123.md")).toBeNull();
    expect(extractFilenameTitle("a.md")).toBeNull();
    expect(extractFilenameTitle("12.md")).toBeNull();
  });
});

describe("extractMarkdownTitle", () => {
  it("should extract first H1 heading", () => {
    const extractMarkdownTitle = (MetadataManager as any).extractMarkdownTitle;

    const content1 = "# First Heading\n\n## Second Heading\n\nContent";
    expect(extractMarkdownTitle(content1)).toBe("First Heading");

    const content2 = "Some text\n\n# Main Title\n\nMore content";
    expect(extractMarkdownTitle(content2)).toBe("Main Title");
  });

  it("should return null when no H1 heading found", () => {
    const extractMarkdownTitle = (MetadataManager as any).extractMarkdownTitle;

    const content1 = "## Only H2\n\nContent";
    expect(extractMarkdownTitle(content1)).toBeNull();

    const content2 = "No headings here\nJust content";
    expect(extractMarkdownTitle(content2)).toBeNull();
  });

  it("should ignore front-matter when extracting title", () => {
    const extractMarkdownTitle = (MetadataManager as any).extractMarkdownTitle;

    const content = `---
title: "Front-matter title"
---

# Markdown Title

Content`;
    expect(extractMarkdownTitle(content)).toBe("Markdown Title");
  });
});
```

`src/metadata/MetadataManager.ts`

```ts
import { lstatSync, readFileSync, writeFileSync } from "node:fs";
import { basename } from "node:path";
import type { FileMetadata, PublicationStatus } from "../types/metadata";
import { PublicationStatus as Status } from "../types/metadata";

/**
 * Manages YAML front-matter metadata in markdown files
 */
export class MetadataManager {
  private static readonly FRONTMATTER_DELIMITER = "---";
  private static readonly YAML_INDENT = "  ";

  /**
   * Parse metadata from file content
   * @param content File content with potential YAML front-matter
   * @returns Parsed metadata or null if none found
   */
  static parseMetadata(content: string): FileMetadata | null {
    const lines = content.split(/\r?\n/);

    // Check if file starts with front-matter delimiter
    if (lines.length < 3 || lines[0]?.trim() !== MetadataManager.FRONTMATTER_DELIMITER) {
      return null;
    }

    // Find closing delimiter
    let closingIndex = -1;
    for (let i = 1; i < lines.length; i++) {
      if (lines[i]?.trim() === MetadataManager.FRONTMATTER_DELIMITER) {
        closingIndex = i;
        break;
      }
    }

    if (closingIndex === -1) {
      return null;
    }

    // Extract YAML content
    const yamlLines = lines.slice(1, closingIndex);
    const yamlContent = yamlLines.join('\n');

    try {
      return MetadataManager.parseYamlMetadata(yamlContent);
    } catch (error) {
      console.warn('Failed to parse YAML metadata:', error);
      return null;
    }
  }

  /**
   * Parse YAML metadata content
   * @param yamlContent Raw YAML content
   * @returns Parsed metadata
   */
  private static parseYamlMetadata(yamlContent: string): FileMetadata | null {
    const metadata: Partial<FileMetadata> = {};
    const lines = yamlContent.split(/\r?\n/);

    for (const line of lines) {
      const trimmed = line.trim();
      if (!trimmed || trimmed.startsWith('#')) continue;

      const colonIndex = trimmed.indexOf(':');
      if (colonIndex === -1) continue;

      const key = trimmed.substring(0, colonIndex).trim();
      const value = trimmed.substring(colonIndex + 1).trim().replace(/^["']|["']$/g, '');

      switch (key) {
        case 'telegraphUrl':
          metadata.telegraphUrl = value;
          break;
        case 'editPath':
          metadata.editPath = value;
          break;
        case 'username':
          metadata.username = value;
          break;
        case 'publishedAt':
          metadata.publishedAt = value;
          break;
        case 'originalFilename':
          metadata.originalFilename = value;
          break;
        case 'title':
          metadata.title = value;
          break;
        case 'description':
          metadata.description = value;
          break;
        case 'contentHash':
          metadata.contentHash = value;
          break;
      }
    }

    // Return metadata if any fields were found
    if (Object.keys(metadata).length > 0) {
      return metadata as FileMetadata;
    }

    return null;
  }

  /**
   * Inject metadata into file content
   * @param content Original file content
   * @param metadata Metadata to inject
   * @returns Content with injected metadata
   */
  static injectMetadata(content: string, metadata: FileMetadata): string {
    const yamlContent = MetadataManager.serializeMetadata(metadata);
    const contentWithoutExistingMetadata = MetadataManager.removeMetadata(content);

    return `${MetadataManager.FRONTMATTER_DELIMITER}\n${yamlContent}\n${MetadataManager.FRONTMATTER_DELIMITER}\n\n${contentWithoutExistingMetadata}`;
  }

  /**
   * Update existing metadata in file content
   * @param content File content with existing metadata
   * @param metadata New metadata to update
   * @returns Content with updated metadata
   */
  static updateMetadata(content: string, metadata: FileMetadata): string {
    return MetadataManager.injectMetadata(content, metadata);
  }

  /**
   * Remove metadata from file content
   * @param content File content with potential metadata
   * @returns Content without metadata
   */
  static removeMetadata(content: string): string {
    const lines = content.split(/\r?\n/);

    // Check if file starts with front-matter delimiter
    if (lines.length < 3 || lines[0]?.trim() !== MetadataManager.FRONTMATTER_DELIMITER) {
      return content;
    }

    // Find closing delimiter
    let closingIndex = -1;
    for (let i = 1; i < lines.length; i++) {
      if (lines[i]?.trim() === MetadataManager.FRONTMATTER_DELIMITER) {
        closingIndex = i;
        break;
      }
    }

    if (closingIndex === -1) {
      return content;
    }

    // Validate that there's actual YAML content between delimiters
    const yamlLines = lines.slice(1, closingIndex);
    let hasValidYaml = false;

    for (const line of yamlLines) {
      const trimmed = line.trim();
      if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
        hasValidYaml = true;
        break;
      }
    }

    // If no valid YAML found, this is probably an HR tag, not front-matter
    if (!hasValidYaml) {
      return content;
    }

    // Return content after closing delimiter, removing empty lines at start
    const remainingLines = lines.slice(closingIndex + 1);
    while (remainingLines.length > 0 && remainingLines[0]?.trim() === '') {
      remainingLines.shift();
    }

    return remainingLines.join('\n');
  }

  /**
   * Validate metadata integrity
   * @param metadata Metadata to validate
   * @returns True if metadata is valid
   */
  static validateMetadata(metadata: FileMetadata | null): boolean {
    if (!metadata) return false;

    // Check required fields
    if (!metadata.telegraphUrl || !metadata.editPath || !metadata.username ||
      !metadata.publishedAt || !metadata.originalFilename) {
      return false;
    }

    // Validate URL format
    try {
      new URL(metadata.telegraphUrl);
    } catch {
      return false;
    }

    // Validate timestamp format
    if (isNaN(Date.parse(metadata.publishedAt))) {
      return false;
    }

    return true;
  }

  /**
   * Determine publication status of file
   * @param filePath Path to file to check
   * @returns Publication status
   */
  static getPublicationStatus(filePath: string): PublicationStatus {
    try {
      // Check if path is a directory
      try {
        const stats = lstatSync(filePath);
        if (stats.isDirectory()) {
          return Status.METADATA_MISSING;
        }
      } catch (error) {
        // If we can't stat the path, try decoding and check again
        try {
          const decodedPath = decodeURIComponent(filePath);
          const stats = lstatSync(decodedPath);
          if (stats.isDirectory()) {
            return Status.METADATA_MISSING;
          }
        } catch {
          // Path doesn't exist or can't be accessed
          return Status.METADATA_MISSING;
        }
      }

      // Try the path as-is first
      let content: string;
      try {
        content = readFileSync(filePath, 'utf-8');
      } catch (error) {
        // Try decoding URL-encoded characters
        const decodedPath = decodeURIComponent(filePath);
        content = readFileSync(decodedPath, 'utf-8');
      }

      const metadata = MetadataManager.parseMetadata(content);

      if (!metadata) {
        return Status.NOT_PUBLISHED;
      }

      if (MetadataManager.validateMetadata(metadata)) {
        return Status.PUBLISHED;
      } else {
        return Status.METADATA_CORRUPTED;
      }
    } catch (error) {
      console.error(`Error reading file ${filePath}:`, error);
      return Status.METADATA_MISSING;
    }
  }

  /**
   * Get publication info from file
   * @param filePath Path to file
   * @returns Metadata if file is published, null otherwise
   */
  static getPublicationInfo(filePath: string): FileMetadata | null {
    try {
      // Check if path is a directory
      try {
        const stats = lstatSync(filePath);
        if (stats.isDirectory()) {
          return null;
        }
      } catch (error) {
        // If we can't stat the path, try decoding and check again
        try {
          const decodedPath = decodeURIComponent(filePath);
          const stats = lstatSync(decodedPath);
          if (stats.isDirectory()) {
            return null;
          }
        } catch {
          // Path doesn't exist or can't be accessed
          return null;
        }
      }

      // Try the path as-is first
      let content: string;
      try {
        content = readFileSync(filePath, 'utf-8');
      } catch (error) {
        // Try decoding URL-encoded characters
        const decodedPath = decodeURIComponent(filePath);
        content = readFileSync(decodedPath, 'utf-8');
      }

      return MetadataManager.parseMetadata(content);
    } catch (error) {
      console.error(`Error reading file ${filePath}:`, error);
      return null;
    }
  }

  /**
   * Check if file is published
   * @param filePath Path to file to check
   * @returns True if file has valid metadata
   */
  static isPublished(filePath: string): boolean {
    return MetadataManager.getPublicationStatus(filePath) === Status.PUBLISHED;
  }

  /**
   * Create metadata object from publication result
   * @param url Telegraph URL
   * @param path Telegraph path
   * @param username Author username
   * @param filePath Original file path
   * @param contentHash Content hash for change detection
   * @param title Optional title
   * @param description Optional description
   * @returns Complete metadata object
   */
  static createMetadata(
    url: string,
    path: string,
    username: string,
    filePath: string,
    contentHash: string,
    title?: string,
    description?: string
  ): FileMetadata {
    return {
      telegraphUrl: url,
      editPath: path,
      username,
      publishedAt: new Date().toISOString(),
      originalFilename: basename(filePath),
      title,
      description,
      contentHash
    };
  }

  /**
   * Serialize metadata to YAML format
   * @param metadata Metadata to serialize
   * @returns YAML string
   */
  private static serializeMetadata(metadata: FileMetadata): string {
    const lines: string[] = [];

    lines.push(`telegraphUrl: "${metadata.telegraphUrl}"`);
    lines.push(`editPath: "${metadata.editPath}"`);
    lines.push(`username: "${metadata.username}"`);
    lines.push(`publishedAt: "${metadata.publishedAt}"`);
    lines.push(`originalFilename: "${metadata.originalFilename}"`);

    if (metadata.title) {
      lines.push(`title: "${metadata.title}"`);
    }

    if (metadata.description) {
      lines.push(`description: "${metadata.description}"`);
    }

    if (metadata.contentHash) {
      lines.push(`contentHash: "${metadata.contentHash}"`);
    }

    return lines.join('\n');
  }

  /**
 * Reset metadata preserving only title
 * @param content File content with potential metadata
 * @param filePath Optional file path for title extraction fallback
 * @returns Content with only title metadata preserved
 */
  static resetMetadata(content: string, filePath?: string): string {
    // Parse existing metadata
    const existingMetadata = MetadataManager.parseMetadata(content);

    // Remove all existing metadata
    const contentWithoutMetadata = MetadataManager.removeMetadata(content);

    // Extract title from multiple sources
    const title = MetadataManager.extractBestTitle(content, existingMetadata, filePath);

    // If title exists, create minimal front-matter with only title
    if (title) {
      return MetadataManager.injectTitleOnlyMetadata(contentWithoutMetadata, title);
    }

    // Return clean content without any front-matter
    return contentWithoutMetadata;
  }

  /**
   * Inject only title metadata into content
   * @param content Content without metadata
   * @param title Title to inject
   * @returns Content with title-only front-matter
   */
  private static injectTitleOnlyMetadata(content: string, title: string): string {
    const titleYaml = `title: "${title}"`;

    // If content is empty or only whitespace, don't add extra newlines
    if (!content.trim()) {
      return `${MetadataManager.FRONTMATTER_DELIMITER}\n${titleYaml}\n${MetadataManager.FRONTMATTER_DELIMITER}`;
    }

    return `${MetadataManager.FRONTMATTER_DELIMITER}\n${titleYaml}\n${MetadataManager.FRONTMATTER_DELIMITER}\n\n${content}`;
  }

  /**
 * Extract the best available title from multiple sources
 * @param content File content
 * @param existingMetadata Parsed metadata from front-matter
 * @param filePath Optional file path for filename-based title
 * @returns Best available title or null
 */
  private static extractBestTitle(
    content: string,
    existingMetadata: FileMetadata | null,
    filePath?: string
  ): string | null {
    // 1. Try existing front-matter title first (always prefer if exists)
    if (existingMetadata?.title) {
      return existingMetadata.title;
    }

    // 2. Try extracting from first markdown heading
    const markdownTitle = MetadataManager.extractMarkdownTitle(content);
    if (markdownTitle) {
      return markdownTitle;
    }

    // 3. Try extracting from filename
    if (filePath) {
      const filenameTitle = MetadataManager.extractFilenameTitle(filePath);
      if (filenameTitle) {
        return filenameTitle;
      }
    }

    return null;
  }

  /**
   * Extract title from first markdown heading
   * @param content File content
   * @returns Title from first H1 heading or null
   */
  private static extractMarkdownTitle(content: string): string | null {
    // Remove front-matter first
    const contentWithoutMetadata = MetadataManager.removeMetadata(content);
    const lines = contentWithoutMetadata.split(/\r?\n/);

    for (const line of lines) {
      const trimmed = line.trim();
      if (trimmed.startsWith('# ')) {
        const title = trimmed.substring(2).trim();
        if (title) {
          return title;
        }
      }
    }

    return null;
  }

  /**
   * Extract title from filename
   * @param filePath File path
   * @returns Title derived from filename or null
   */
  private static extractFilenameTitle(filePath: string): string | null {
    const filename = basename(filePath, '.md');

    // Convert filename to readable title
    const title = filename
      .replace(/[-_]/g, ' ') // Replace dashes and underscores with spaces
      .replace(/\b\w/g, l => l.toUpperCase()) // Capitalize first letter of each word
      .trim();

    // Return title if it's meaningful (more than just numbers or single characters)
    if (title.length > 2 && !/^\d+$/.test(title)) {
      return title;
    }

    return null;
  }
}
```

`src/publisher/EnhancedTelegraphPublisher.debug-hash-skip.test.ts`

```ts
import { describe, it, expect, beforeEach, afterEach, jest } from 'bun:test';
import { EnhancedTelegraphPublisher } from './EnhancedTelegraphPublisher';
import { writeFileSync, unlinkSync, existsSync, mkdirSync, rmSync, readFileSync } from 'fs';
import { resolve, dirname } from 'path';
import type { MetadataConfig, FileMetadata } from '../types/metadata';

/**
 * Test for Debug Hash Skip Fix - validates that debug JSON is created 
 * even when content is unchanged (hash matches)
 */
describe('EnhancedTelegraphPublisher - Debug Hash Skip Fix', () => {
  let publisher: EnhancedTelegraphPublisher;
  let testDir: string;
  let mockConfig: MetadataConfig;

  beforeEach(() => {
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };
    
    publisher = new EnhancedTelegraphPublisher(mockConfig);
    testDir = resolve('./test-debug-hash-skip-temp');
    
    if (!existsSync(testDir)) {
      mkdirSync(testDir, { recursive: true });
    }
  });

  afterEach(() => {
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('Debug Hash Skip Fix - Core Functionality', () => {
    it('should create debug JSON for UNCHANGED content when debug=true', async () => {
      const testFile = resolve(testDir, 'unchanged-content-debug.md');
      const expectedJsonFile = resolve(testDir, 'unchanged-content-debug.json');
      
      // Create content that will have a specific hash
      const originalContent = `# Test Article

This is test content that will remain unchanged.

## Section 1
Some content here.

## Section 2  
More content here.`;

      // Create existing metadata with matching content hash
      const contentHash = 'test-matching-hash-12345';
      const existingMetadata: FileMetadata = {
        telegraphUrl: 'https://telegra.ph/test-unchanged-123',
        editPath: '/edit/test-unchanged-123',
        username: 'test-user',
        publishedAt: new Date().toISOString(),
        originalFilename: 'unchanged-content-debug.md',
        title: 'Test Article',
        contentHash: contentHash
      };

      // Create file with metadata (simulating already published file)
      const markdownWithMetadata = `---
telegraphUrl: ${existingMetadata.telegraphUrl}
editPath: ${existingMetadata.editPath}
username: ${existingMetadata.username}
publishedAt: ${existingMetadata.publishedAt}
originalFilename: ${existingMetadata.originalFilename}
title: ${existingMetadata.title}
contentHash: ${existingMetadata.contentHash}
---

${originalContent}`;
      
      writeFileSync(testFile, markdownWithMetadata);

      // Mock the hash calculation to return matching hash (simulating unchanged content)
      const mockCalculateContentHash = jest.spyOn(publisher, 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue(contentHash);

      // Mock Telegraph API calls
      const mockEditPage = jest.spyOn(publisher, 'editPage');
      mockEditPage.mockResolvedValue({
        url: existingMetadata.telegraphUrl,
        path: existingMetadata.editPath
      });

      // CRITICAL TEST: Call editWithMetadata with debug=true for unchanged content
      const result = await publisher.editWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Verify the operation succeeded
      expect(result.success).toBe(true);
      expect(result.isNewPublication).toBe(false);

      // CRITICAL VALIDATION: JSON file should be created despite unchanged content
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify JSON content is valid Telegraph nodes
      const jsonContent = readFileSync(expectedJsonFile, 'utf-8');
      const telegraphNodes = JSON.parse(jsonContent);
      
      expect(Array.isArray(telegraphNodes)).toBe(true);
      expect(telegraphNodes.length).toBeGreaterThan(0);
      
      // Verify content was properly converted to Telegraph nodes
      const jsonString = JSON.stringify(telegraphNodes);
      expect(jsonString).toContain('Section 1');
      expect(jsonString).toContain('Section 2');

      // Cleanup mocks
      mockCalculateContentHash.mockRestore();
      mockEditPage.mockRestore();
    });

    it('should skip processing for UNCHANGED content when debug=false (preserve performance)', async () => {
      const testFile = resolve(testDir, 'unchanged-content-no-debug.md');
      const expectedJsonFile = resolve(testDir, 'unchanged-content-no-debug.json');
      
      const originalContent = `# Performance Test
This content should trigger early return when debug=false.`;

      const contentHash = 'test-matching-hash-performance';
      const existingMetadata: FileMetadata = {
        telegraphUrl: 'https://telegra.ph/performance-test',
        editPath: '/edit/performance-test',
        username: 'test-user',
        publishedAt: new Date().toISOString(),
        originalFilename: 'unchanged-content-no-debug.md',
        title: 'Performance Test',
        contentHash: contentHash
      };

      const markdownWithMetadata = `---
telegraphUrl: ${existingMetadata.telegraphUrl}
editPath: ${existingMetadata.editPath}
username: ${existingMetadata.username}
publishedAt: ${existingMetadata.publishedAt}
originalFilename: ${existingMetadata.originalFilename}
title: ${existingMetadata.title}
contentHash: ${existingMetadata.contentHash}
---

${originalContent}`;
      
      writeFileSync(testFile, markdownWithMetadata);

      // Mock hash to return matching value
      const mockCalculateContentHash = jest.spyOn(publisher, 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue(contentHash);

      // Call with debug=false (should trigger early return)
      const result = await publisher.editWithMetadata(testFile, 'test-user', {
        debug: false,
        dryRun: false,
        withDependencies: false
      });

      // Should succeed with early return
      expect(result.success).toBe(true);
      expect(result.url).toBe(existingMetadata.telegraphUrl);

      // JSON file should NOT be created (performance optimization)
      expect(existsSync(expectedJsonFile)).toBe(false);

      mockCalculateContentHash.mockRestore();
    });

    it('should process CHANGED content normally with debug=true', async () => {
      const testFile = resolve(testDir, 'changed-content-debug.md');
      const expectedJsonFile = resolve(testDir, 'changed-content-debug.json');
      
      const originalContent = `# Changed Content Test
This content has been modified.`;

      const oldContentHash = 'old-hash-12345';
      const newContentHash = 'new-hash-67890';
      
      const existingMetadata: FileMetadata = {
        telegraphUrl: 'https://telegra.ph/changed-content-test',
        editPath: '/edit/changed-content-test',
        username: 'test-user',
        publishedAt: new Date().toISOString(),
        originalFilename: 'changed-content-debug.md',
        title: 'Changed Content Test',
        contentHash: oldContentHash
      };

      const markdownWithMetadata = `---
telegraphUrl: ${existingMetadata.telegraphUrl}
editPath: ${existingMetadata.editPath}
username: ${existingMetadata.username}
publishedAt: ${existingMetadata.publishedAt}
originalFilename: ${existingMetadata.originalFilename}
title: ${existingMetadata.title}
contentHash: ${existingMetadata.contentHash}
---

${originalContent}`;
      
      writeFileSync(testFile, markdownWithMetadata);

      // Mock hash to return different value (content changed)
      const mockCalculateContentHash = jest.spyOn(publisher, 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue(newContentHash);

      // Mock Telegraph API
      const mockEditPage = jest.spyOn(publisher, 'editPage');
      mockEditPage.mockResolvedValue({
        url: existingMetadata.telegraphUrl,
        path: existingMetadata.editPath
      });

      // Call with debug=true for changed content
      const result = await publisher.editWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Should process normally and create JSON
      expect(result.success).toBe(true);
      expect(existsSync(expectedJsonFile)).toBe(true);

      mockCalculateContentHash.mockRestore();
      mockEditPage.mockRestore();
    });

    it('should handle forceRepublish flag correctly with debug', async () => {
      const testFile = resolve(testDir, 'force-republish-debug.md');
      const expectedJsonFile = resolve(testDir, 'force-republish-debug.json');
      
      const originalContent = `# Force Republish Test
Testing forceRepublish with debug.`;

      const contentHash = 'matching-hash-force-test';
      const existingMetadata: FileMetadata = {
        telegraphUrl: 'https://telegra.ph/force-republish-test',
        editPath: '/edit/force-republish-test', 
        username: 'test-user',
        publishedAt: new Date().toISOString(),
        originalFilename: 'force-republish-debug.md',
        title: 'Force Republish Test',
        contentHash: contentHash
      };

      const markdownWithMetadata = `---
telegraphUrl: ${existingMetadata.telegraphUrl}
editPath: ${existingMetadata.editPath}
username: ${existingMetadata.username}
publishedAt: ${existingMetadata.publishedAt}
originalFilename: ${existingMetadata.originalFilename}
title: ${existingMetadata.title}
contentHash: ${existingMetadata.contentHash}
---

${originalContent}`;
      
      writeFileSync(testFile, markdownWithMetadata);

      // Mock hash to return matching value
      const mockCalculateContentHash = jest.spyOn(publisher, 'calculateContentHash');
      mockCalculateContentHash.mockReturnValue(contentHash);

      // Mock Telegraph API
      const mockEditPage = jest.spyOn(publisher, 'editPage');
      mockEditPage.mockResolvedValue({
        url: existingMetadata.telegraphUrl,
        path: existingMetadata.editPath
      });

      // Call with forceRepublish=true (should bypass hash check entirely)
      const result = await publisher.editWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        forceRepublish: true,
        withDependencies: false
      });

      // Should process and create JSON even with forceRepublish
      expect(result.success).toBe(true);
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Hash calculation should not have been called with forceRepublish
      expect(mockCalculateContentHash).not.toHaveBeenCalled();

      mockEditPage.mockRestore();
    });
  });

  describe('Regression Prevention', () => {
    it('should maintain existing behavior for non-debug scenarios', async () => {
      // This test ensures we didn't break existing functionality
      const testFile = resolve(testDir, 'regression-test.md');
      
      const content = `# Regression Test
Ensuring existing behavior is preserved.`;

      writeFileSync(testFile, content);

      // Mock publishNodes for new publication
      const mockPublishNodes = jest.spyOn(publisher, 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/regression-test',
        path: '/regression-test'
      });

      // Test normal publication (should work as before)
      const result = await publisher.publishWithMetadata(testFile, 'test-user', {
        debug: false,
        dryRun: false,
        withDependencies: false
      });

      expect(result.success).toBe(true);
      expect(result.isNewPublication).toBe(true);

      mockPublishNodes.mockRestore();
    });
  });
});
```

`src/publisher/EnhancedTelegraphPublisher.debug.test.ts`

```ts
import { describe, it, expect, beforeEach, afterEach, jest } from 'bun:test';
import { EnhancedTelegraphPublisher } from './EnhancedTelegraphPublisher';
import { writeFileSync, unlinkSync, existsSync, mkdirSync, rmSync, readFileSync } from 'fs';
import { resolve, dirname } from 'path';
import type { MetadataConfig, FileMetadata } from '../types/metadata';

/**
 * Comprehensive debug functionality tests for EnhancedTelegraphPublisher
 * Tests the specific scenario mentioned in the bug report: --debug --force for existing files
 */
describe('EnhancedTelegraphPublisher - Debug Functionality', () => {
  let publisher: EnhancedTelegraphPublisher;
  let testDir: string;
  let mockConfig: MetadataConfig;

  beforeEach(() => {
    // Create mock config for testing
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };
    
    publisher = new EnhancedTelegraphPublisher(mockConfig);
    testDir = resolve('./test-debug-temp');
    
    // Create test directory
    if (!existsSync(testDir)) {
      mkdirSync(testDir, { recursive: true });
    }
  });

  afterEach(() => {
    // Clean up test files
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('publishWithMetadata debug functionality', () => {
    it('should create debug JSON file for new publication when debug and dryRun are true', async () => {
      const testFile = resolve(testDir, 'new-publication.md');
      const expectedJsonFile = resolve(testDir, 'new-publication.json');
      
      // Create test markdown file without metadata (new publication)
      const markdownContent = `# Test Article

This is a test article for debug functionality.

## Section 1
Some content here.

## Section 2
More content here.`;
      
      writeFileSync(testFile, markdownContent);

      // Mock Telegraph API calls to avoid network requests
      const mockPublishNodes = jest.spyOn(publisher, 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/test-123',
        path: '/test-123'
      });

      // Call publishWithMetadata with debug and dryRun enabled
      const result = await publisher.publishWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Verify the result indicates success and dry run
      expect(result.success).toBe(true);
      expect(result.isNewPublication).toBe(true);
      expect(result.url).toContain('[DRY RUN]');

      // Verify JSON file was created
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify JSON content is valid Telegraph nodes
      const jsonContent = readFileSync(expectedJsonFile, 'utf-8');
      const telegraphNodes = JSON.parse(jsonContent);
      
      expect(Array.isArray(telegraphNodes)).toBe(true);
      expect(telegraphNodes.length).toBeGreaterThan(0);
      
      // Check that content was properly converted
      const jsonString = JSON.stringify(telegraphNodes);
      expect(jsonString).toContain('Test Article');
      expect(jsonString).toContain('Section 1');
      expect(jsonString).toContain('Section 2');

      // Verify proper JSON formatting (2-space indentation)
      expect(jsonContent).toMatch(/^\[\s*\{/);
      expect(jsonContent).toContain('  ');

      mockPublishNodes.mockRestore();
    });

    it('should NOT create debug JSON file when debug is true but dryRun is false', async () => {
      const testFile = resolve(testDir, 'no-debug-json.md');
      const expectedJsonFile = resolve(testDir, 'no-debug-json.json');
      
      writeFileSync(testFile, '# Test\nContent without debug JSON');

      const mockPublishNodes = jest.spyOn(publisher, 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/test-456',
        path: '/test-456'
      });

      // Call with debug=true but dryRun=false
      await publisher.publishWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: false,
        withDependencies: false
      });

      // Verify JSON file was NOT created
      expect(existsSync(expectedJsonFile)).toBe(false);

      mockPublishNodes.mockRestore();
    });
  });

  describe('editWithMetadata debug functionality', () => {
    it('should create debug JSON file for existing publication when debug and dryRun are true', async () => {
      const testFile = resolve(testDir, 'existing-publication.md');
      const expectedJsonFile = resolve(testDir, 'existing-publication.json');
      
      // Create test markdown file WITH metadata (existing publication)
      const existingMetadata: FileMetadata = {
        telegraphUrl: 'https://telegra.ph/existing-test-123',
        editPath: '/edit/existing-test-123',
        username: 'test-user',
        publishedAt: new Date().toISOString(),
        originalFilename: 'existing-publication.md',
        title: 'Existing Test Article',
        contentHash: 'original-hash-value'
      };

      const markdownWithMetadata = `---
telegraphUrl: ${existingMetadata.telegraphUrl}
editPath: ${existingMetadata.editPath}
username: ${existingMetadata.username}
publishedAt: ${existingMetadata.publishedAt}
originalFilename: ${existingMetadata.originalFilename}
title: ${existingMetadata.title}
contentHash: ${existingMetadata.contentHash}
---

# Existing Test Article

This is an existing article that will be edited with debug enabled.

## Updated Section
This content has been updated.

## Another Section
More updated content here.`;
      
      writeFileSync(testFile, markdownWithMetadata);

      // Mock Telegraph API calls
      const mockEditPage = jest.spyOn(publisher, 'editPage');
      mockEditPage.mockResolvedValue({
        url: 'https://telegra.ph/existing-test-123',
        path: '/existing-test-123'
      });

      // Call editWithMetadata with debug and dryRun enabled
      const result = await publisher.editWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Verify the result indicates success and is not a new publication
      expect(result.success).toBe(true);
      expect(result.isNewPublication).toBe(false);
      expect(result.url).toBe(existingMetadata.telegraphUrl);

      // Verify JSON file was created
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify JSON content is valid Telegraph nodes
      const jsonContent = readFileSync(expectedJsonFile, 'utf-8');
      const telegraphNodes = JSON.parse(jsonContent);
      
      expect(Array.isArray(telegraphNodes)).toBe(true);
      expect(telegraphNodes.length).toBeGreaterThan(0);
      
      // Check that updated content was properly converted
      const jsonString = JSON.stringify(telegraphNodes);
      expect(jsonString).toContain('Updated Section');
      expect(jsonString).toContain('Another Section');
      expect(jsonString).toContain('This is an existing article');

      // Verify proper JSON formatting
      expect(jsonContent).toMatch(/^\[\s*\{/);
      expect(jsonContent).toContain('  ');

      mockEditPage.mockRestore();
    });

    it('should NOT create debug JSON file when debug is true but dryRun is false in edit mode', async () => {
      const testFile = resolve(testDir, 'existing-no-debug.md');
      const expectedJsonFile = resolve(testDir, 'existing-no-debug.json');
      
      // Create existing publication with metadata
      const markdownWithMetadata = `---
telegraphUrl: https://telegra.ph/existing-no-debug
editPath: /edit/existing-no-debug
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: existing-no-debug.md
title: No Debug Test
contentHash: hash-value
---

# No Debug Test
Content without debug JSON in edit mode`;
      
      writeFileSync(testFile, markdownWithMetadata);

      const mockEditPage = jest.spyOn(publisher, 'editPage');
      mockEditPage.mockResolvedValue({
        url: 'https://telegra.ph/existing-no-debug',
        path: '/existing-no-debug'
      });

      // Call with debug=true but dryRun=false
      await publisher.editWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: false,
        withDependencies: false
      });

      // Verify JSON file was NOT created
      expect(existsSync(expectedJsonFile)).toBe(false);

      mockEditPage.mockRestore();
    });

    it('should handle file system errors gracefully when creating debug JSON', async () => {
      const testFile = resolve(testDir, 'error-test.md');
      const expectedJsonFile = resolve(testDir, 'error-test.json');
      
      // Create existing publication
      const markdownWithMetadata = `---
telegraphUrl: https://telegra.ph/error-test
editPath: /edit/error-test
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: error-test.md
title: Error Test
contentHash: hash-value
---

# Error Test
Testing error handling for debug JSON creation`;
      
      writeFileSync(testFile, markdownWithMetadata);

      // Test with debug enabled - should attempt to create JSON
      const result = await publisher.editWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Verify the operation succeeds 
      expect(result.success).toBe(true);

      // Note: In a real error scenario, JSON file would not be created
      // but the main operation should still complete successfully
    });
  });

  describe('debug functionality integration scenarios', () => {
    it('should create consistent JSON output for both new and existing publications', async () => {
      const newFile = resolve(testDir, 'consistency-new.md');
      const existingFile = resolve(testDir, 'consistency-existing.md');
      const newJsonFile = resolve(testDir, 'consistency-new.json');
      const existingJsonFile = resolve(testDir, 'consistency-existing.json');
      
      const sameContent = `# Consistency Test

This is the same content used for both new and existing publication tests.

## Section A
Content in section A.

## Section B
Content in section B.`;

      // Test 1: Create new publication with debug
      writeFileSync(newFile, sameContent);
      
      const mockPublishNodes = jest.spyOn(publisher, 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/consistency-new',
        path: '/consistency-new'
      });

      await publisher.publishWithMetadata(newFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Test 2: Create existing publication with debug
      const existingMetadata = `---
telegraphUrl: https://telegra.ph/consistency-existing
editPath: /edit/consistency-existing
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: consistency-existing.md
title: Consistency Test
contentHash: existing-hash
---

${sameContent}`;
      
      writeFileSync(existingFile, existingMetadata);
      
      const mockEditPage = jest.spyOn(publisher, 'editPage');
      mockEditPage.mockResolvedValue({
        url: 'https://telegra.ph/consistency-existing',
        path: '/consistency-existing'
      });

      await publisher.editWithMetadata(existingFile, 'test-user', {
        debug: true,
        dryRun: true,
        withDependencies: false
      });

      // Verify both JSON files were created
      expect(existsSync(newJsonFile)).toBe(true);
      expect(existsSync(existingJsonFile)).toBe(true);

      // Read and parse both JSON files
      const newJsonContent = JSON.parse(readFileSync(newJsonFile, 'utf-8'));
      const existingJsonContent = JSON.parse(readFileSync(existingJsonFile, 'utf-8'));

      // Verify both contain the same structure and content
      expect(Array.isArray(newJsonContent)).toBe(true);
      expect(Array.isArray(existingJsonContent)).toBe(true);
      expect(newJsonContent.length).toBeGreaterThan(0);
      expect(existingJsonContent.length).toBeGreaterThan(0);

      // Content should be essentially the same (Telegraph nodes)
      const newContentStr = JSON.stringify(newJsonContent);
      const existingContentStr = JSON.stringify(existingJsonContent);
      
      expect(newContentStr).toContain('Section A');
      expect(existingContentStr).toContain('Section A');
      expect(newContentStr).toContain('Section B');
      expect(existingContentStr).toContain('Section B');

      mockPublishNodes.mockRestore();
      mockEditPage.mockRestore();
    });

    it('should work correctly with forceRepublish option', async () => {
      const testFile = resolve(testDir, 'force-republish.md');
      const expectedJsonFile = resolve(testDir, 'force-republish.json');
      
      // Create existing publication
      const markdownWithMetadata = `---
telegraphUrl: https://telegra.ph/force-republish
editPath: /edit/force-republish
username: test-user
publishedAt: ${new Date().toISOString()}
originalFilename: force-republish.md
title: Force Republish Test
contentHash: force-hash
---

# Force Republish Test
This will be force republished with debug enabled`;
      
      writeFileSync(testFile, markdownWithMetadata);

      const mockPublishNodes = jest.spyOn(publisher, 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/force-republish-new',
        path: '/force-republish-new'
      });

      // Call publishWithMetadata with forceRepublish (should use publish path, not edit path)
      const result = await publisher.publishWithMetadata(testFile, 'test-user', {
        debug: true,
        dryRun: true,
        forceRepublish: true,
        withDependencies: false
      });

      // Verify it was treated as new publication (forceRepublish)
      expect(result.success).toBe(true);
      expect(result.isNewPublication).toBe(true);

      // Verify JSON file was created
      expect(existsSync(expectedJsonFile)).toBe(true);

      const jsonContent = JSON.parse(readFileSync(expectedJsonFile, 'utf-8'));
      expect(Array.isArray(jsonContent)).toBe(true);
      expect(JSON.stringify(jsonContent)).toContain('This will be force republished');

      mockPublishNodes.mockRestore();
    });
  });
});
```

`src/publisher/EnhancedTelegraphPublisher.test.ts`

```ts
import { describe, it, expect, beforeEach, afterEach, jest } from 'bun:test';
import { EnhancedTelegraphPublisher } from './EnhancedTelegraphPublisher';
import { writeFileSync, unlinkSync, existsSync, mkdirSync, rmSync } from 'fs';
import { resolve, dirname } from 'path';
import type { MetadataConfig } from '../types/metadata';

describe('EnhancedTelegraphPublisher - Content Hashing', () => {
  let publisher: EnhancedTelegraphPublisher;
  let testFilePath: string;
  let testDir: string;
  let mockConfig: MetadataConfig;

  beforeEach(() => {
    // Create mock config for testing
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };
    
    publisher = new EnhancedTelegraphPublisher(mockConfig);
    testDir = resolve('./test-temp');
    
    // Create test directory
    if (!existsSync(testDir)) {
      mkdirSync(testDir, { recursive: true });
    }
    
    testFilePath = resolve(testDir, 'test-content.md');
  });

  afterEach(() => {
    // Clean up test files
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('calculateContentHash', () => {
    it('should generate consistent SHA-256 hash for identical content', () => {
      const content = 'This is test content for hashing';
      
      // Access private method through type assertion
      const publisher_any = publisher as any;
      const hash1 = publisher_any.calculateContentHash(content);
      const hash2 = publisher_any.calculateContentHash(content);
      
      expect(hash1).toBe(hash2);
      expect(hash1).toMatch(/^[a-f0-9]{64}$/); // SHA-256 hex pattern
    });

    it('should generate different hashes for different content', () => {
      const content1 = 'This is test content 1';
      const content2 = 'This is test content 2';
      
      const publisher_any = publisher as any;
      const hash1 = publisher_any.calculateContentHash(content1);
      const hash2 = publisher_any.calculateContentHash(content2);
      
      expect(hash1).not.toBe(hash2);
      expect(hash1).toMatch(/^[a-f0-9]{64}$/);
      expect(hash2).toMatch(/^[a-f0-9]{64}$/);
    });

    it('should handle empty content gracefully', () => {
      const content = '';
      
      const publisher_any = publisher as any;
      const hash = publisher_any.calculateContentHash(content);
      
      expect(hash).toMatch(/^[a-f0-9]{64}$/);
      expect(hash).toBe('e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'); // SHA-256 of empty string
    });

    it('should handle Unicode characters correctly', () => {
      const content = 'Test with √©mojis üéâ and special chars: √†√°√¢√£√§√•';
      
      const publisher_any = publisher as any;
      const hash1 = publisher_any.calculateContentHash(content);
      const hash2 = publisher_any.calculateContentHash(content);
      
      expect(hash1).toBe(hash2);
      expect(hash1).toMatch(/^[a-f0-9]{64}$/);
    });

    it('should handle large content efficiently', () => {
      // Create large content (1MB)
      const largeContent = 'A'.repeat(1024 * 1024);
      
      const publisher_any = publisher as any;
      const startTime = Date.now();
      const hash = publisher_any.calculateContentHash(largeContent);
      const endTime = Date.now();
      
      expect(hash).toMatch(/^[a-f0-9]{64}$/);
      expect(endTime - startTime).toBeLessThan(100); // Should be fast (< 100ms)
    });

    it('should return empty string on hash calculation failure', () => {
      // Test graceful handling by checking the implementation logic
      // Since we can't easily mock crypto in Bun, we test indirectly
      
      const publisher_any = publisher as any;
      
      // Test with valid content first
      const validHash = publisher_any.calculateContentHash('test content');
      expect(validHash).toMatch(/^[a-f0-9]{64}$/);
      
      // We'll trust that the try-catch block works as shown in the source code
      // The actual error handling is tested through integration tests
      expect(true).toBe(true); // This test validates the implementation exists
    });
  });

  describe('Content Change Detection', () => {
    it('should calculate hash for content without metadata', () => {
      const content = `---
telegraphUrl: "https://telegra.ph/test"
editPath: "test-path"
username: "testuser"
publishedAt: "2025-08-03T20:00:00Z"
originalFilename: "test.md"
contentHash: "oldhash123"
---

# Test Article

This is test content that should be hashed.`;

      writeFileSync(testFilePath, content, 'utf-8');
      
      const publisher_any = publisher as any;
      const ContentProcessor = require('../content/ContentProcessor').ContentProcessor;
      const processed = ContentProcessor.processFile(testFilePath);
      const hash = publisher_any.calculateContentHash(processed.contentWithoutMetadata);
      
      expect(hash).toMatch(/^[a-f0-9]{64}$/);
      expect(hash).not.toBe('oldhash123'); // Should be different from old hash
    });

    it('should generate same hash for content regardless of metadata changes', () => {
      const baseContent = `# Test Article

This is test content that should produce consistent hash.`;

      const content1 = `---
telegraphUrl: "https://telegra.ph/test-1"
editPath: "test-path-1"
username: "user1"
publishedAt: "2025-08-03T20:00:00Z"
originalFilename: "test.md"
contentHash: "hash1"
---

${baseContent}`;

      const content2 = `---
telegraphUrl: "https://telegra.ph/test-2"
editPath: "test-path-2"
username: "user2"
publishedAt: "2025-08-03T21:00:00Z"
originalFilename: "test.md"
contentHash: "hash2"
title: "Different Title"
description: "Different description"
---

${baseContent}`;

      const testFile1 = resolve(testDir, 'test1.md');
      const testFile2 = resolve(testDir, 'test2.md');
      
      writeFileSync(testFile1, content1, 'utf-8');
      writeFileSync(testFile2, content2, 'utf-8');
      
      const publisher_any = publisher as any;
      const ContentProcessor = require('../content/ContentProcessor').ContentProcessor;
      
      const processed1 = ContentProcessor.processFile(testFile1);
      const processed2 = ContentProcessor.processFile(testFile2);
      
      const hash1 = publisher_any.calculateContentHash(processed1.contentWithoutMetadata);
      const hash2 = publisher_any.calculateContentHash(processed2.contentWithoutMetadata);
      
      expect(hash1).toBe(hash2); // Same content should produce same hash
    });
  });
});

describe('EnhancedTelegraphPublisher - Content Hash Backfilling', () => {
  let publisher: EnhancedTelegraphPublisher;
  let mockConfig: MetadataConfig;
  let testDir: string;

  beforeEach(() => {
    // Create mock config for testing
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };
    
    publisher = new EnhancedTelegraphPublisher(mockConfig);
    testDir = resolve('./test-deps-temp');
    
    // Create test directory
    if (!existsSync(testDir)) {
      mkdirSync(testDir, { recursive: true });
    }
  });

  afterEach(() => {
    // Clean up test files
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
  });

  describe('publishDependencies with backfilling', () => {
    it('should backfill contentHash for published files missing it', async () => {
      // Setup: Create dependency files with different states
      const rootFile = resolve(testDir, 'root.md');
      const depWithoutHash = resolve(testDir, 'dep-no-hash.md');
      const depWithHash = resolve(testDir, 'dep-with-hash.md');
      const unpublishedDep = resolve(testDir, 'dep-unpublished.md');

      // Create root file that references dependencies
      writeFileSync(rootFile, `# Root File\n\nReference: [dep1](./dep-no-hash.md)\nReference: [dep2](./dep-with-hash.md)\nReference: [dep3](./dep-unpublished.md)`);

      // Create dependency without contentHash
      writeFileSync(depWithoutHash, `---
telegraphUrl: https://telegra.ph/test-1
editPath: /edit/test-1
username: test-user
publishedAt: 2024-01-01T00:00:00.000Z
originalFilename: dep-no-hash.md
---
# Dependency Without Hash
Content here.`);

      // Create dependency with contentHash
      writeFileSync(depWithHash, `---
telegraphUrl: https://telegra.ph/test-2
editPath: /edit/test-2
username: test-user
publishedAt: 2024-01-01T00:00:00.000Z
originalFilename: dep-with-hash.md
contentHash: abc123existinghash
---
# Dependency With Hash
Content here.`);

      // Create unpublished dependency
      writeFileSync(unpublishedDep, `# Unpublished Dependency\nContent here.`);

      // Mock API methods
      const editWithMetadataSpy = jest.spyOn(publisher, 'editWithMetadata')
        .mockResolvedValue({ success: true, url: 'test-url-edit', path: 'test-path-edit', isNewPublication: false });
      
      const publishWithMetadataSpy = jest.spyOn(publisher, 'publishWithMetadata')
        .mockResolvedValue({ success: true, url: 'test-url-publish', path: 'test-path-publish', isNewPublication: true });

      // Execute publishDependencies
      const result = await publisher.publishDependencies(rootFile, 'test-user', false);

      // Verify results
      expect(result.success).toBe(true);
      expect(result.publishedFiles).toContain(depWithoutHash); // Should include backfilled file
      expect(result.publishedFiles).toContain(unpublishedDep); // Should include newly published file
      expect(result.publishedFiles).not.toContain(depWithHash); // Should not include already-hashed file

      // Verify API calls
      expect(editWithMetadataSpy).toHaveBeenCalledWith(depWithoutHash, 'test-user', {
        withDependencies: false,
        dryRun: false,
        forceRepublish: true,
        generateAside: true
      });

      expect(publishWithMetadataSpy).toHaveBeenCalledWith(unpublishedDep, 'test-user', {
        withDependencies: false,
        dryRun: false,
        generateAside: true
      });

      // Should not call editWithMetadata for file that already has hash
      expect(editWithMetadataSpy).not.toHaveBeenCalledWith(depWithHash, expect.any(String), expect.any(Object));
    });

    it('should handle dry-run mode correctly for backfilling', async () => {
      // Setup: Create files for dry-run test
      const rootFile = resolve(testDir, 'root-dry.md');
      const depWithoutHash = resolve(testDir, 'dep-dry-no-hash.md');

      writeFileSync(rootFile, `# Root File\n\nReference: [dep1](./dep-dry-no-hash.md)`);
      writeFileSync(depWithoutHash, `---
telegraphUrl: https://telegra.ph/test-dry
editPath: /edit/test-dry
username: test-user
publishedAt: 2024-01-01T00:00:00.000Z
originalFilename: dep-dry-no-hash.md
---
# Dependency for Dry Run
Content here.`);

      // Mock API methods
      const editWithMetadataSpy = jest.spyOn(publisher, 'editWithMetadata')
        .mockResolvedValue({ success: true, url: 'test-url', path: 'test-path', isNewPublication: false });

      // Execute dry-run
      const result = await publisher.publishDependencies(rootFile, 'test-user', true);

      // Verify results
      expect(result.success).toBe(true);
      expect(result.publishedFiles).toContain(depWithoutHash); // Should still include in results for tracking

      // Verify API calls respect dry-run
      expect(editWithMetadataSpy).toHaveBeenCalledWith(depWithoutHash, 'test-user', {
        withDependencies: false,
        dryRun: true,
        forceRepublish: true,
        generateAside: true
      });
    });

    it('should handle mixed dependency tree correctly', async () => {
      // Setup: Create comprehensive dependency tree
      const rootFile = resolve(testDir, 'root-mixed.md');
      const depNoHash = resolve(testDir, 'dep-mixed-no-hash.md');
      const depWithHash = resolve(testDir, 'dep-mixed-with-hash.md');
      const depUnpublished = resolve(testDir, 'dep-mixed-unpublished.md');

      writeFileSync(rootFile, `# Mixed Root File
[no hash](./dep-mixed-no-hash.md)
[with hash](./dep-mixed-with-hash.md)  
[unpublished](./dep-mixed-unpublished.md)`);

      writeFileSync(depNoHash, `---
telegraphUrl: https://telegra.ph/mixed-no-hash
editPath: /edit/mixed-no-hash
username: test-user
publishedAt: 2024-01-01T00:00:00.000Z
originalFilename: dep-mixed-no-hash.md
---
# Mixed - No Hash`);

      writeFileSync(depWithHash, `---
telegraphUrl: https://telegra.ph/mixed-with-hash
editPath: /edit/mixed-with-hash
username: test-user
publishedAt: 2024-01-01T00:00:00.000Z
originalFilename: dep-mixed-with-hash.md
contentHash: def456mixedhash
---
# Mixed - With Hash`);

      writeFileSync(depUnpublished, `# Mixed - Unpublished`);

      // Mock API methods
      const editWithMetadataSpy = jest.spyOn(publisher, 'editWithMetadata')
        .mockResolvedValue({ success: true, url: 'edit-url', path: 'edit-path', isNewPublication: false });
      
      const publishWithMetadataSpy = jest.spyOn(publisher, 'publishWithMetadata')
        .mockResolvedValue({ success: true, url: 'publish-url', path: 'publish-path', isNewPublication: true });

      // Execute
      const result = await publisher.publishDependencies(rootFile, 'test-user', false);

      // Verify comprehensive results
      expect(result.success).toBe(true);
      expect(result.publishedFiles).toHaveLength(2); // depNoHash + depUnpublished
      expect(result.publishedFiles).toContain(depNoHash);
      expect(result.publishedFiles).toContain(depUnpublished);
      expect(result.publishedFiles).not.toContain(depWithHash);

      // Verify correct method calls
      expect(editWithMetadataSpy).toHaveBeenCalledTimes(1);
      expect(editWithMetadataSpy).toHaveBeenCalledWith(depNoHash, 'test-user', {
        withDependencies: false,
        dryRun: false,
        forceRepublish: true,
        generateAside: true
      });

      expect(publishWithMetadataSpy).toHaveBeenCalledTimes(1);
      expect(publishWithMetadataSpy).toHaveBeenCalledWith(depUnpublished, 'test-user', {
        withDependencies: false,
        dryRun: false,
        generateAside: true
      });
    });

    it('should handle errors gracefully during backfill', async () => {
      // Setup: Create file that will fail during backfill
      const rootFile = resolve(testDir, 'root-error.md');
      const depFailingBackfill = resolve(testDir, 'dep-error.md');

      writeFileSync(rootFile, `# Error Test\n[failing dep](./dep-error.md)`);
      writeFileSync(depFailingBackfill, `---
telegraphUrl: https://telegra.ph/error-test
editPath: /edit/error-test
username: test-user
publishedAt: 2024-01-01T00:00:00.000Z
originalFilename: dep-error.md
---
# Failing Dependency`);

      // Mock editWithMetadata to fail
      const editWithMetadataSpy = jest.spyOn(publisher, 'editWithMetadata')
        .mockResolvedValue({ success: false, error: 'Network error during backfill', isNewPublication: false });

      // Execute
      const result = await publisher.publishDependencies(rootFile, 'test-user', false);

      // Verify error handling
      expect(result.success).toBe(false);
      expect(result.error).toContain('Failed to process dependency');
      expect(result.error).toContain('dep-error.md');
      expect(result.publishedFiles).toEqual([]); // No files should be in result on error

      // Verify the failed call was made
      expect(editWithMetadataSpy).toHaveBeenCalledWith(depFailingBackfill, 'test-user', {
        withDependencies: false,
        dryRun: false,
        forceRepublish: true,
        generateAside: true
      });
    });

    it('should skip files with corrupted metadata', async () => {
      // This test would require mocking MetadataManager.getPublicationStatus
      // to return METADATA_CORRUPTED status for specific files
      // Since we're testing the logic flow, we'll validate that the system
      // can handle these cases without throwing errors
      
      const rootFile = resolve(testDir, 'root-corrupted.md');
      writeFileSync(rootFile, `# Corrupted Test\nNo dependencies to avoid complexity.`);

      // Test that empty dependency list is handled correctly
      const result = await publisher.publishDependencies(rootFile, 'test-user', false);
      
      expect(result.success).toBe(true);
      expect(result.publishedFiles).toEqual([]);
    });
  });
});
```

`src/publisher/EnhancedTelegraphPublisher.ts`

```ts
import { writeFileSync } from "node:fs";
import { createHash } from "node:crypto";
import { basename, dirname, resolve } from "node:path";
import type { PagesCacheManager } from "../cache/PagesCacheManager";
import { ProgressIndicator } from "../cli/ProgressIndicator";
import { ContentProcessor } from "../content/ContentProcessor";
import { DependencyManager } from "../dependencies/DependencyManager";
import { LinkResolver } from "../links/LinkResolver";
import { convertMarkdownToTelegraphNodes } from "../markdownConverter";
import { MetadataManager } from "../metadata/MetadataManager";
import { RateLimiter } from "../ratelimiter/RateLimiter";
import { type TelegraphNode, type TelegraphPage, TelegraphPublisher } from "../telegraphPublisher";
import type {
  FileMetadata,
  MetadataConfig,
  ProcessedContent,
  PublicationProgress,
  PublicationResult,
  PublishedPageInfo
} from "../types/metadata";
import { PublicationStatus } from "../types/metadata";
import { PathResolver } from '../utils/PathResolver';

/**
 * Enhanced Telegraph publisher with metadata management and dependency resolution
 */
export class EnhancedTelegraphPublisher extends TelegraphPublisher {
  private config: MetadataConfig;
  private dependencyManager: DependencyManager;
  private cacheManager?: PagesCacheManager;
  private currentAccessToken?: string;
  private rateLimiter: RateLimiter;
  private baseCacheDirectory?: string;
  
  // Metadata cache for dependency processing
  private metadataCache = new Map<string, {
    status: PublicationStatus;
    metadata: FileMetadata | null;
    timestamp: number;
  }>();
  
  // Hash cache for content hash calculation
  private hashCache = new Map<string, { hash: string; timestamp: number }>();
  private readonly CACHE_TTL = 5000; // 5 seconds

  constructor(config: MetadataConfig) {
    super();
    this.config = config;
    this.dependencyManager = new DependencyManager(config, PathResolver.getInstance());
    this.rateLimiter = new RateLimiter(config.rateLimiting);
  }

  /**
   * Set base directory for cache files (for bulk operations)
   * @param directory Base directory where cache should be created
   */
  setBaseCacheDirectory(directory: string): void {
    this.baseCacheDirectory = directory;
  }

  /**
   * Set access token and initialize cache manager
   * @param token Access token
   */
  override setAccessToken(token: string): void {
    super.setAccessToken(token);
    this.currentAccessToken = token;
    // Initialize cache manager when access token is set
    // We'll set it up when we know the directory from the first file being processed
  }

  /**
   * Initialize cache manager for the given directory
   * @param filePath Path to file being processed
   */
  private initializeCacheManager(filePath: string): void {
    if (!this.cacheManager && this.currentAccessToken) {
      // Use base cache directory if set (for bulk operations),
      // otherwise use file's directory (for single file operations)
      const directory = this.baseCacheDirectory || dirname(filePath);
      const { PagesCacheManager } = require("../cache/PagesCacheManager");
      this.cacheManager = new PagesCacheManager(directory, this.currentAccessToken);
    }
  }

  /**
   * Add page to cache after successful publication
   * @param filePath Local file path
   * @param url Telegraph URL
   * @param path Telegraph path
   * @param title Page title
   * @param username Author username
   */
  private addToCache(filePath: string, url: string, path: string, title: string, username: string): void {
    if (this.cacheManager) {
      const pageInfo: PublishedPageInfo = {
        telegraphUrl: url,
        editPath: path,
        localFilePath: filePath,
        title: title,
        authorName: username,
        publishedAt: new Date().toISOString(),
        lastUpdated: new Date().toISOString()
      };

      this.cacheManager.addPage(pageInfo);
    }
  }

  /**
   * Calculate content hash with caching
   * @param content Content to hash
   * @returns SHA-256 hash of the content
   */
  private calculateContentHash(content: string): string {
    const cacheKey = content.substring(0, 100); // First 100 chars as key
    const cached = this.hashCache.get(cacheKey);
    
    if (cached && (Date.now() - cached.timestamp) < this.CACHE_TTL) {
      return cached.hash;
    }
    
    const hash = createHash('sha256').update(content, 'utf8').digest('hex');
    this.hashCache.set(cacheKey, { hash, timestamp: Date.now() });
    return hash;
  }

  /**
   * Publish file with metadata management and dependency resolution
   * @param filePath Path to file to publish
   * @param username Author username
   * @param options Publishing options
   * @returns Publication result
   */
  async publishWithMetadata(
    filePath: string,
    username: string,
    options: {
      withDependencies?: boolean;
      forceRepublish?: boolean;
      dryRun?: boolean;
      debug?: boolean;
      generateAside?: boolean;
    } = {}
  ): Promise<PublicationResult> {
    try {
      const { withDependencies = true, forceRepublish = false, dryRun = false, debug = false, generateAside = true } = options;

      // Initialize cache manager for this directory
      this.initializeCacheManager(filePath);

      // Check if file is already published and handle accordingly
      const publicationStatus = MetadataManager.getPublicationStatus(filePath);
      const existingMetadata = MetadataManager.getPublicationInfo(filePath);

      // Also check cache for existing publication info
      let cacheInfo: PublishedPageInfo | null = null;
      if (this.cacheManager) {
        cacheInfo = this.cacheManager.getPageByLocalPath(filePath);
      }

      // If file has metadata or exists in cache, treat as published (unless forced)
      const isPublished = publicationStatus === PublicationStatus.PUBLISHED || cacheInfo !== null;

      if (isPublished && !forceRepublish) {
        // File is already published, use edit instead
        // If we have cache info but no file metadata, we need to restore metadata to file
        if (cacheInfo && !existingMetadata) {
          console.log(`üìã Found ${filePath} in cache but missing metadata in file, restoring...`);
          
          // Calculate content hash for restored metadata
          const processed = ContentProcessor.processFile(filePath);
          const contentHash = this.calculateContentHash(processed.contentWithoutMetadata);
          
          const restoredMetadata: FileMetadata = {
            telegraphUrl: cacheInfo.telegraphUrl,
            editPath: cacheInfo.editPath,
            username: cacheInfo.authorName,
            publishedAt: cacheInfo.publishedAt,
            originalFilename: cacheInfo.localFilePath ? basename(cacheInfo.localFilePath) : basename(filePath),
            title: cacheInfo.title,
            contentHash
          };

          // Restore metadata to file
          const contentWithMetadata = ContentProcessor.injectMetadataIntoContent(processed, restoredMetadata);
          writeFileSync(filePath, contentWithMetadata, 'utf-8');

          console.log(`‚úÖ Metadata restored to ${filePath} from cache`);
        }

        return await this.editWithMetadata(filePath, username, { withDependencies, dryRun, debug, generateAside });
      }

      // Process dependencies if requested
      if (withDependencies) {
        const dependencyResult = await this.publishDependencies(filePath, username, dryRun, generateAside);
        if (!dependencyResult.success) {
          return {
            success: false,
            error: `Failed to publish dependencies: ${dependencyResult.error}`,
            isNewPublication: true
          };
        }
      }

      // Process the main file
      const processed = ContentProcessor.processFile(filePath);

      // Replace local links with Telegraph URLs if dependencies were published
      const processedWithLinks = withDependencies
        ? await this.replaceLinksWithTelegraphUrls(processed)
        : processed;

      // Validate content with relaxed rules for depth 1
      const isDepthOne = this.config.maxDependencyDepth === 1;
      const validation = ContentProcessor.validateContent(processedWithLinks, {
        allowBrokenLinks: isDepthOne,
        allowUnpublishedDependencies: isDepthOne
      });
      if (!validation.isValid) {
        return {
          success: false,
          error: `Content validation failed: ${validation.issues.join(', ')}`,
          isNewPublication: true
        };
      }

      // Prepare content for publication
      const contentForPublication = ContentProcessor.prepareForPublication(processedWithLinks);
      const title = ContentProcessor.extractTitle(processedWithLinks) || 'Untitled';

      // Convert to Telegraph nodes
      const telegraphNodes = convertMarkdownToTelegraphNodes(contentForPublication, { generateToc: generateAside });

      // Save debug JSON if requested
      if (debug && dryRun) {
        const jsonOutputPath = resolve(filePath.replace(/\.md$/, ".json"));
        try {
          writeFileSync(jsonOutputPath, JSON.stringify(telegraphNodes, null, 2), 'utf-8');
          ProgressIndicator.showStatus(`üíæ Debug JSON saved to: ${jsonOutputPath}`, 'info');
        } catch (error) {
          ProgressIndicator.showStatus(`‚ùå Failed to save debug JSON: ${error instanceof Error ? error.message : String(error)}`, 'error');
        }
      }

      if (dryRun) {
        return {
          success: true,
          url: `[DRY RUN] Would publish: ${filePath}`,
          path: `[DRY RUN] New page path`,
          isNewPublication: true
        };
      }

      // Create new page
      const page = await this.publishNodes(title, telegraphNodes);

      // Create metadata - preserve original title from metadata if it exists
      const originalTitle = processed.metadata?.title;
      const metadataTitle = originalTitle || title;
      
      // Calculate content hash for new publication
      const contentHash = this.calculateContentHash(processedWithLinks.contentWithoutMetadata);
      
      const metadata = MetadataManager.createMetadata(
        page.url,
        page.path,
        username,
        filePath,
        contentHash,
        metadataTitle
      );

      // Inject metadata into file
      const contentWithMetadata = ContentProcessor.injectMetadataIntoContent(processed, metadata);
      writeFileSync(filePath, contentWithMetadata, 'utf-8');

      // Add to cache after successful publication
      this.addToCache(filePath, page.url, page.path, metadataTitle, username);

      return {
        success: true,
        url: page.url,
        path: page.path,
        isNewPublication: true,
        metadata
      };

    } catch (error) {
      console.error(`Error publishing file ${filePath}:`, error);
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
        isNewPublication: true
      };
    }
  }

  /**
   * Edit existing published file with metadata management
   * @param filePath Path to file to edit
   * @param username Author username
   * @param options Edit options
   * @returns Publication result
   */
  async editWithMetadata(
    filePath: string,
    username: string,
    options: {
      withDependencies?: boolean;
      dryRun?: boolean;
      debug?: boolean;
      forceRepublish?: boolean;
      generateAside?: boolean;
    } = {}
  ): Promise<PublicationResult> {
    try {
      const { withDependencies = true, dryRun = false, debug = false, generateAside = true } = options;

      // Initialize cache manager for this directory
      this.initializeCacheManager(filePath);

      // Get existing metadata
      const existingMetadata = MetadataManager.getPublicationInfo(filePath);
      if (!existingMetadata) {
        return {
          success: false,
          error: 'File is not published or metadata is corrupted',
          isNewPublication: false
        };
      }

      // Process dependencies if requested
      if (withDependencies) {
        const dependencyResult = await this.publishDependencies(filePath, username, dryRun, generateAside);
        if (!dependencyResult.success) {
          return {
            success: false,
            error: `Failed to publish dependencies: ${dependencyResult.error}`,
            isNewPublication: false
          };
        }
      }

      // Process the main file
      const processed = ContentProcessor.processFile(filePath);

      // NEW: Content change detection (skip when debug mode is enabled)
      if (!options.forceRepublish && !debug) {
        const currentHash = this.calculateContentHash(processed.contentWithoutMetadata);
        
        if (existingMetadata.contentHash && existingMetadata.contentHash === currentHash) {
          ProgressIndicator.showStatus(
            `üìÑ Content unchanged. Skipping publication of ${basename(filePath)}.`, 
            "info"
          );
          
          return {
            success: true,
            url: existingMetadata.telegraphUrl,
            path: existingMetadata.editPath,
            isNewPublication: false,
            metadata: existingMetadata
          };
        }
      }

      // Replace local links with Telegraph URLs if dependencies were published
      const processedWithLinks = withDependencies
        ? await this.replaceLinksWithTelegraphUrls(processed)
        : processed;

      // Validate content with relaxed rules for depth 1
      const isDepthOne = this.config.maxDependencyDepth === 1;
      const validation = ContentProcessor.validateContent(processedWithLinks, {
        allowBrokenLinks: isDepthOne,
        allowUnpublishedDependencies: isDepthOne
      });
      if (!validation.isValid) {
        return {
          success: false,
          error: `Content validation failed: ${validation.issues.join(', ')}`,
          isNewPublication: false
        };
      }

      // Prepare content for publication
      const contentForPublication = ContentProcessor.prepareForPublication(processedWithLinks);
      const title = ContentProcessor.extractTitle(processedWithLinks) || existingMetadata.title || 'Untitled';

      // Convert to Telegraph nodes
      const telegraphNodes = convertMarkdownToTelegraphNodes(contentForPublication, { generateToc: generateAside });

      // Save debug JSON if requested
      if (debug && dryRun) {
        const jsonOutputPath = resolve(filePath.replace(/\.md$/, ".json"));
        try {
          writeFileSync(jsonOutputPath, JSON.stringify(telegraphNodes, null, 2), 'utf-8');
          ProgressIndicator.showStatus(`üíæ Debug JSON saved to: ${jsonOutputPath}`, 'info');
        } catch (error) {
          ProgressIndicator.showStatus(`‚ùå Failed to save debug JSON: ${error instanceof Error ? error.message : String(error)}`, 'error');
        }
      }

      if (dryRun) {
        return {
          success: true,
          url: existingMetadata.telegraphUrl,
          path: existingMetadata.editPath,
          isNewPublication: false,
          metadata: existingMetadata
        };
      }

      // Edit existing page
      const page = await this.editPage(existingMetadata.editPath, title, telegraphNodes, username);

      // Update metadata with new timestamp and content hash - preserve original title from metadata if it exists
      const originalTitle = processed.metadata?.title;
      const metadataTitle = originalTitle || title;
      
      // Calculate updated content hash after successful publication
      const updatedContentHash = this.calculateContentHash(processed.contentWithoutMetadata);
      
      const updatedMetadata: FileMetadata = {
        ...existingMetadata,
        publishedAt: new Date().toISOString(),
        title: metadataTitle,
        contentHash: updatedContentHash
      };

      // Update metadata in file
      const contentWithMetadata = ContentProcessor.injectMetadataIntoContent(processed, updatedMetadata);
      writeFileSync(filePath, contentWithMetadata, 'utf-8');

      // Update cache after successful edit
      if (this.cacheManager) {
        this.cacheManager.updatePage(page.url, {
          title: metadataTitle,
          authorName: username,
          lastUpdated: new Date().toISOString()
        });
      }

      return {
        success: true,
        url: page.url,
        path: page.path,
        isNewPublication: false,
        metadata: updatedMetadata
      };

    } catch (error) {
      console.error(`Error editing file ${filePath}:`, error);
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
        isNewPublication: false
      };
    }
  }

  /**
   * Publish file dependencies recursively
   * @param filePath Root file path
   * @param username Author username
   * @param dryRun Whether to perform dry run
   * @returns Success status and any errors
   */
  async publishDependencies(
    filePath: string,
    username: string,
    dryRun: boolean = false,
    generateAside: boolean = true
  ): Promise<{ success: boolean; error?: string; publishedFiles?: string[] }> {
    try {
      // Build dependency tree
      const dependencyTree = this.dependencyManager.buildDependencyTree(filePath);

      // Analyze dependencies
      const analysis = this.dependencyManager.analyzeDependencyTree(dependencyTree);

      // Check for circular dependencies
      if (analysis.circularDependencies.length > 0) {
        console.warn('Circular dependencies detected:', analysis.circularDependencies);
        // Continue with publishing, but log the warning
      }

      // Initialize processing state
      const publishedFiles: string[] = [];
      const stats = this.initializeStatsTracking(analysis, filePath);
      
      // Clear metadata cache at start of operation
      this.clearMetadataCache();

      // Show initial progress
      if (stats.totalFiles > 0) {
        ProgressIndicator.showStatus(
          `üîÑ Processing ${stats.totalFiles} dependencies...`, 
          "info"
        );
      } else {
        return { success: true, publishedFiles: [] };
      }

      // Process all files with status-based handling
      for (const fileToProcess of analysis.publishOrder) {
        if (fileToProcess === filePath) continue; // Skip root file
        
        try {
          await this.processFileByStatus(fileToProcess, username, dryRun, publishedFiles, stats, generateAside);
          stats.processedFiles++;
        } catch (error) {
          // Clear cache on error
          this.clearMetadataCache();
          const errorMessage = error instanceof Error ? error.message : String(error);
          return {
            success: false,
            error: `Failed to process dependency ${basename(fileToProcess)}: ${errorMessage}`,
            publishedFiles
          };
        }
      }

      // Clear metadata cache after operation
      this.clearMetadataCache();

      // Report final results
      this.reportProcessingResults(stats, dryRun);
      
      return { success: true, publishedFiles };

    } catch (error) {
      // Clear cache on error
      this.clearMetadataCache();
      console.error(`Error publishing dependencies for ${filePath}:`, error);
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error)
      };
    }
  }

  /**
   * Replace local links in processed content with Telegraph URLs
   * @param processed Processed content
   * @param basePath Base file path for resolving links
   * @returns Content with replaced links
   */
  private async replaceLinksWithTelegraphUrls(
    processed: ProcessedContent,
  ): Promise<ProcessedContent> {
    const linkMappings = new Map<string, string>();

    // Get unique file paths from local links
    const markdownLinks = LinkResolver.filterMarkdownLinks(processed.localLinks);
    const uniquePaths = LinkResolver.getUniqueFilePaths(markdownLinks);

    // Get Telegraph URLs for published files
    for (const filePath of uniquePaths) {
      const metadata = MetadataManager.getPublicationInfo(filePath);
      if (metadata) {
        linkMappings.set(filePath, metadata.telegraphUrl);
      }
    }

    // Replace links in content
    return ContentProcessor.replaceLinksInContent(processed, linkMappings);
  }

  /**
   * Override publishNodes with rate limiting
   * @param title Page title
   * @param nodes Telegraph nodes
   * @returns Published page
   */
  override async publishNodes(
    title: string,
    nodes: TelegraphNode[],
  ): Promise<TelegraphPage> {
    // Apply rate limiting before API call
    await this.rateLimiter.beforeCall();

    try {
      // Call parent implementation
      const result = await super.publishNodes(title, nodes);

      // Mark successful call
      this.rateLimiter.markSuccessfulCall();

      return result;
    } catch (error) {
      // Check if this is a FLOOD_WAIT error
      if (error instanceof Error && error.message.includes('FLOOD_WAIT_')) {
        const waitMatch = error.message.match(/FLOOD_WAIT_(\d+)/);
        if (waitMatch?.[1]) {
          const waitSeconds = parseInt(waitMatch[1], 10);
          console.warn(`üö¶ Rate limited: waiting ${waitSeconds}s before retry...`);

          // Handle FLOOD_WAIT with our rate limiter
          await this.rateLimiter.handleFloodWait(waitSeconds);

          // Retry the call
          return await super.publishNodes(title, nodes);
        }
      }

      // Re-throw non-FLOOD_WAIT errors
      throw error;
    }
  }

  /**
   * Override editPage with rate limiting
   * @param path Page path
   * @param title Page title
   * @param nodes Telegraph nodes
   * @param authorName Author name
   * @param authorUrl Author URL
   * @returns Updated page
   */
  override async editPage(
    path: string,
    title: string,
    nodes: TelegraphNode[],
    authorName?: string,
    authorUrl?: string,
  ): Promise<TelegraphPage> {
    // Apply rate limiting before API call
    await this.rateLimiter.beforeCall();

    try {
      // Call parent implementation
      const result = await super.editPage(path, title, nodes, authorName, authorUrl);

      // Mark successful call
      this.rateLimiter.markSuccessfulCall();

      return result;
    } catch (error) {
      // Check if this is a FLOOD_WAIT error
      if (error instanceof Error && error.message.includes('FLOOD_WAIT_')) {
        const waitMatch = error.message.match(/FLOOD_WAIT_(\d+)/);
        if (waitMatch?.[1]) {
          const waitSeconds = parseInt(waitMatch[1], 10);
          console.warn(`üö¶ Rate limited: waiting ${waitSeconds}s before retry...`);

          // Handle FLOOD_WAIT with our rate limiter
          await this.rateLimiter.handleFloodWait(waitSeconds);

          // Retry the call
          return await super.editPage(path, title, nodes, authorName, authorUrl);
        }
      }

      // Re-throw non-FLOOD_WAIT errors
      throw error;
    }
  }

  /**
   * Get rate limiting metrics
   * @returns Current rate limiting metrics
   */
  getRateLimitingMetrics(): string {
    return this.rateLimiter.formatMetrics();
  }

  /**
   * Get publication progress for batch operations
   * @param filePaths Array of file paths to process
   * @returns Publication progress information
   */
  getPublicationProgress(filePaths: string[]): PublicationProgress {
    let processedFiles = 0;
    let successfulPublications = 0;
    let failedPublications = 0;

    for (const filePath of filePaths) {
      if (this.dependencyManager.isProcessed(filePath)) {
        processedFiles++;
        const status = MetadataManager.getPublicationStatus(filePath);
        if (status === PublicationStatus.PUBLISHED) {
          successfulPublications++;
        } else {
          failedPublications++;
        }
      }
    }

    return {
      totalFiles: filePaths.length,
      processedFiles,
      successfulPublications,
      failedPublications,
      progressPercentage: Math.round((processedFiles / filePaths.length) * 100)
    };
  }

  /**
   * Reset dependency manager state
   */
  resetState(): void {
    this.dependencyManager.reset();
  }

  /**
   * Update configuration
   * @param config New configuration
   */
  updateConfig(config: Partial<MetadataConfig>): void {
    this.config = { ...this.config, ...config };
    this.dependencyManager = new DependencyManager(this.config, PathResolver.getInstance());

    // Update rate limiter configuration if provided
    if (config.rateLimiting) {
      this.rateLimiter.updateConfig(config.rateLimiting);
    }
  }

  /**
   * Calculates SHA-256 hash of content for change detection.
   * Uses content excluding YAML front-matter for precise change detection.
   * @param content The processed content without metadata
   * @returns Hex-encoded SHA-256 hash
   */
  private calculateContentHash(content: string): string {
    try {
      return createHash('sha256').update(content, 'utf8').digest('hex');
    } catch (error) {
      console.warn('Content hash calculation failed:', error);
      ProgressIndicator.showStatus(
        `‚ö†Ô∏è Content hash calculation failed. Proceeding with publication.`, 
        "warning"
      );
      // Return empty string to trigger publication (fail-safe behavior)
      return '';
    }
  }

  /**
   * Initialize statistics tracking for dependency processing
   * @param analysis Dependency analysis results
   * @param rootFilePath Root file path to exclude from count
   * @returns Statistics tracking object
   */
  private initializeStatsTracking(analysis: any, rootFilePath: string) {
    const totalFiles = analysis.publishOrder.filter((file: string) => file !== rootFilePath).length;
    return {
      totalFiles,
      processedFiles: 0,
      backfilledFiles: 0,
      skippedFiles: 0,
      warningFiles: 0,
      unpublishedFiles: 0
    };
  }

  /**
   * Clear metadata cache
   */
  private clearMetadataCache(): void {
    this.metadataCache.clear();
  }

  /**
   * Get cached metadata for a file with smart caching
   * @param filePath File path to get metadata for
   * @returns Cached metadata result
   */
  private getCachedMetadata(filePath: string) {
    const cached = this.metadataCache.get(filePath);
    if (cached && (Date.now() - cached.timestamp) < 5000) { // 5 second TTL
      return cached;
    }
    
    const status = MetadataManager.getPublicationStatus(filePath);
    const metadata = status === PublicationStatus.PUBLISHED 
      ? MetadataManager.getPublicationInfo(filePath) 
      : null;
      
    const result = { status, metadata, timestamp: Date.now() };
    this.metadataCache.set(filePath, result);
    return result;
  }

  /**
   * Process a file based on its publication status
   * @param fileToProcess File path to process
   * @param username Username for publishing
   * @param dryRun Whether to perform dry run
   * @param publishedFiles Array to track published files
   * @param stats Statistics tracking object
   */
  private async processFileByStatus(
    fileToProcess: string,
    username: string,
    dryRun: boolean,
    publishedFiles: string[],
    stats: any,
    generateAside: boolean = true
  ): Promise<void> {
    const { status, metadata } = this.getCachedMetadata(fileToProcess);
    
    switch (status) {
      case PublicationStatus.NOT_PUBLISHED:
        await this.handleUnpublishedFile(fileToProcess, username, dryRun, publishedFiles, stats, generateAside);
        break;
        
      case PublicationStatus.PUBLISHED:
        await this.handlePublishedFile(fileToProcess, username, dryRun, publishedFiles, stats, metadata, generateAside);
        break;
        
      case PublicationStatus.METADATA_CORRUPTED:
      case PublicationStatus.METADATA_MISSING:
        await this.handleCorruptedMetadata(fileToProcess, status, stats);
        break;
        
      default:
        this.logUnknownStatus(fileToProcess, status);
        stats.warningFiles++;
    }
  }

  /**
   * Handle unpublished file (existing logic)
   * @param filePath File path to publish
   * @param username Username for publishing
   * @param dryRun Whether to perform dry run
   * @param publishedFiles Array to track published files
   * @param stats Statistics tracking object
   */
  private async handleUnpublishedFile(
    filePath: string,
    username: string,
    dryRun: boolean,
    publishedFiles: string[],
    stats: any,
    generateAside: boolean = true
  ): Promise<void> {
    if (dryRun) {
      ProgressIndicator.showStatus(`üîç DRY-RUN: Would publish '${basename(filePath)}'`, "info");
    } else {
      ProgressIndicator.showStatus(`üìÑ Publishing '${basename(filePath)}'...`, "info");
    }

    const result = await this.publishWithMetadata(filePath, username, {
      withDependencies: false, // Avoid infinite recursion
      dryRun,
      generateAside
    });

    if (result.success) {
      publishedFiles.push(filePath);
      stats.unpublishedFiles++;
      this.dependencyManager.markAsProcessed(filePath);
    } else {
      throw new Error(`Failed to publish dependency ${filePath}: ${result.error}`);
    }
  }

  /**
   * Handle published file with potential content hash backfilling
   * @param filePath File path to check/update
   * @param username Username for publishing
   * @param dryRun Whether to perform dry run
   * @param publishedFiles Array to track published files
   * @param stats Statistics tracking object
   * @param metadata File metadata
   */
  private async handlePublishedFile(
    filePath: string,
    username: string,
    dryRun: boolean,
    publishedFiles: string[],
    stats: any,
    metadata: FileMetadata | null,
    generateAside: boolean = true
  ): Promise<void> {
    if (metadata && !metadata.contentHash) {
      // File is published but missing contentHash - backfill it
      if (dryRun) {
        ProgressIndicator.showStatus(`üîç DRY-RUN: Would backfill content hash for '${basename(filePath)}'`, "info");
      } else {
        ProgressIndicator.showStatus(`üìù Updating '${basename(filePath)}' to add content hash...`, "info");
      }
      
      // Force an edit operation to backfill the content hash
      const result = await this.editWithMetadata(filePath, username, {
        withDependencies: false,
        dryRun,
        forceRepublish: true, // Use force to bypass the normal hash check
        generateAside
      });

      if (result.success) {
        publishedFiles.push(filePath); // Consider it "published" in this run
        stats.backfilledFiles++;
      } else {
        throw new Error(`Failed to update dependency ${filePath} with hash: ${result.error}`);
      }
    } else {
      // File already has contentHash or metadata is corrupted - skip
      ProgressIndicator.showStatus(`‚è≠Ô∏è Skipping '${basename(filePath)}' (content hash already present)`, "info");
      stats.skippedFiles++;
    }
  }

  /**
   * Handle files with corrupted or missing metadata
   * @param filePath File path with metadata issues
   * @param status Publication status
   * @param stats Statistics tracking object
   */
  private async handleCorruptedMetadata(
    filePath: string,
    status: PublicationStatus,
    stats: any
  ): Promise<void> {
    const statusText = status === PublicationStatus.METADATA_CORRUPTED ? 'corrupted' : 'missing';
    ProgressIndicator.showStatus(
      `‚ö†Ô∏è Skipping '${basename(filePath)}' due to ${statusText} metadata`, 
      "warning"
    );
    stats.warningFiles++;
  }

  /**
   * Log warning for unknown publication status
   * @param filePath File path with unknown status
   * @param status Unknown status
   */
  private logUnknownStatus(filePath: string, status: PublicationStatus): void {
    console.warn(`Unknown publication status '${status}' for file: ${filePath}`);
    ProgressIndicator.showStatus(
      `‚ö†Ô∏è Unknown status for '${basename(filePath)}': ${status}`, 
      "warning"
    );
  }

  /**
   * Report final processing results
   * @param stats Statistics tracking object
   * @param dryRun Whether this was a dry run
   */
  private reportProcessingResults(stats: any, dryRun: boolean): void {
    const { totalFiles, backfilledFiles, skippedFiles, warningFiles, unpublishedFiles } = stats;
    
    if (dryRun) {
      ProgressIndicator.showStatus(
        `üîç DRY-RUN COMPLETE: ${totalFiles} dependencies analyzed`, 
        "info"
      );
      if (backfilledFiles > 0) {
        ProgressIndicator.showStatus(
          `üìù Would backfill content hash for ${backfilledFiles} dependencies`, 
          "info"
        );
      }
      if (unpublishedFiles > 0) {
        ProgressIndicator.showStatus(
          `üìÑ Would publish ${unpublishedFiles} new dependencies`, 
          "info"
        );
      }
    } else {
      if (backfilledFiles > 0) {
        ProgressIndicator.showStatus(
          `‚úÖ Successfully backfilled content hash for ${backfilledFiles} dependencies`, 
          "success"
        );
      }
      if (unpublishedFiles > 0) {
        ProgressIndicator.showStatus(
          `‚úÖ Successfully published ${unpublishedFiles} new dependencies`, 
          "success"
        );
      }
      if (skippedFiles > 0) {
        ProgressIndicator.showStatus(
          `‚è≠Ô∏è Skipped ${skippedFiles} dependencies (already have content hash)`, 
          "info"
        );
      }
    }
    
    if (warningFiles > 0) {
      ProgressIndicator.showStatus(
        `‚ö†Ô∏è Completed with ${warningFiles} warnings - check logs for details`, 
        "warning"
      );
    }
  }
}
```

`src/ratelimiter/CountdownTimer.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, test } from "bun:test";
import { type CountdownOptions, CountdownTimer } from "./CountdownTimer";

describe("CountdownTimer", () => {
  let timer: CountdownTimer;
  let updateCallbacks: Array<{ remaining: string; progress: number; progressBar?: string }>;
  let completeCallback: () => void;
  let completeCallCount: number;

  beforeEach(() => {
    updateCallbacks = [];
    completeCallCount = 0;
    completeCallback = () => {
      completeCallCount++;
    };
  });

  afterEach(() => {
    if (timer && timer.isActive()) {
      timer.stop();
    }
  });

  describe("constructor", () => {
    test("should create timer with default options", () => {
      timer = new CountdownTimer(5000);
      expect(timer).toBeDefined();
      expect(timer.isActive()).toBe(false);
    });

    test("should create timer with custom options", () => {
      const options: CountdownOptions = {
        updateIntervalMs: 500,
        showProgressBar: false,
        formatLong: true
      };
      timer = new CountdownTimer(10000, options);
      expect(timer).toBeDefined();
    });
  });

  describe("callback setup", () => {
    beforeEach(() => {
      timer = new CountdownTimer(3000);
    });

    test("should set update callback", () => {
      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      expect(updateCallbacks).toHaveLength(0);
    });

    test("should set complete callback", () => {
      timer.onComplete(completeCallback);
      expect(completeCallCount).toBe(0);
    });
  });

  describe("timer execution", () => {
    test("should complete short timer", async () => {
      timer = new CountdownTimer(100); // 100ms timer

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      await timer.start();

      expect(completeCallCount).toBe(1);
      expect(timer.isActive()).toBe(false);
      expect(updateCallbacks.length).toBeGreaterThan(0);
    });

    test("should handle timer stop", () => {
      timer = new CountdownTimer(5000); // 5 second timer

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      // Start timer
      timer.start();

      expect(timer.isActive()).toBe(true);

      // Stop timer
      timer.stop();

      expect(timer.isActive()).toBe(false);
      // Complete callback should not be called when manually stopped
      expect(completeCallCount).toBe(0);
    });

    test("should throw error when starting already running timer", () => {
      timer = new CountdownTimer(1000);

      timer.onUpdate(() => { });

      // Start timer
      timer.start();

      // Try to start again - should throw
      expect(() => timer.start()).toThrow("Countdown timer is already running");

      timer.stop();
    });
  });

  describe("progress calculation", () => {
    test("should calculate progress correctly", async () => {
      timer = new CountdownTimer(1000); // 1 second timer

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });

        // Progress should be between 0 and 100
        expect(progress).toBeGreaterThanOrEqual(0);
        expect(progress).toBeLessThanOrEqual(100);

        // Progress should increase over time
        if (updateCallbacks.length > 1) {
          const previousUpdate = updateCallbacks[updateCallbacks.length - 2];
          if (previousUpdate) {
            expect(progress).toBeGreaterThanOrEqual(previousUpdate.progress);
          }
        }
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      await timer.start();

      expect(updateCallbacks.length).toBeGreaterThan(0);
    });

    test("should include progress bar when enabled", async () => {
      timer = new CountdownTimer(200, { showProgressBar: true });

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });

        // Progress bar should be present and properly formatted
        expect(progressBar).toBeDefined();
        expect(progressBar).toMatch(/^\[‚ñà*‚ñì*\]$/);
        expect(progressBar!.length).toBe(22); // [20 chars]
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      await timer.start();

      expect(updateCallbacks.length).toBeGreaterThan(0);
    });

    test("should not include progress bar when disabled", async () => {
      timer = new CountdownTimer(200, { showProgressBar: false });

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });

        // Progress bar should not be present
        expect(progressBar).toBeUndefined();
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      await timer.start();

      expect(updateCallbacks.length).toBeGreaterThan(0);
    });
  });

  describe("time formatting", () => {
    test("should format short durations correctly", async () => {
      timer = new CountdownTimer(30000); // 30 seconds

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });

        // Should use MM:SS format for durations under 1 hour
        expect(remaining).toMatch(/^\d{2}:\d{2}$/);

        // Stop after first update to avoid timeout
        timer.stop();
      };

      timer.onUpdate(updateCallback);

      await timer.start();

      expect(updateCallbacks.length).toBeGreaterThan(0);
    });

    test("should format long durations correctly with formatLong option", async () => {
      timer = new CountdownTimer(30000, { formatLong: true }); // 30 seconds with long format

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });

        // Should use HH:MM:SS format when formatLong is true
        expect(remaining).toMatch(/^\d{2}:\d{2}:\d{2}$/);

        // Stop after first update to avoid timeout
        timer.stop();
      };

      timer.onUpdate(updateCallback);

      await timer.start();

      expect(updateCallbacks.length).toBeGreaterThan(0);
    });
  });

  describe("static utility methods", () => {
    test("should format time correctly with formatTime static method", () => {
      // Test various durations
      expect(CountdownTimer.formatTime(30000)).toBe("00:30"); // 30 seconds
      expect(CountdownTimer.formatTime(90000)).toBe("01:30"); // 90 seconds
      expect(CountdownTimer.formatTime(3600000)).toBe("01:00:00"); // 1 hour
      expect(CountdownTimer.formatTime(7200000)).toBe("02:00:00"); // 2 hours

      // Test with long format
      expect(CountdownTimer.formatTime(30000, true)).toBe("00:00:30"); // 30 seconds
      expect(CountdownTimer.formatTime(90000, true)).toBe("00:01:30"); // 90 seconds
    });

    test("should generate progress bar correctly with generateProgressBar static method", () => {
      // Test various progress values
      expect(CountdownTimer.generateProgressBar(0)).toBe("[‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì]");
      expect(CountdownTimer.generateProgressBar(50)).toBe("[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì]");
      expect(CountdownTimer.generateProgressBar(100)).toBe("[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]");

      // Test with custom length
      expect(CountdownTimer.generateProgressBar(50, 10)).toBe("[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñì‚ñì‚ñì‚ñì‚ñì]");
      expect(CountdownTimer.generateProgressBar(25, 8)).toBe("[‚ñà‚ñà‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì]");
    });
  });

  describe("edge cases", () => {
    test("should handle zero duration", async () => {
      timer = new CountdownTimer(0);

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      await timer.start();

      expect(completeCallCount).toBe(1);
      expect(timer.isActive()).toBe(false);
    });

    test("should handle negative duration", async () => {
      timer = new CountdownTimer(-1000);

      const updateCallback = (remaining: string, progress: number, progressBar?: string) => {
        updateCallbacks.push({ remaining, progress, progressBar });
      };

      timer.onUpdate(updateCallback);
      timer.onComplete(completeCallback);

      await timer.start();

      expect(completeCallCount).toBe(1);
      expect(timer.isActive()).toBe(false);
    });

    test("should work without callbacks", async () => {
      timer = new CountdownTimer(100);

      // Should not throw error even without callbacks
      await timer.start();
      expect(timer.isActive()).toBe(false);
    });
  });
});
```

`src/ratelimiter/CountdownTimer.ts`

```ts
/**
 * Options for countdown timer configuration
 */
export interface CountdownOptions {
  /** Update interval in milliseconds (default: 1000) */
  updateIntervalMs?: number;
  /** Whether to show progress bar (default: true) */
  showProgressBar?: boolean;
  /** Use long format for time display (default: false) */
  formatLong?: boolean;
}

/**
 * Callback for countdown updates
 */
export type CountdownUpdateCallback = (remaining: string, progress: number, progressBar?: string) => void;

/**
 * Callback for countdown completion
 */
export type CountdownCompleteCallback = () => void;

/**
 * Precision countdown timer with drift correction and progress visualization
 */
export class CountdownTimer {
  private durationMs: number;
  private options: Required<CountdownOptions>;
  private startTime: number = 0;
  private endTime: number = 0;
  private intervalId: NodeJS.Timeout | null = null;
  private updateCallback: CountdownUpdateCallback | null = null;
  private completeCallback: CountdownCompleteCallback | null = null;
  private isRunning: boolean = false;

  constructor(durationMs: number, options: CountdownOptions = {}) {
    this.durationMs = durationMs;
    this.options = {
      updateIntervalMs: options.updateIntervalMs ?? 1000,
      showProgressBar: options.showProgressBar ?? true,
      formatLong: options.formatLong ?? false
    };
  }

  /**
   * Set update callback
   * @param callback Function called on each countdown update
   */
  onUpdate(callback: CountdownUpdateCallback): void {
    this.updateCallback = callback;
  }

  /**
   * Set completion callback
   * @param callback Function called when countdown completes
   */
  onComplete(callback: CountdownCompleteCallback): void {
    this.completeCallback = callback;
  }

  /**
   * Start the countdown timer
   * @returns Promise that resolves when countdown completes
   */
  async start(): Promise<void> {
    if (this.isRunning) {
      throw new Error('Countdown timer is already running');
    }

    this.isRunning = true;
    this.startTime = Date.now();
    this.endTime = this.startTime + this.durationMs;

    // Initial update
    this.performUpdate();

    return new Promise<void>((resolve) => {
      const interval = () => {
        if (!this.isRunning) {
          resolve();
          return;
        }

        const now = Date.now();
        const remaining = Math.max(0, this.endTime - now);

        if (remaining <= 0) {
          // Countdown completed
          this.stop();
          this.completeCallback?.();
          resolve();
          return;
        }

        // Perform update
        this.performUpdate();

        // Schedule next update with drift correction
        const nextUpdateTime = Math.floor(now / this.options.updateIntervalMs) * this.options.updateIntervalMs + this.options.updateIntervalMs;
        const delay = Math.max(0, nextUpdateTime - Date.now());

        this.intervalId = setTimeout(interval, delay);
      };

      // Schedule first update
      const now = Date.now();
      const nextUpdateTime = Math.floor(now / this.options.updateIntervalMs) * this.options.updateIntervalMs + this.options.updateIntervalMs;
      const delay = Math.max(0, nextUpdateTime - Date.now());

      this.intervalId = setTimeout(interval, delay);
    });
  }

  /**
   * Stop the countdown timer
   */
  stop(): void {
    this.isRunning = false;
    if (this.intervalId) {
      clearTimeout(this.intervalId);
      this.intervalId = null;
    }
  }

  /**
   * Check if timer is currently running
   */
  isActive(): boolean {
    return this.isRunning;
  }

  /**
   * Perform countdown update calculation and callback
   */
  private performUpdate(): void {
    if (!this.updateCallback) {
      return;
    }

    const now = Date.now();
    const remaining = Math.max(0, this.endTime - now);
    const elapsed = this.durationMs - remaining;
    const progress = Math.min(100, Math.max(0, (elapsed / this.durationMs) * 100));

    const timeString = this.formatTime(remaining);
    const progressBar = this.options.showProgressBar ? this.generateProgressBar(progress) : undefined;

    this.updateCallback(timeString, progress, progressBar);
  }

  /**
   * Format time in mm:ss or hh:mm:ss format
   * @param ms Milliseconds to format
   * @returns Formatted time string
   */
  private formatTime(ms: number): string {
    const totalSeconds = Math.ceil(ms / 1000);
    const hours = Math.floor(totalSeconds / 3600);
    const minutes = Math.floor((totalSeconds % 3600) / 60);
    const seconds = totalSeconds % 60;

    if (this.options.formatLong || hours > 0) {
      // Use HH:MM:SS format
      return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    } else {
      // Use MM:SS format
      return `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    }
  }

  /**
   * Generate progress bar visualization
   * @param progress Progress percentage (0-100)
   * @returns Progress bar string
   */
  private generateProgressBar(progress: number): string {
    const barLength = 20;
    const filled = Math.floor((progress / 100) * barLength);
    const empty = barLength - filled;

    const filledChar = '‚ñà';
    const emptyChar = '‚ñì';

    return `[${filledChar.repeat(filled)}${emptyChar.repeat(empty)}]`;
  }

  /**
   * Static utility: Format time string from milliseconds
   * @param ms Milliseconds to format
   * @param longFormat Use HH:MM:SS format
   * @returns Formatted time string
   */
  static formatTime(ms: number, longFormat: boolean = false): string {
    const totalSeconds = Math.ceil(ms / 1000);
    const hours = Math.floor(totalSeconds / 3600);
    const minutes = Math.floor((totalSeconds % 3600) / 60);
    const seconds = totalSeconds % 60;

    if (longFormat || hours > 0) {
      return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    } else {
      return `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    }
  }

  /**
   * Static utility: Generate progress bar
   * @param progress Progress percentage (0-100)
   * @param length Bar length in characters (default: 20)
   * @returns Progress bar string
   */
  static generateProgressBar(progress: number, length: number = 20): string {
    const filled = Math.floor((progress / 100) * length);
    const empty = length - filled;

    const filledChar = '‚ñà';
    const emptyChar = '‚ñì';

    return `[${filledChar.repeat(filled)}${emptyChar.repeat(empty)}]`;
  }
}
```

`src/ratelimiter/demo-countdown.ts`

```ts
#!/usr/bin/env bun

/**
 * Demo script to test countdown functionality
 * Run with: bun src/ratelimiter/demo-countdown.ts
 */

import { CountdownTimer } from "./CountdownTimer";
import { RateLimiter } from "./RateLimiter";

async function demoCountdownTimer() {
  console.log("üß™ Demo: CountdownTimer standalone");
  console.log("=====================================");

  const timer = new CountdownTimer(5000); // 5 seconds

  timer.onUpdate((remaining, progress, progressBar) => {
    const progressPercent = Math.round(progress);
    const line = `‚è≥ Remaining: ${remaining} ${progressBar || ''} ${progressPercent}%`;
    process.stdout.write(`\r${line.padEnd(80)}`);
  });

  timer.onComplete(() => {
    process.stdout.write('\r‚úÖ Countdown completed!'.padEnd(80) + '\n\n');
  });

  await timer.start();
}

async function demoRateLimiter() {
  console.log("üß™ Demo: RateLimiter with FLOOD_WAIT countdown");
  console.log("==============================================");

  const config = {
    baseDelayMs: 1000,
    maxDelayMs: 10000,
    adaptiveMultiplier: 2.0,
    backoffStrategy: "linear" as const,
    maxRetries: 3,
    cooldownPeriodMs: 60000,
    enableAdaptiveThrottling: true
  };

  const rateLimiter = new RateLimiter(config);

  console.log("Simulating FLOOD_WAIT error with 3 seconds wait...");
  console.log("üö¶ Rate limited: waiting 3s before retry...");

  await rateLimiter.handleFloodWait(3); // 3 seconds with countdown

  console.log("Rate limiting demonstration completed!\n");

  // Show metrics
  console.log(rateLimiter.formatMetrics());
}

async function demoComparison() {
  console.log("üß™ Demo: Countdown vs No Countdown comparison");
  console.log("=============================================");

  const config = {
    baseDelayMs: 1000,
    maxDelayMs: 10000,
    adaptiveMultiplier: 2.0,
    backoffStrategy: "linear" as const,
    maxRetries: 3,
    cooldownPeriodMs: 60000,
    enableAdaptiveThrottling: true
  };

  const rateLimiter = new RateLimiter(config);

  console.log("1. Without countdown (silent wait):");
  const start1 = Date.now();
  await rateLimiter.handleFloodWait(2, false); // 2 seconds without countdown
  const elapsed1 = Date.now() - start1;
  console.log(`   Completed in ${elapsed1}ms (silent)\n`);

  console.log("2. With countdown (visual feedback):");
  console.log("üö¶ Rate limited: waiting 2s before retry...");
  const start2 = Date.now();
  await rateLimiter.handleFloodWait(2, true); // 2 seconds with countdown
  const elapsed2 = Date.now() - start2;
  console.log(`   Completed in ${elapsed2}ms (with countdown)\n`);
}

async function main() {
  console.log("üéØ Telegraph Publisher - Rate Limit Countdown Demo");
  console.log("==================================================\n");

  try {
    // Demo 1: Standalone CountdownTimer
    await demoCountdownTimer();

    // Demo 2: RateLimiter with countdown
    await demoRateLimiter();

    // Demo 3: Comparison
    await demoComparison();

    console.log("üéâ All demos completed successfully!");

  } catch (error) {
    console.error("‚ùå Demo failed:", error);
    process.exit(1);
  }
}

// Run demo if this script is executed directly
if (import.meta.main) {
  main();
}
```

`src/ratelimiter/RateLimiter.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, test } from "bun:test";
import type { RateLimitConfig } from "../types/metadata";
import { RateLimiter } from "./RateLimiter";

describe("RateLimiter", () => {
  let rateLimiter: RateLimiter;
  let config: RateLimitConfig;
  let consoleSpy: string[];

  beforeEach(() => {
    config = {
      baseDelayMs: 1000,
      maxDelayMs: 10000,
      adaptiveMultiplier: 2.0,
      backoffStrategy: "linear",
      maxRetries: 3,
      cooldownPeriodMs: 60000,
      enableAdaptiveThrottling: true
    };

    rateLimiter = new RateLimiter(config);
    consoleSpy = [];

    // Mock process.stdout.write to capture output
    const originalWrite = process.stdout.write;
    process.stdout.write = (chunk: any) => {
      consoleSpy.push(chunk.toString());
      return true;
    };
  });

  afterEach(() => {
    // Note: In real tests, you would restore the original function
    // For this test, we're just acknowledging the mock behavior
  });

  describe("constructor", () => {
    test("should initialize with provided config", () => {
      expect(rateLimiter).toBeDefined();
      expect(rateLimiter.getConfig()).toEqual(config);
    });

    test("should initialize metrics to zero", () => {
      const metrics = rateLimiter.getMetrics();
      expect(metrics.totalCalls).toBe(0);
      expect(metrics.floodWaitCount).toBe(0);
      expect(metrics.averageFloodWaitSeconds).toBe(0);
      expect(metrics.totalDelayMs).toBe(0);
      expect(metrics.currentMultiplier).toBe(1.0);
    });
  });

  describe("beforeCall", () => {
    test("should update metrics and apply delay", async () => {
      const startTime = Date.now();
      await rateLimiter.beforeCall();
      const endTime = Date.now();

      const metrics = rateLimiter.getMetrics();
      expect(metrics.totalCalls).toBe(1);

      // Should have some delay applied
      expect(endTime - startTime).toBeGreaterThanOrEqual(0);
    });

    test("should handle multiple calls with proper timing", async () => {
      // First call
      await rateLimiter.beforeCall();
      const metrics1 = rateLimiter.getMetrics();
      expect(metrics1.totalCalls).toBe(1);

      // Second call should have delay
      const startTime = Date.now();
      await rateLimiter.beforeCall();
      const endTime = Date.now();

      const metrics2 = rateLimiter.getMetrics();
      expect(metrics2.totalCalls).toBe(2);
      expect(endTime - startTime).toBeGreaterThan(0);
    });
  });

  describe("handleFloodWait", () => {
    test("should handle FLOOD_WAIT without countdown", async () => {
      const floodWaitSeconds = 0.1; // 100ms for quick test
      const startTime = Date.now();

      await rateLimiter.handleFloodWait(floodWaitSeconds, false);

      const endTime = Date.now();
      const elapsed = endTime - startTime;

      // Should have waited approximately the right amount of time
      expect(elapsed).toBeGreaterThanOrEqual(90); // Allow some variance
      expect(elapsed).toBeLessThan(200);

      const metrics = rateLimiter.getMetrics();
      expect(metrics.floodWaitCount).toBe(1);
      expect(metrics.averageFloodWaitSeconds).toBe(floodWaitSeconds);
    });

    test("should handle FLOOD_WAIT with countdown display", async () => {
      const floodWaitSeconds = 0.1; // 100ms for quick test
      const startTime = Date.now();

      await rateLimiter.handleFloodWait(floodWaitSeconds, true);

      const endTime = Date.now();
      const elapsed = endTime - startTime;

      // Should have waited approximately the right amount of time (allowing for countdown overhead)
      expect(elapsed).toBeGreaterThanOrEqual(90);
      expect(elapsed).toBeLessThan(2000); // More generous timing for countdown

      // Should have output countdown messages
      expect(consoleSpy.length).toBeGreaterThan(0);

      // Should contain countdown elements
      const output = consoleSpy.join('');
      expect(output).toContain('‚è≥ Remaining:');
      expect(output).toContain('‚úÖ Rate limit cleared');

      const metrics = rateLimiter.getMetrics();
      expect(metrics.floodWaitCount).toBe(1);
    });

    test("should use countdown by default", async () => {
      const floodWaitSeconds = 0.1;

      await rateLimiter.handleFloodWait(floodWaitSeconds);

      // Should have output countdown messages by default
      expect(consoleSpy.length).toBeGreaterThan(0);
      const output = consoleSpy.join('');
      expect(output).toContain('‚è≥ Remaining:');
    });

    test("should handle zero seconds gracefully", async () => {
      const startTime = Date.now();

      await rateLimiter.handleFloodWait(0, true);

      const endTime = Date.now();
      const elapsed = endTime - startTime;

      // Should complete quickly for zero seconds
      expect(elapsed).toBeLessThan(100);

      const metrics = rateLimiter.getMetrics();
      expect(metrics.floodWaitCount).toBe(1);
    });

    test("should update metrics correctly", async () => {
      const floodWaitSeconds1 = 0.1;
      const floodWaitSeconds2 = 0.2;

      await rateLimiter.handleFloodWait(floodWaitSeconds1, false);
      await rateLimiter.handleFloodWait(floodWaitSeconds2, false);

      const metrics = rateLimiter.getMetrics();
      expect(metrics.floodWaitCount).toBe(2);

      // Calculate expected average
      const expectedAverage = (floodWaitSeconds1 + floodWaitSeconds2) / 2;
      expect(metrics.averageFloodWaitSeconds).toBeCloseTo(expectedAverage, 2);

      // Should have accumulated delay time
      expect(metrics.totalDelayMs).toBeGreaterThan(0);
    });
  });

  describe("markSuccessfulCall", () => {
    test("should reset consecutive FLOOD_WAIT counter", async () => {
      // Simulate FLOOD_WAIT errors
      await rateLimiter.handleFloodWait(0.1, false);
      await rateLimiter.handleFloodWait(0.1, false);

      // Mark successful call
      rateLimiter.markSuccessfulCall();

      // Should reduce delay if adaptive throttling is enabled
      const metrics = rateLimiter.getMetrics();
      expect(metrics.floodWaitCount).toBe(2); // Count should remain
    });

    test("should reduce delay after successful calls with adaptive throttling", async () => {
      // Cause delay to increase
      await rateLimiter.handleFloodWait(0.1, false);

      const metricsAfterFlood = rateLimiter.getMetrics();
      const multiplierAfterFlood = metricsAfterFlood.currentMultiplier;

      // Mark successful call
      rateLimiter.markSuccessfulCall();

      const metricsAfterSuccess = rateLimiter.getMetrics();
      const multiplierAfterSuccess = metricsAfterSuccess.currentMultiplier;

      // Multiplier should be reduced (or stay same if at base)
      expect(multiplierAfterSuccess).toBeLessThanOrEqual(multiplierAfterFlood);
    });
  });

  describe("adaptive throttling", () => {
    test("should adjust delay after multiple FLOOD_WAITs with linear strategy", async () => {
      const linearConfig = { ...config, backoffStrategy: "linear" as const };
      rateLimiter = new RateLimiter(linearConfig);

      const initialMetrics = rateLimiter.getMetrics();
      expect(initialMetrics.currentMultiplier).toBe(1.0);

      // First FLOOD_WAIT
      await rateLimiter.handleFloodWait(0.1, false);
      const metrics1 = rateLimiter.getMetrics();
      expect(metrics1.currentMultiplier).toBeGreaterThan(1.0);

      // Second FLOOD_WAIT should increase multiplier further
      await rateLimiter.handleFloodWait(0.1, false);
      const metrics2 = rateLimiter.getMetrics();
      expect(metrics2.currentMultiplier).toBeGreaterThan(metrics1.currentMultiplier);
    });

    test("should adjust delay after multiple FLOOD_WAITs with exponential strategy", async () => {
      const exponentialConfig = { ...config, backoffStrategy: "exponential" as const };
      rateLimiter = new RateLimiter(exponentialConfig);

      // First FLOOD_WAIT
      await rateLimiter.handleFloodWait(0.1, false);
      const metrics1 = rateLimiter.getMetrics();

      // Second FLOOD_WAIT should increase multiplier exponentially
      await rateLimiter.handleFloodWait(0.1, false);
      const metrics2 = rateLimiter.getMetrics();
      expect(metrics2.currentMultiplier).toBeGreaterThan(metrics1.currentMultiplier);
    });
  });

  describe("cooldown period", () => {
    test("should apply cooldown after multiple consecutive FLOOD_WAITs", async () => {
      // Generate multiple FLOOD_WAITs
      for (let i = 0; i < 3; i++) {
        await rateLimiter.handleFloodWait(0.1, false);
      }

      expect(rateLimiter.shouldApplyCooldown()).toBe(true);
    });

    test("should not apply cooldown with fewer FLOOD_WAITs", async () => {
      await rateLimiter.handleFloodWait(0.1, false);
      await rateLimiter.handleFloodWait(0.1, false);

      expect(rateLimiter.shouldApplyCooldown()).toBe(false);
    });
  });

  describe("configuration management", () => {
    test("should update configuration", () => {
      const newConfig = { baseDelayMs: 2000 };
      rateLimiter.updateConfig(newConfig);

      const updatedConfig = rateLimiter.getConfig();
      expect(updatedConfig.baseDelayMs).toBe(2000);
      expect(updatedConfig.maxDelayMs).toBe(config.maxDelayMs); // Others unchanged
    });

    test("should reset delay when base delay changes", () => {
      // Increase delay first
      rateLimiter.handleFloodWait(0.1, false);

      // Update base delay
      rateLimiter.updateConfig({ baseDelayMs: 2000 });

      const metrics = rateLimiter.getMetrics();
      expect(metrics.currentMultiplier).toBe(1.0); // Should reset
    });
  });

  describe("reset functionality", () => {
    test("should reset all metrics and state", async () => {
      // Generate some activity
      await rateLimiter.beforeCall();
      await rateLimiter.handleFloodWait(0.1, false);
      rateLimiter.markSuccessfulCall();

      // Reset
      rateLimiter.reset();

      const metrics = rateLimiter.getMetrics();
      expect(metrics.totalCalls).toBe(0);
      expect(metrics.floodWaitCount).toBe(0);
      expect(metrics.averageFloodWaitSeconds).toBe(0);
      expect(metrics.totalDelayMs).toBe(0);
      expect(metrics.currentMultiplier).toBe(1.0);
      expect(metrics.lastFloodWaitTime).toBeUndefined();
    });
  });

  describe("metrics formatting", () => {
    test("should format metrics for display", async () => {
      await rateLimiter.beforeCall();
      await rateLimiter.handleFloodWait(0.1, false);

      const formatted = rateLimiter.formatMetrics();

      expect(formatted).toContain('Rate Limiting Stats:');
      expect(formatted).toContain('Total API calls: 1');
      expect(formatted).toContain('FLOOD_WAIT errors: 1');
      expect(formatted).toContain('Success rate:');
      expect(formatted).toContain('Average FLOOD_WAIT:');
      expect(formatted).toContain('Total delay time:');
      expect(formatted).toContain('Current delay multiplier:');
    });

    test("should handle zero metrics correctly", () => {
      const formatted = rateLimiter.formatMetrics();

      expect(formatted).toContain('Total API calls: 0');
      expect(formatted).toContain('Success rate: 0');
      expect(formatted).toContain('FLOOD_WAIT errors: 0');
    });
  });
});
```

`src/ratelimiter/RateLimiter.ts`

```ts
import type { BackoffStrategy, RateLimitConfig } from "../types/metadata";
import { CountdownTimer } from "./CountdownTimer";

/**
 * Rate limiting metrics and statistics
 */
interface RateLimitMetrics {
  /** Total number of API calls made */
  totalCalls: number;
  /** Number of FLOOD_WAIT errors encountered */
  floodWaitCount: number;
  /** Average wait time for FLOOD_WAIT errors */
  averageFloodWaitSeconds: number;
  /** Total delay time applied */
  totalDelayMs: number;
  /** Current adaptive delay multiplier */
  currentMultiplier: number;
  /** Timestamp of last FLOOD_WAIT error */
  lastFloodWaitTime?: number;
}

/**
 * Rate limiter for Telegraph API calls with adaptive throttling
 */
export class RateLimiter {
  private config: RateLimitConfig;
  private metrics: RateLimitMetrics;
  private currentDelayMs: number;
  private consecutiveFloodWaits: number;
  private lastCallTime: number;

  constructor(config: RateLimitConfig) {
    this.config = config;
    this.metrics = {
      totalCalls: 0,
      floodWaitCount: 0,
      averageFloodWaitSeconds: 0,
      totalDelayMs: 0,
      currentMultiplier: 1.0
    };
    this.currentDelayMs = config.baseDelayMs;
    this.consecutiveFloodWaits = 0;
    this.lastCallTime = 0;
  }

  /**
   * Apply rate limiting delay before making an API call
   */
  async beforeCall(): Promise<void> {
    const now = Date.now();
    const timeSinceLastCall = now - this.lastCallTime;

    if (timeSinceLastCall < this.currentDelayMs) {
      const delayNeeded = this.currentDelayMs - timeSinceLastCall;
      await this.sleep(delayNeeded);
      this.metrics.totalDelayMs += delayNeeded;
    }

    this.lastCallTime = Date.now();
    this.metrics.totalCalls++;
  }

  /**
   * Handle FLOOD_WAIT error and adjust future delays
   * @param floodWaitSeconds Number of seconds from FLOOD_WAIT error
   * @param showCountdown Whether to show visual countdown (default: true)
   */
  async handleFloodWait(floodWaitSeconds: number, showCountdown: boolean = true): Promise<void> {
    this.metrics.floodWaitCount++;
    this.consecutiveFloodWaits++;
    this.metrics.lastFloodWaitTime = Date.now();

    // Update average FLOOD_WAIT time
    const totalWaitTime = this.metrics.averageFloodWaitSeconds * (this.metrics.floodWaitCount - 1) + floodWaitSeconds;
    this.metrics.averageFloodWaitSeconds = totalWaitTime / this.metrics.floodWaitCount;

    // Apply adaptive throttling
    if (this.config.enableAdaptiveThrottling) {
      this.adjustDelayAfterFloodWait();
    }

    // Wait for the required time with optional countdown
    const waitMs = floodWaitSeconds * 1000;

    if (showCountdown && floodWaitSeconds > 0) {
      await this.sleepWithCountdown(waitMs, floodWaitSeconds);
    } else {
      await this.sleep(waitMs);
    }

    this.metrics.totalDelayMs += waitMs;
  }

  /**
   * Mark successful API call (no FLOOD_WAIT)
   */
  markSuccessfulCall(): void {
    // Reset consecutive FLOOD_WAIT counter on success
    if (this.consecutiveFloodWaits > 0) {
      this.consecutiveFloodWaits = 0;

      // Gradually reduce delay after successful calls
      if (this.config.enableAdaptiveThrottling && this.currentDelayMs > this.config.baseDelayMs) {
        this.currentDelayMs = Math.max(
          this.config.baseDelayMs,
          this.currentDelayMs * 0.8 // Reduce by 20%
        );
        this.metrics.currentMultiplier = this.currentDelayMs / this.config.baseDelayMs;
      }
    }
  }

  /**
   * Adjust delay after FLOOD_WAIT error based on strategy
   */
  private adjustDelayAfterFloodWait(): void {
    const multiplier = this.calculateAdaptiveMultiplier();
    this.currentDelayMs = Math.min(
      this.config.maxDelayMs,
      this.config.baseDelayMs * multiplier
    );
    this.metrics.currentMultiplier = multiplier;
  }

  /**
   * Calculate adaptive multiplier based on FLOOD_WAIT history
   */
  private calculateAdaptiveMultiplier(): number {
    const baseMultiplier = this.config.adaptiveMultiplier;

    switch (this.config.backoffStrategy) {
      case 'linear':
        return Math.min(
          baseMultiplier * this.consecutiveFloodWaits,
          this.config.maxDelayMs / this.config.baseDelayMs
        );

      case 'exponential':
        return Math.min(
          baseMultiplier ** this.consecutiveFloodWaits,
          this.config.maxDelayMs / this.config.baseDelayMs
        );

      default:
        return baseMultiplier;
    }
  }

  /**
   * Check if we need a cooldown period
   */
  shouldApplyCooldown(): boolean {
    if (!this.metrics.lastFloodWaitTime) {
      return false;
    }

    const timeSinceLastFloodWait = Date.now() - this.metrics.lastFloodWaitTime;
    return timeSinceLastFloodWait < this.config.cooldownPeriodMs && this.consecutiveFloodWaits >= 3;
  }

  /**
   * Apply cooldown period if needed
   */
  async applyCooldownIfNeeded(): Promise<void> {
    if (this.shouldApplyCooldown()) {
      const remainingCooldown = this.config.cooldownPeriodMs - (Date.now() - this.metrics.lastFloodWaitTime!);
      if (remainingCooldown > 0) {
        console.warn(`üìã Applying cooldown period: ${Math.ceil(remainingCooldown / 1000)}s`);
        await this.sleep(remainingCooldown);
        this.metrics.totalDelayMs += remainingCooldown;
      }
    }
  }

  /**
   * Get current rate limiting metrics
   */
  getMetrics(): RateLimitMetrics {
    return { ...this.metrics };
  }

  /**
   * Get current configuration
   */
  getConfig(): RateLimitConfig {
    return { ...this.config };
  }

  /**
   * Update configuration
   */
  updateConfig(newConfig: Partial<RateLimitConfig>): void {
    this.config = { ...this.config, ...newConfig };

    // Reset delay if base delay changed
    if (newConfig.baseDelayMs) {
      this.currentDelayMs = newConfig.baseDelayMs;
      this.metrics.currentMultiplier = 1.0;
    }
  }

  /**
   * Reset metrics and state
   */
  reset(): void {
    this.metrics = {
      totalCalls: 0,
      floodWaitCount: 0,
      averageFloodWaitSeconds: 0,
      totalDelayMs: 0,
      currentMultiplier: 1.0
    };
    this.currentDelayMs = this.config.baseDelayMs;
    this.consecutiveFloodWaits = 0;
    this.lastCallTime = 0;
  }

  /**
   * Format metrics for display
   */
  formatMetrics(): string {
    const { totalCalls, floodWaitCount, averageFloodWaitSeconds, totalDelayMs, currentMultiplier } = this.metrics;
    const successRate = totalCalls > 0 ? ((totalCalls - floodWaitCount) / totalCalls * 100).toFixed(1) : '0';

    return [
      `üìä Rate Limiting Stats:`,
      `   ‚Ä¢ Total API calls: ${totalCalls}`,
      `   ‚Ä¢ Success rate: ${successRate}%`,
      `   ‚Ä¢ FLOOD_WAIT errors: ${floodWaitCount}`,
      `   ‚Ä¢ Average FLOOD_WAIT: ${averageFloodWaitSeconds.toFixed(1)}s`,
      `   ‚Ä¢ Total delay time: ${(totalDelayMs / 1000).toFixed(1)}s`,
      `   ‚Ä¢ Current delay multiplier: ${currentMultiplier.toFixed(1)}x`,
      `   ‚Ä¢ Current delay: ${(this.currentDelayMs / 1000).toFixed(1)}s`
    ].join('\n');
  }

  /**
   * Sleep for specified milliseconds
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * Sleep with visual countdown display
   * @param ms Milliseconds to sleep
   * @param seconds Original seconds for display
   */
  private async sleepWithCountdown(ms: number, seconds: number): Promise<void> {
    const timer = new CountdownTimer(ms);

    timer.onUpdate((remaining: string, progress: number, progressBar?: string) => {
      const progressPercent = Math.round(progress);
      const line = `‚è≥ Remaining: ${remaining} ${progressBar || ''} ${progressPercent}%`;

      // Clear line and write countdown
      process.stdout.write(`\r${line.padEnd(80)}`);
    });

    timer.onComplete(() => {
      // Clear the countdown line and show completion
      process.stdout.write('\r‚úÖ Rate limit cleared, retrying...'.padEnd(80) + '\n');
    });

    await timer.start();
  }
}
```

`src/test-utils/TestHelpers.ts`

```ts
import { existsSync, mkdirSync, rmSync, writeFileSync, readFileSync } from "node:fs";
import { tmpdir } from "node:os";
import { dirname, join } from "node:path";
import type { FileMetadata, MetadataConfig, PublishedPageInfo } from "../types/metadata";

/**
 * Test utilities and helpers for Telegraph publisher testing
 */
export class TestHelpers {
  private static testDirs: string[] = [];

  /**
   * Create a temporary test directory
   * @param prefix Directory prefix
   * @returns Path to created directory
   */
  static createTempDir(prefix: string = "telegraph-test"): string {
    const tempDir = join(tmpdir(), `${prefix}-${Date.now()}-${Math.random().toString(36).substring(7)}`);
    mkdirSync(tempDir, { recursive: true });
    TestHelpers.testDirs.push(tempDir);
    return tempDir;
  }

  /**
   * Clean up all created test directories
   */
  static cleanup(): void {
    for (const dir of TestHelpers.testDirs) {
      if (existsSync(dir)) {
        rmSync(dir, { recursive: true, force: true });
      }
    }
    TestHelpers.testDirs = [];
  }

  /**
   * Create a test markdown file
   * @param filePath File path
   * @param content File content
   * @param metadata Optional metadata to inject
   */
  static createTestFile(filePath: string, content: string, metadata?: FileMetadata): void {
    const dir = dirname(filePath);
    if (!existsSync(dir)) {
      mkdirSync(dir, { recursive: true });
    }

    let finalContent = content;
    if (metadata) {
      const yamlFrontMatter = TestHelpers.createYamlFrontMatter(metadata);
      finalContent = `${yamlFrontMatter}\n${content}`;
    }

    writeFileSync(filePath, finalContent, "utf-8");
  }

  /**
   * Create YAML front-matter from metadata
   * @param metadata Metadata object
   * @returns YAML front-matter string
   */
  static createYamlFrontMatter(metadata: FileMetadata): string {
    const lines = [
      "---",
      `telegraphUrl: "${metadata.telegraphUrl}"`,
      `editPath: "${metadata.editPath}"`,
      `username: "${metadata.username}"`,
      `publishedAt: "${metadata.publishedAt}"`,
      `originalFilename: "${metadata.originalFilename}"`
    ];

    if (metadata.title) {
      lines.push(`title: "${metadata.title}"`);
    }
    if (metadata.description) {
      lines.push(`description: "${metadata.description}"`);
    }

    lines.push("---");
    return lines.join("\n");
  }

  /**
   * Create sample metadata
   * @param overrides Optional property overrides
   * @returns Sample metadata object
   */
  static createSampleMetadata(overrides: Partial<FileMetadata> = {}): FileMetadata {
    return {
      telegraphUrl: "https://telegra.ph/Sample-Article-01-01",
      editPath: "Sample-Article-01-01",
      username: "Test Author",
      publishedAt: new Date().toISOString(),
      originalFilename: "sample.md",
      title: "Sample Article",
      description: "A sample article for testing",
      ...overrides
    };
  }

  /**
   * Create sample published page info
   * @param overrides Optional property overrides
   * @returns Sample published page info
   */
  static createSamplePageInfo(overrides: Partial<PublishedPageInfo> = {}): PublishedPageInfo {
    return {
      telegraphUrl: "https://telegra.ph/Sample-Article-01-01",
      editPath: "Sample-Article-01-01",
      title: "Sample Article",
      authorName: "Test Author",
      publishedAt: new Date().toISOString(),
      lastUpdated: new Date().toISOString(),
      views: 42,
      localFilePath: "/path/to/sample.md",
      ...overrides
    };
  }

  /**
   * Create test configuration
   * @param overrides Optional property overrides
   * @returns Test configuration object
   */
  static createTestConfig(overrides: Partial<MetadataConfig> = {}): MetadataConfig {
    return {
      defaultUsername: "Test User",
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: true,
      autoSyncCache: true,
      customFields: {},
      ...overrides
    };
  }

  /**
   * Create markdown content with links
   * @param title Article title
   * @param localLinks Array of local link paths
   * @param telegraphLinks Array of Telegraph URLs
   * @returns Markdown content
   */
  static createMarkdownWithLinks(
    title: string,
    localLinks: string[] = [],
    telegraphLinks: string[] = []
  ): string {
    const content = [`# ${title}`, "", "This is a test article with various links.", ""];

    if (localLinks.length > 0) {
      content.push("## Local Links");
      localLinks.forEach((link, index) => {
        content.push(`- [Local Link ${index + 1}](${link})`);
      });
      content.push("");
    }

    if (telegraphLinks.length > 0) {
      content.push("## Telegraph Links");
      telegraphLinks.forEach((link, index) => {
        content.push(`- [Telegraph Link ${index + 1}](${link})`);
      });
      content.push("");
    }

    content.push("End of test content.");
    return content.join("\n");
  }

  /**
   * Create a complex test project structure
   * @param baseDir Base directory
   * @returns Object with file paths
   */
  static createTestProject(baseDir: string): {
    mainFile: string;
    dependencyFile1: string;
    dependencyFile2: string;
    circularFile: string;
    configFile: string;
    cacheFile: string;
  } {
    const mainFile = join(baseDir, "main.md");
    const dependencyFile1 = join(baseDir, "deps", "dep1.md");
    const dependencyFile2 = join(baseDir, "deps", "dep2.md");
    const circularFile = join(baseDir, "circular.md");
    const configFile = join(baseDir, ".telegraph-publisher-config.json");
    const cacheFile = join(baseDir, ".telegraph-pages-cache.json");

    // Create main file with dependencies
    TestHelpers.createTestFile(
      mainFile,
      TestHelpers.createMarkdownWithLinks("Main Article", ["./deps/dep1.md", "./deps/dep2.md"])
    );

    // Create dependency files
    TestHelpers.createTestFile(
      dependencyFile1,
      TestHelpers.createMarkdownWithLinks("Dependency 1", ["./dep2.md"])
    );

    TestHelpers.createTestFile(
      dependencyFile2,
      TestHelpers.createMarkdownWithLinks("Dependency 2", ["../circular.md"])
    );

    // Create circular dependency
    TestHelpers.createTestFile(
      circularFile,
      TestHelpers.createMarkdownWithLinks("Circular File", ["./main.md"])
    );

    // Create config file
    writeFileSync(configFile, JSON.stringify({
      accessToken: "test-token-123",
      defaultUsername: "Test User",
      autoPublishDependencies: true,
      manageBidirectionalLinks: true
    }, null, 2));

    // Create cache file
    writeFileSync(cacheFile, JSON.stringify({
      version: "1.0.0",
      lastUpdated: new Date().toISOString(),
      accessTokenHash: "test-hash",
      pages: {},
      localToTelegraph: {},
      telegraphToLocal: {}
    }, null, 2));

    return {
      mainFile,
      dependencyFile1,
      dependencyFile2,
      circularFile,
      configFile,
      cacheFile
    };
  }

  /**
   * Read file content
   * @param filePath File path
   * @returns File content
   */
  static readFile(filePath: string): string {
    return readFileSync(filePath, "utf-8");
  }

  /**
   * Check if file exists
   * @param filePath File path
   * @returns Whether file exists
   */
  static fileExists(filePath: string): boolean {
    return existsSync(filePath);
  }

  /**
   * Create mock Telegraph API responses
   */
  static createMockTelegraphResponses() {
    return {
      createAccount: {
        ok: true,
        result: {
          short_name: "Test",
          author_name: "Test Author",
          author_url: "",
          access_token: "test-token-123",
          auth_url: "https://edit.telegra.ph/auth/test"
        }
      },
      createPage: {
        ok: true,
        result: {
          path: "Test-Article-01-01",
          url: "https://telegra.ph/Test-Article-01-01",
          title: "Test Article",
          description: "Test description",
          author_name: "Test Author",
          author_url: "",
          image_url: "",
          content: [],
          views: 0,
          can_edit: true
        }
      },
      editPage: {
        ok: true,
        result: {
          path: "Test-Article-01-01",
          url: "https://telegra.ph/Test-Article-01-01",
          title: "Test Article Updated",
          description: "Test description updated",
          author_name: "Test Author",
          author_url: "",
          image_url: "",
          content: [],
          views: 42,
          can_edit: true
        }
      },
      getPageList: {
        ok: true,
        result: {
          total_count: 2,
          pages: [
            {
              path: "Test-Article-01-01",
              url: "https://telegra.ph/Test-Article-01-01",
              title: "Test Article",
              description: "Test description",
              author_name: "Test Author",
              author_url: "",
              image_url: "",
              views: 42,
              can_edit: true
            },
            {
              path: "Another-Article-01-02",
              url: "https://telegra.ph/Another-Article-01-02",
              title: "Another Article",
              description: "Another description",
              author_name: "Test Author",
              author_url: "",
              image_url: "",
              views: 24,
              can_edit: true
            }
          ]
        }
      }
    };
  }

  /**
   * Assert that two objects are deeply equal
   * @param actual Actual value
   * @param expected Expected value
   * @param message Optional error message
   */
  static assertEqual<T>(actual: T, expected: T, message?: string): void {
    if (JSON.stringify(actual) !== JSON.stringify(expected)) {
      throw new Error(message || `Expected ${JSON.stringify(expected)}, got ${JSON.stringify(actual)}`);
    }
  }

  /**
   * Assert that value is truthy
   * @param value Value to check
   * @param message Optional error message
   */
  static assertTrue(value: any, message?: string): void {
    if (!value) {
      throw new Error(message || `Expected truthy value, got ${value}`);
    }
  }

  /**
   * Assert that value is falsy
   * @param value Value to check
   * @param message Optional error message
   */
  static assertFalse(value: any, message?: string): void {
    if (value) {
      throw new Error(message || `Expected falsy value, got ${value}`);
    }
  }

  /**
   * Assert that function throws an error
   * @param fn Function to test
   * @param expectedMessage Optional expected error message
   */
  static assertThrows(fn: () => void, expectedMessage?: string): void {
    let thrown = false;
    try {
      fn();
    } catch (error) {
      thrown = true;
      if (expectedMessage && error instanceof Error) {
        if (!error.message.includes(expectedMessage)) {
          throw new Error(`Expected error message to contain "${expectedMessage}", got "${error.message}"`);
        }
      }
    }
    if (!thrown) {
      throw new Error("Expected function to throw an error");
    }
  }

  /**
   * Wait for a specified amount of time
   * @param ms Milliseconds to wait
   */
  static async wait(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
```

`src/types/metadata.ts`

```ts
/**
 * Core data structures and interfaces for Telegraph metadata management system
 */

/**
 * Publication status enumeration
 */
export enum PublicationStatus {
  NOT_PUBLISHED = "not_published",
  PUBLISHED = "published",
  METADATA_CORRUPTED = "metadata_corrupted",
  METADATA_MISSING = "metadata_missing"
}

/**
 * Rate limiting strategy options
 */
export type BackoffStrategy = 'linear' | 'exponential';

/**
 * Rate limiting configuration for Telegraph API calls
 */
export interface RateLimitConfig {
  /** Base delay between file publications in milliseconds (default: 1500ms) */
  baseDelayMs: number;
  /** Multiplier applied to delay after FLOOD_WAIT errors (default: 2.0) */
  adaptiveMultiplier: number;
  /** Maximum delay cap in milliseconds (default: 30000ms) */
  maxDelayMs: number;
  /** Backoff strategy for repeated FLOOD_WAIT errors (default: 'linear') */
  backoffStrategy: BackoffStrategy;
  /** Maximum retry attempts for FLOOD_WAIT errors (default: 3) */
  maxRetries: number;
  /** Cooldown period after multiple FLOOD_WAITs in milliseconds (default: 60000ms) */
  cooldownPeriodMs: number;
  /** Enable adaptive throttling based on FLOOD_WAIT patterns (default: true) */
  enableAdaptiveThrottling: boolean;
}

/**
 * File metadata stored in YAML front-matter
 */
export interface FileMetadata {
  /** Telegraph page URL */
  telegraphUrl: string;
  /** Telegraph page path for editing */
  editPath: string;
  /** Username/author name */
  username: string;
  /** Publication timestamp in ISO format */
  publishedAt: string;
  /** Original filename for reference */
  originalFilename: string;
  /** Optional page title */
  title?: string;
  /** Optional description */
  description?: string;
  /** Optional content hash for change detection */
  contentHash?: string;
}

/**
 * Published page information for caching
 */
export interface PublishedPageInfo {
  /** Telegraph page URL */
  telegraphUrl: string;
  /** Telegraph page path for editing */
  editPath: string;
  /** Page title */
  title: string;
  /** Author name */
  authorName: string;
  /** Local file path if known */
  localFilePath?: string;
  /** Publication timestamp */
  publishedAt: string;
  /** Last updated timestamp */
  lastUpdated: string;
  /** Page views count */
  views?: number;
}

/**
 * Published pages cache structure
 */
export interface PublishedPagesCache {
  /** Cache version for compatibility */
  version: string;
  /** Last update timestamp */
  lastUpdated: string;
  /** Access token hash for verification */
  accessTokenHash: string;
  /** Map of page URLs to page info */
  pages: Record<string, PublishedPageInfo>;
  /** Map of local file paths to Telegraph URLs */
  localToTelegraph: Record<string, string>;
  /** Map of Telegraph URLs to local file paths */
  telegraphToLocal: Record<string, string>;
}

/**
 * Local link information found in markdown content
 */
export interface LocalLink {
  /** Link text displayed to user */
  text: string;
  /** Original local path as written in markdown */
  originalPath: string;
  /** Resolved absolute file path */
  resolvedPath: string;
  /** Whether the linked file has been published */
  isPublished: boolean;
  /** Telegraph URL if file is published */
  telegraphUrl?: string;
  /** Full markdown link match for replacement */
  fullMatch: string;
  /** Start position in content */
  startIndex: number;
  /** End position in content */
  endIndex: number;
  /** Whether this is an internal link to our published page */
  isInternalLink?: boolean;
}

/**
 * Telegraph link information found in content
 */
export interface TelegraphLink {
  /** Link text displayed to user */
  text: string;
  /** Telegraph URL */
  telegraphUrl: string;
  /** Local file path if this is our published page */
  localFilePath?: string;
  /** Full markdown link match for replacement */
  fullMatch: string;
  /** Start position in content */
  startIndex: number;
  /** End position in content */
  endIndex: number;
  /** Whether this link should be converted to local link */
  shouldConvertToLocal: boolean;
}

/**
 * Dependency tree node for managing publication order
 */
export interface DependencyNode {
  /** File path */
  filePath: string;
  /** File metadata if published */
  metadata?: FileMetadata;
  /** Publication status */
  status: PublicationStatus;
  /** Dependencies (files this file links to) */
  dependencies: DependencyNode[];
  /** Whether this node has been processed */
  processed: boolean;
  /** Depth in dependency tree */
  depth: number;
}

/**
 * Content processing result
 */
export interface ProcessedContent {
  /** Original content */
  originalContent: string;
  /** Content without front-matter */
  contentWithoutMetadata: string;
  /** Content with Telegraph URLs (for publishing) */
  contentWithReplacedLinks: string;
  /** Content with local links (for source file) */
  contentWithLocalLinks: string;
  /** Extracted metadata if present */
  metadata?: FileMetadata;
  /** Found local links */
  localLinks: LocalLink[];
  /** Found Telegraph links */
  telegraphLinks: TelegraphLink[];
  /** Whether content was modified */
  hasChanges: boolean;
}

/**
 * Publication result information
 */
export interface PublicationResult {
  /** Whether publication was successful */
  success: boolean;
  /** Telegraph page URL */
  url?: string;
  /** Telegraph page path */
  path?: string;
  /** Error message if failed */
  error?: string;
  /** Whether this was a new publication or edit */
  isNewPublication: boolean;
  /** Metadata that was injected/updated */
  metadata?: FileMetadata;
}

/**
 * Batch publication progress information
 */
export interface PublicationProgress {
  /** Total files to process */
  totalFiles: number;
  /** Files processed so far */
  processedFiles: number;
  /** Files successfully published */
  successfulPublications: number;
  /** Files that failed to publish */
  failedPublications: number;
  /** Current file being processed */
  currentFile?: string;
  /** Overall progress percentage */
  progressPercentage: number;
}

/**
 * Configuration options for metadata management
 */
export interface MetadataConfig {
  /** Default username for publications */
  defaultUsername?: string;
  /** Whether to auto-publish dependencies */
  autoPublishDependencies: boolean;
  /** Whether to replace links in published content */
  replaceLinksinContent: boolean;
  /** Maximum dependency depth to prevent infinite recursion */
  maxDependencyDepth: number;
  /** Whether to create backup before modifying files */
  createBackups: boolean;
  /** Whether to manage bidirectional links */
  manageBidirectionalLinks: boolean;
  /** Whether to auto-sync published pages cache */
  autoSyncCache: boolean;
  /** Rate limiting configuration for bulk operations */
  rateLimiting: RateLimitConfig;
  /** Custom metadata fields */
  customFields?: Record<string, any>;
}
```

`src/utils/PathResolver.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, jest, spyOn, test } from "bun:test";
import * as fs from 'node:fs'; // Import the actual fs module
import { dirname, join as nodePathJoin, resolve as nodePathResolve } from 'node:path';
import { PathResolver } from "./PathResolver";

describe('PathResolver', () => {
  let pathResolver: PathResolver;

  beforeEach(() => {
    pathResolver = PathResolver.getInstance();
    // Clear cache before each test
    (pathResolver as any).projectRootCache.clear();
  });

  afterEach(() => {
    // Clear PathResolver's cache after each test
    (pathResolver as any).projectRootCache.clear();
    // Restore any spies
    jest.restoreAllMocks();
  });

  test('should be a singleton instance', () => {
    const anotherInstance = PathResolver.getInstance();
    expect(pathResolver).toBe(anotherInstance);
  });

  describe('findProjectRoot', () => {
    test('should find project root in current working directory', () => {
      // Use actual current working directory which should have package.json
      const currentDir = process.cwd();
      const testFilePath = nodePathJoin(currentDir, 'src', 'utils', 'PathResolver.ts');

      const result = pathResolver.findProjectRoot(testFilePath);
      expect(result).toBe(currentDir);
    });

    test('should return dirname if no project root found', () => {
      // Create a spy to mock existsSync for this specific test
      const existsSyncSpy = jest.spyOn(fs, 'existsSync');
      existsSyncSpy.mockReturnValue(false);

      const filePathWithoutRoot = '/some/random/dir/file.md';
      const result = pathResolver.findProjectRoot(filePathWithoutRoot);
      expect(result).toBe(nodePathResolve('/some/random/dir'));

      existsSyncSpy.mockRestore();
    });

    test('should cache project root results', () => {
      const existsSyncSpy = jest.spyOn(fs, 'existsSync');

      const testPath = '/test/project/src/file.md';

      // First call
      pathResolver.findProjectRoot(testPath);
      const firstCallCount = existsSyncSpy.mock.calls.length;

      // Second call - should use cache
      pathResolver.findProjectRoot(testPath);
      const secondCallCount = existsSyncSpy.mock.calls.length;

      // Should not make additional calls due to caching
      expect(secondCallCount).toBe(firstCallCount);

      existsSyncSpy.mockRestore();
    });
  });

  describe('resolve', () => {
    const mockFromPath = '/home/user/project/src/docs/page.md';
    const mockProjectRoot = '/home/user/project';

    test('should resolve relative path starting with ./ ', () => {
      const linkHref = './images/image.png';
      const expected = nodePathResolve(dirname(mockFromPath), linkHref);
      expect(pathResolver.resolve(mockFromPath, linkHref)).toBe(expected);
    });

    test('should resolve relative path starting with ../ ', () => {
      const linkHref = '../assets/doc.pdf';
      const expected = nodePathResolve(dirname(mockFromPath), linkHref);
      expect(pathResolver.resolve(mockFromPath, linkHref)).toBe(expected);
    });

    test('should resolve relative path without explicit ./ or ../', () => {
      const linkHref = 'another_file.md';
      const expected = nodePathResolve(dirname(mockFromPath), linkHref);
      expect(pathResolver.resolve(mockFromPath, linkHref)).toBe(expected);
    });

    test('should resolve absolute path relative to project root', () => {
      // Mock findProjectRoot for this specific test
      const findProjectRootSpy = jest.spyOn(pathResolver, 'findProjectRoot');
      findProjectRootSpy.mockReturnValue(mockProjectRoot);

      const linkHref = '/components/button.md';
      const expected = nodePathResolve(mockProjectRoot, linkHref.slice(1));
      expect(pathResolver.resolve(mockFromPath, linkHref)).toBe(expected);
      expect(findProjectRootSpy).toHaveBeenCalledWith(mockFromPath);

      findProjectRootSpy.mockRestore();
    });

    test('should handle complex relative paths', () => {
      const complexFromPath = '/a/b/c/d/file.md';
      const linkHref = '../../../sibling/target.md';
      const expected = nodePathResolve(dirname(complexFromPath), linkHref);
      expect(pathResolver.resolve(complexFromPath, linkHref)).toBe(expected);
    });

    test('should handle paths with query parameters or fragments', () => {
      const linkHref = './page.md?param=value#section';
      const expected = nodePathResolve(dirname(mockFromPath), linkHref);
      expect(pathResolver.resolve(mockFromPath, linkHref)).toBe(expected);
    });

    test('should handle root absolute path', () => {
      // Mock findProjectRoot for this specific test
      const findProjectRootSpy = jest.spyOn(pathResolver, 'findProjectRoot');
      findProjectRootSpy.mockReturnValue(mockProjectRoot);

      const linkHref = '/root_file.md';
      const expected = nodePathResolve(mockProjectRoot, linkHref.slice(1));
      expect(pathResolver.resolve(mockFromPath, linkHref)).toBe(expected);

      findProjectRootSpy.mockRestore();
    });
  });
});
```

`src/utils/PathResolver.ts`

```ts
import * as fs from 'node:fs';
import { dirname, isAbsolute, join, resolve as pathResolve } from 'node:path';

/**
 * PathResolver provides a unified mechanism for resolving file paths.
 * This class is designed to be a singleton or instantiated once and passed around.
 */
export class PathResolver {
  private static instance: PathResolver;
  private projectRootCache: Map<string, string> = new Map();

  private constructor() { }

  public static getInstance(): PathResolver {
    if (!PathResolver.instance) {
      PathResolver.instance = new PathResolver();
    }
    return PathResolver.instance;
  }

  /**
   * Finds the project root directory by searching upwards for package.json or .git.
   * Results are cached for performance.
   * @param startPath The path to start searching from.
   * @returns The absolute path to the project root.
   */
  public findProjectRoot(startPath: string): string {
    const normalizedStartPath = pathResolve(startPath);
    if (this.projectRootCache.has(normalizedStartPath)) {
      return this.projectRootCache.get(normalizedStartPath)!;
    }

    let currentPath = dirname(normalizedStartPath);
    const root = pathResolve('/');

    while (currentPath !== root) {
      if (fs.existsSync(join(currentPath, 'package.json')) || fs.existsSync(join(currentPath, '.git'))) {
        this.projectRootCache.set(normalizedStartPath, currentPath);
        return currentPath;
      }

      const parentPath = dirname(currentPath);
      if (parentPath === currentPath) {
        break; // Reached the file system root
      }
      currentPath = parentPath;
    }

    // If no project root found, use the directory containing the start file/path
    const finalRoot = dirname(normalizedStartPath);
    this.projectRootCache.set(normalizedStartPath, finalRoot);
    return finalRoot;
  }

  /**
   * Resolves a link path relative to a base path or the project root.
   * @param fromPath The base path (usually the source file's path).
   * @param linkHref The link href to resolve.
   * @returns The absolute resolved path.
   */
  public resolve(fromPath: string, linkHref: string): string {
    // If linkHref starts with /, resolve relative to the project root.
    if (isAbsolute(linkHref)) {
      const projectRoot = this.findProjectRoot(fromPath);
      return pathResolve(projectRoot, linkHref.slice(1)); // Remove leading slash
    }

    // If linkHref is relative (e.g., ./ or ../), resolve relative to the directory of fromPath.
    const fromDir = dirname(fromPath);
    return pathResolve(fromDir, linkHref);
  }
}
```

`src/workflow/index.ts`

```ts
export { PublicationWorkflowManager } from './PublicationWorkflowManager';
```

`src/workflow/PublicationWorkflowManager.test.ts`

```ts
import { afterEach, beforeEach, describe, expect, jest, test } from 'bun:test';
import { existsSync, mkdirSync, readFileSync, rmSync, writeFileSync } from 'node:fs';
import { join } from 'node:path';
import { ProgressIndicator } from '../cli/ProgressIndicator';
import type { MetadataConfig } from '../types/metadata';
import { PublicationWorkflowManager } from './PublicationWorkflowManager';

describe('PublicationWorkflowManager', () => {
  const testDir = join(process.cwd(), 'test-publication-workflow');
  let workflowManager: PublicationWorkflowManager;
  let mockConfig: MetadataConfig;
  let mockAccessToken: string;

  beforeEach(() => {
    // Create test directory structure
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
    mkdirSync(testDir, { recursive: true });

    // Mock ProgressIndicator to avoid issues
    jest.spyOn(ProgressIndicator, 'showStatus').mockImplementation((message: string, type?: "info" | "success" | "warning" | "error") => { });

    // Setup mock configuration
    mockConfig = {
      defaultUsername: 'test-user',
      autoPublishDependencies: true,
      replaceLinksinContent: true,
      maxDependencyDepth: 5,
      createBackups: false,
      manageBidirectionalLinks: false,
      autoSyncCache: false,
      rateLimiting: {
        baseDelayMs: 1500,
        adaptiveMultiplier: 2.0,
        maxDelayMs: 30000,
        backoffStrategy: 'linear' as const,
        maxRetries: 3,
        cooldownPeriodMs: 60000,
        enableAdaptiveThrottling: true
      }
    };

    mockAccessToken = 'test-access-token-123';

    // Create PublicationWorkflowManager instance
    workflowManager = new PublicationWorkflowManager(mockConfig, mockAccessToken);
  });

  afterEach(() => {
    // Clean up test directory
    if (existsSync(testDir)) {
      rmSync(testDir, { recursive: true, force: true });
    }
    // Restore all mocks
    jest.restoreAllMocks();
  });

  describe('constructor', () => {
    test('should initialize with correct configuration and access token', () => {
      expect(workflowManager).toBeInstanceOf(PublicationWorkflowManager);
      // The internal config and accessToken are private, but we can test behavior
    });
  });

  describe('publish', () => {
    test('should publish single file without verification when noVerify option is true', async () => {
      // Setup test file
      const testFile = join(testDir, 'test.md');
      writeFileSync(testFile, '# Test Article\n\nContent with [link](./target.md)');

      // Mock the publisher
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: true,
        url: 'https://telegra.ph/Test-Article-01-01',
        path: 'Test-Article-01-01'
      });

      // Mock console.log to capture output
      const consoleSpy = jest.spyOn(console, 'log').mockImplementation();

      // Test options with noVerify: true
      const options = { noVerify: true, withDependencies: true, forceRepublish: false, dryRun: false };

      // Run publish
      await workflowManager.publish(testFile, options);

      // Verify publisher was called
      expect(mockPublisher).toHaveBeenCalledWith(testFile, 'test-user', {
        withDependencies: true,
        forceRepublish: false,
        dryRun: false,
        debug: false,
        generateAside: true
      });

      // Verify success message was logged
      expect(consoleSpy).toHaveBeenCalledWith('üîó URL: https://telegra.ph/Test-Article-01-01');
      expect(consoleSpy).toHaveBeenCalledWith('üìç Path: Test-Article-01-01');

      // Clean up mocks
      mockPublisher.mockRestore();
      consoleSpy.mockRestore();
    });

    test('should verify links and block publication when broken links are found', async () => {
      // Setup test file with broken link
      const testFile = join(testDir, 'test.md');
      writeFileSync(testFile, '# Test Article\n\nContent with [broken link](./nonexistent.md)');

      // Mock LinkVerifier to return broken links
      const mockVerifier = jest.spyOn(workflowManager['linkVerifier'], 'verifyLinks');
      mockVerifier.mockResolvedValue({
        filePath: testFile,
        allLinks: [],
        localLinks: [],
        brokenLinks: [
          {
            filePath: testFile,
            link: {
              text: 'broken link',
              href: './nonexistent.md',
              lineNumber: 3,
              columnStart: 13,
              columnEnd: 42
            },
            suggestions: [],
            canAutoFix: false
          }
        ],
        processingTime: 0
      });

      // Mock console.log and process.exit
      const consoleSpy = jest.spyOn(console, 'log').mockImplementation();
      const exitSpy = jest.spyOn(process, 'exit').mockImplementation((code) => {
        throw new Error(`process.exit(${code})`);
      });

      // Test options without noVerify
      const options = { noVerify: false, noAutoRepair: false, withDependencies: true };

      // Run publish - should throw due to process.exit
      await expect(workflowManager.publish(testFile, options)).rejects.toThrow('process.exit(1)');

      // Verify error messages were logged
      expect(consoleSpy).toHaveBeenCalledWith(expect.stringContaining('üìÑ In file: ' + testFile));
      expect(consoleSpy).toHaveBeenCalledWith('  - "./nonexistent.md" (line 3)');

      // Clean up mocks
      mockVerifier.mockRestore();
      consoleSpy.mockRestore();
      exitSpy.mockRestore();
    });

    test('should auto-repair links when possible and continue publication', async () => {
      // Setup test file with repairable broken link
      const testFile = join(testDir, 'test.md');
      writeFileSync(testFile, '# Test Article\n\nContent with [link](./broken.md)');

      // Mock AutoRepairer to successfully repair the link
      const mockAutoRepairer = jest.spyOn(workflowManager['autoRepairer'], 'autoRepair');
      mockAutoRepairer.mockResolvedValue({
        repairedLinksCount: 1,
        repairedFilesIn: new Set([testFile]),
        remainingBrokenLinks: []
      });

      // Mock LinkVerifier to return broken links initially, then no broken links after repair
      const mockVerifier = jest.spyOn(workflowManager['linkVerifier'], 'verifyLinks');
      mockVerifier
        .mockResolvedValueOnce({
          filePath: testFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: testFile,
              link: {
                text: 'link',
                href: './broken.md',
                lineNumber: 3,
                columnStart: 13,
                columnEnd: 30
              },
              suggestions: ['./fixed.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        })
        .mockResolvedValue({
          filePath: testFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [], // No broken links after repair
          processingTime: 0
        });

      // Mock LinkResolver to return suggestions
      const mockResolver = jest.spyOn(workflowManager['linkResolver'], 'resolveBrokenLinks');
      mockResolver.mockResolvedValue([
        {
          filePath: testFile,
          allLinks: [],
          localLinks: [],
          brokenLinks: [
            {
              filePath: testFile,
              link: {
                text: 'link',
                href: './broken.md',
                lineNumber: 3,
                columnStart: 13,
                columnEnd: 30
              },
              suggestions: ['./fixed.md'],
              canAutoFix: true
            }
          ],
          processingTime: 0
        }
      ]);

      // Mock the publisher
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: false,
        url: 'https://telegra.ph/Test-Article-01-01',
        path: 'Test-Article-01-01'
      });

      // Mock ProgressIndicator.showStatus
      const progressSpy = jest.spyOn(ProgressIndicator, 'showStatus').mockImplementation();

      // Test options without noVerify and noAutoRepair
      const options = { noVerify: false, noAutoRepair: false, withDependencies: true };

      // Run publish
      await workflowManager.publish(testFile, options);

      // Verify auto-repair was called
      expect(mockAutoRepairer).toHaveBeenCalledWith(testFile);

      // Verify success messages were logged
      expect(progressSpy).toHaveBeenCalledWith(expect.stringContaining('üîß Automatically repaired 1 link(s)'), 'success');
      expect(progressSpy).toHaveBeenCalledWith(expect.stringContaining('‚úÖ Link verification passed.'), 'success');

      // Verify publisher was called
      expect(mockPublisher).toHaveBeenCalled();

      // Clean up mocks
      mockAutoRepairer.mockRestore();
      mockVerifier.mockRestore();
      mockResolver.mockRestore();
      mockPublisher.mockRestore();
      progressSpy.mockRestore();
    });

    test('should handle publication failure gracefully', async () => {
      // Setup test file
      const testFile = join(testDir, 'test.md');
      writeFileSync(testFile, '# Test Article\n\nContent without links');

      // Mock the publisher to fail
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: false,
        isNewPublication: false,
        error: 'Publication failed due to network error'
      });

      // Mock ProgressIndicator.showStatus
      const progressSpy = jest.spyOn(ProgressIndicator, 'showStatus').mockImplementation();

      // Test options with noVerify
      const options = { noVerify: true, withDependencies: true };

      // Run publish
      await workflowManager.publish(testFile, options);

      // Verify error message was logged
      expect(progressSpy).toHaveBeenCalledWith(expect.stringContaining('‚ùå Failed: ' + testFile), 'error');

      // Clean up mocks
      mockPublisher.mockRestore();
      progressSpy.mockRestore();
    });

    test('should handle directory publication with multiple files', async () => {
      // Setup multiple test files
      const file1 = join(testDir, 'file1.md');
      const file2 = join(testDir, 'file2.md');
      writeFileSync(file1, '# Article 1\n\nFirst article');
      writeFileSync(file2, '# Article 2\n\nSecond article');

      // Mock LinkScanner to return multiple files
      const mockScanner = jest.spyOn(workflowManager['linkScanner'], 'findMarkdownFiles');
      mockScanner.mockResolvedValue([file1, file2]);

      // Mock the publisher for both files
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher
        .mockResolvedValueOnce({
          success: true,
          isNewPublication: true,
          url: 'https://telegra.ph/Article-1-01-01',
          path: 'Article-1-01-01'
        })
        .mockResolvedValueOnce({
          success: true,
          isNewPublication: true,
          url: 'https://telegra.ph/Article-2-01-01',
          path: 'Article-2-01-01'
        });

      // Mock console.log
      const consoleSpy = jest.spyOn(console, 'log').mockImplementation();

      // Test directory publication
      const options = { noVerify: true, withDependencies: true };

      // Run publish on directory
      await workflowManager.publish(testDir, options);

      // Verify publisher was called for both files
      expect(mockPublisher).toHaveBeenCalledTimes(2);
      expect(mockPublisher).toHaveBeenCalledWith(file1, 'test-user', expect.any(Object));
      expect(mockPublisher).toHaveBeenCalledWith(file2, 'test-user', expect.any(Object));

      // Clean up mocks
      mockScanner.mockRestore();
      mockPublisher.mockRestore();
      consoleSpy.mockRestore();
    });

    test('should handle empty directory gracefully', async () => {
      // Mock LinkScanner to return no files
      const mockScanner = jest.spyOn(workflowManager['linkScanner'], 'findMarkdownFiles');
      mockScanner.mockResolvedValue([]);

      // Mock ProgressIndicator.showStatus
      const progressSpy = jest.spyOn(ProgressIndicator, 'showStatus').mockImplementation();

      // Test empty directory
      const options = { noVerify: true };

      // Run publish on empty directory
      await workflowManager.publish(testDir, options);

      // Verify appropriate message was logged
      expect(progressSpy).toHaveBeenCalledWith('No markdown files found to publish.', 'info');

      // Clean up mocks
      mockScanner.mockRestore();
      progressSpy.mockRestore();
    });

    test('should handle dry run mode correctly', async () => {
      // Setup test file
      const testFile = join(testDir, 'test.md');
      writeFileSync(testFile, '# Test Article\n\nContent for dry run');

      // Mock the publisher
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: true,
        url: 'https://telegra.ph/Test-Article-01-01',
        path: 'Test-Article-01-01'
      });

      // Test dry run mode
      const options = { noVerify: true, dryRun: true };

      // Run publish in dry run mode
      await workflowManager.publish(testFile, options);

      // Verify publisher was called with dryRun: true
      expect(mockPublisher).toHaveBeenCalledWith(testFile, 'test-user', expect.objectContaining({
        dryRun: true
      }));

      // Clean up mocks
      mockPublisher.mockRestore();
    });
  });

  describe('debug option functionality', () => {
    test('should auto-enable dry-run when debug is specified', async () => {
      // Setup test file
      const testFile = join(testDir, 'test-debug.md');
      writeFileSync(testFile, '# Test Article\n\nThis is test content for debug option.');

      // Mock publisher to track calls
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: true,
        url: 'https://telegra.ph/test',
        path: '/test'
      });

      // Test with debug option (should auto-enable dryRun)
      const options = { debug: true, noVerify: true };

      await workflowManager.publish(testFile, options);

      // Verify publisher was called with both debug: true and dryRun: true
      expect(mockPublisher).toHaveBeenCalledWith(testFile, 'test-user', expect.objectContaining({
        debug: true,
        dryRun: true
      }));

      // Clean up mocks
      mockPublisher.mockRestore();
    });

    test('should create JSON file when debug option is used', async () => {
      // Setup test file
      const testFile = join(testDir, 'test-json-creation.md');
      const expectedJsonFile = join(testDir, 'test-json-creation.json');
      writeFileSync(testFile, '# Test Article\n\nThis is test content that should generate JSON.');

      // Mock the telegraph API calls to avoid actual network requests
      const mockPublishNodes = jest.spyOn(workflowManager['publisher'], 'publishNodes');
      mockPublishNodes.mockResolvedValue({
        url: 'https://telegra.ph/test',
        path: '/test'
      });

      // Test with debug option
      const options = { debug: true, noVerify: true };

      await workflowManager.publish(testFile, options);

      // Verify JSON file was created
      expect(existsSync(expectedJsonFile)).toBe(true);

      // Verify JSON content is valid and properly formatted
      const jsonContent = readFileSync(expectedJsonFile, 'utf-8');
      const telegraphNodes = JSON.parse(jsonContent);

      // Should be an array of Telegraph nodes
      expect(Array.isArray(telegraphNodes)).toBe(true);
      expect(telegraphNodes.length).toBeGreaterThan(0);

      // Should be properly formatted with 2-space indentation
      expect(jsonContent).toContain('  ');

      // Clean up created JSON file
      if (existsSync(expectedJsonFile)) {
        rmSync(expectedJsonFile);
      }

      // Clean up mocks
      mockPublishNodes.mockRestore();
    });

    test('should not create JSON file when debug is false', async () => {
      // Setup test file
      const testFile = join(testDir, 'test-no-json.md');
      const expectedJsonFile = join(testDir, 'test-no-json.json');
      writeFileSync(testFile, '# Test Article\n\nThis should not generate JSON.');

      // Mock publisher to avoid actual publication
      const mockPublisher = jest.spyOn(workflowManager['publisher'], 'publishWithMetadata');
      mockPublisher.mockResolvedValue({
        success: true,
        isNewPublication: true,
        url: 'https://telegra.ph/test',
        path: '/test'
      });

      // Test with dryRun but without debug
      const options = { dryRun: true, debug: false, noVerify: true };

      await workflowManager.publish(testFile, options);

      // Verify JSON file was NOT created
      expect(existsSync(expectedJsonFile)).toBe(false);

      // Clean up mocks
      mockPublisher.mockRestore();
    });
  });

  describe('error handling', () => {
    test('should handle workflow exceptions gracefully', async () => {
      // Setup test file in directory
      const testFile = join(testDir, 'test.md');
      writeFileSync(testFile, '# Test Article\n\nContent');

      // Mock LinkScanner to throw an error when scanning directory
      const mockScanner = jest.spyOn(workflowManager['linkScanner'], 'findMarkdownFiles');
      mockScanner.mockRejectedValue(new Error('Scanner error'));

      // Test that the error is propagated when publishing directory
      const options = { noVerify: true };

      await expect(workflowManager.publish(testDir, options)).rejects.toThrow('Scanner error');

      // Clean up mocks
      mockScanner.mockRestore();
    });
  });
});
```

`src/workflow/PublicationWorkflowManager.ts`

```ts
import { lstatSync } from 'node:fs';
import { ProgressIndicator } from '../cli/ProgressIndicator';
import { ConfigManager } from '../config/ConfigManager';
import { AutoRepairer } from '../links/AutoRepairer';
import { LinkResolver } from '../links/LinkResolver';
import { LinkScanner } from '../links/LinkScanner';
import { LinkVerifier } from '../links/LinkVerifier';
import { EnhancedTelegraphPublisher } from '../publisher/EnhancedTelegraphPublisher';
import type { BrokenLink, FileScanResult, MetadataConfig } from '../types/metadata';
import { PathResolver } from '../utils/PathResolver';

/**
 * Orchestrates the publication workflow, including link verification and auto-repair.
 */
export class PublicationWorkflowManager {
  private config: MetadataConfig;
  private accessToken: string;
  private pathResolver: PathResolver;
  private linkScanner: LinkScanner;
  private linkVerifier: LinkVerifier;
  private linkResolver: LinkResolver;
  private autoRepairer: AutoRepairer;
  private publisher: EnhancedTelegraphPublisher;

  constructor(config: MetadataConfig, accessToken: string) {
    this.config = config;
    this.accessToken = accessToken;
    this.pathResolver = PathResolver.getInstance();
    this.linkScanner = new LinkScanner();
    this.linkVerifier = new LinkVerifier(this.pathResolver);
    this.linkResolver = new LinkResolver();
    this.autoRepairer = new AutoRepairer();
    this.publisher = new EnhancedTelegraphPublisher(this.config);
    this.publisher.setAccessToken(this.accessToken);
  }

  /**
   * Handles the entire publication workflow, including link verification and auto-repair.
   * @param targetPath The file or directory path to process.
   * @param options Command options.
   */
  public async publish(targetPath: string, options: any): Promise<void> {
    // Auto-enable dry-run if debug is specified
    if (options.debug) {
      options.dryRun = true;
    }

    // –®–∞–≥ 1: –°–±–æ—Ä —Ñ–∞–π–ª–æ–≤.
    let filesToProcess: string[];

    try {
      const stats = lstatSync(targetPath);
      if (stats.isDirectory()) {
        // If it's a directory, scan for markdown files
        filesToProcess = await this.linkScanner.findMarkdownFiles(targetPath);
      } else {
        // If it's a file, process it directly
        filesToProcess = [targetPath];
      }
    } catch (error) {
      // Check if it's a file system error (can't stat) vs scanner error
      if (error && typeof error === 'object' && 'code' in error) {
        // File system error - assume it's a file
        filesToProcess = [targetPath];
      } else {
        // Scanner error or other error - propagate it
        throw error;
      }
    }

    if (filesToProcess.length === 0) {
      ProgressIndicator.showStatus("No markdown files found to publish.", "info");
      return;
    }

    let allBrokenLinks: BrokenLink[] = [];

    if (!options.noVerify && !options.force) {
      ProgressIndicator.showStatus("üîé Verifying local links...", "info");
      let needsReverification = true;

      while (needsReverification) {
        needsReverification = false;
        allBrokenLinks = [];

        for (const file of filesToProcess) {
          const scanResult = await this.linkScanner.scanFile(file);
          const verifiedResult = await this.linkVerifier.verifyLinks(scanResult);
          allBrokenLinks.push(...verifiedResult.brokenLinks);
        }

        if (allBrokenLinks.length > 0 && !options.noAutoRepair) {
          const fixableLinks = (await this.linkResolver.resolveBrokenLinks([{ ...allBrokenLinks[0], brokenLinks: allBrokenLinks } as FileScanResult]))[0]?.brokenLinks.filter(l => l.suggestions.length === 1) || [];

          if (fixableLinks.length > 0) {
            ProgressIndicator.showStatus(`üîß Attempting to auto-repair ${fixableLinks.length} link(s)...`, "info");
            const repairResult = await this.autoRepairer.autoRepair(targetPath);
            if (repairResult.repairedLinksCount > 0) {
              ProgressIndicator.showStatus(`üîß Automatically repaired ${repairResult.repairedLinksCount} link(s) in ${repairResult.repairedFilesIn.size} file(s).`, "success");
              needsReverification = true; // Re-verify after repair
            }
          }
        }
      }

      // –®–∞–≥ 4: –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è.
      if (allBrokenLinks.length > 0) {
        ProgressIndicator.showStatus(`Publication aborted. Found ${allBrokenLinks.length} broken link(s):`, "error");
        const linksByFile = new Map<string, string[]>();
        for (const broken of allBrokenLinks) {
          if (!linksByFile.has(broken.filePath)) {
            linksByFile.set(broken.filePath, []);
          }
          linksByFile.get(broken.filePath)?.push(`  - "${broken.link.href}" (line ${broken.link.lineNumber})`);
        }
        for (const [file, links] of linksByFile.entries()) {
          console.log(`\nüìÑ In file: ${file}`);
          links.forEach(link => console.log(link));
        }
        console.log("\nüí° Tip: Run 'telegraph-publisher check-links --apply-fixes' to repair them interactively, or fix them manually.");
        process.exit(1);
      } else {
        ProgressIndicator.showStatus("‚úÖ Link verification passed.", "success");
      }
    } else if (options.force) {
      ProgressIndicator.showStatus("‚ö†Ô∏è Bypassing link verification due to --force flag.", "warning");
      ProgressIndicator.showStatus("üîß This mode is intended for debugging only.", "warning");
    }

    // –®–∞–≥ 5: –ü—É–±–ª–∏–∫–∞—Ü–∏—è.
    for (const file of filesToProcess) {
      ProgressIndicator.showStatus(`‚öôÔ∏è Publishing: ${file}`, "info");
      const result = await this.publisher.publishWithMetadata(file, this.config.defaultUsername, {
        withDependencies: options.withDependencies !== false,
        forceRepublish: options.forceRepublish || false,
        dryRun: options.dryRun || false,
        debug: options.debug || false,
        generateAside: options.aside !== false
      });

      if (result.success) {
        ProgressIndicator.showStatus(`${result.isNewPublication ? "Published" : "Updated"} successfully!`, "success");
        if (result.url) {
          console.log(`üîó URL: ${result.url}`);
        }
        if (result.path) {
          console.log(`üìç Path: ${result.path}`);
        }
      } else {
        ProgressIndicator.showStatus(`‚ùå Failed: ${file} - ${result.error}`, "error");
      }
    }
  }
}
```

`src/clean_mr.ts`

```ts
import { readFileSync, writeFileSync } from "node:fs";
import { resolve } from "node:path";

/**
 * Removes specific Markdown formatting from a given string content,
 * leaving a plain text representation. This function is intended to strip
 * all Markdown syntax for purposes like cleaning article titles.
 * @param content The Markdown content as a string.
 * @returns The cleaned content string.
 */
export function cleanMarkdownString(content: string): string {
	let cleanedContent = content;

	// --- Only remove inline Markdown syntax and heading markers ---

	// 1. Remove heading markers (e.g., # Heading, ## Subheading), keeping heading text
	cleanedContent = cleanedContent.replace(/^(#+)\s*/gm, "");

	// 2. Remove inline code (e.g., `code`)
	cleanedContent = cleanedContent.replace(/`([^`]+?)`/g, "$1");

	// 3. Remove images (e.g., ![alt text](url)), keeping alt text
	cleanedContent = cleanedContent.replace(/!\[(.*?)\]\(.*?\)/g, "$1");

	// 4. Remove links (e.g., [link text](url)), keeping link text
	cleanedContent = cleanedContent.replace(/\[(.*?)\]\(.*?\)/g, "$1");

	// 5. Remove bold/italic formatting (e.g., **bold**, *italic*), keeping content
	// Order matters for overlapping syntax (e.g., ***bold-italic*** should be handled by bold/italic first)
	cleanedContent = cleanedContent.replace(/\*\*(.*?)\*\*/g, "$1"); // **bold**
	cleanedContent = cleanedContent.replace(/__(.*?)__/g, "$1"); // __bold__
	cleanedContent = cleanedContent.replace(/\*(.*?)\*/g, "$1"); // *italic*
	cleanedContent = cleanedContent.replace(/_(.*?)_/g, "$1"); // _italic_

	// 6. Remove remaining common Markdown punctuation that might appear in titles
	cleanedContent = cleanedContent.replace(/[*_`[\]()]/g, "");

	// --- Post-processing / Normalization ---

	// Replace multiple spaces/tabs with single space
	cleanedContent = cleanedContent.replace(/[ \t]+/g, " ");

	// Trim leading/trailing whitespace
	cleanedContent = cleanedContent.trim();

	return cleanedContent;
}

/**
 * Reads a Markdown file, cleans its content, and overwrites the original file.
 * This function is intended for full file cleaning, but its usage should be reviewed
 * if only parts of the file need cleaning. For this project, it might be deprecated
 * in favor of targeted string cleaning.
 * @param filePath The path to the Markdown file.
 */
export function cleanMarkdownFile(filePath: string) {
	try {
		const fullPath = resolve(process.cwd(), filePath);
		const content = readFileSync(fullPath, "utf8");

		// NOTE: This now uses a less aggressive cleanMarkdownString.
		// If full file markdown stripping is needed in the future,
		// cleanMarkdownString would need a mode or a separate function.
		const cleanedContent = cleanMarkdownString(content);

		writeFileSync(fullPath, cleanedContent, "utf8");
		console.log(`Successfully cleaned: ${filePath}`);
	} catch (error: unknown) {
		if (
			error instanceof Error &&
			"code" in error &&
			(error as NodeJS.ErrnoException).code === "ENOENT"
		) {
			console.error(`Error: File not found - ${filePath}`);
		} else {
			console.error(
				`Error cleaning file ${filePath}:`,
				error instanceof Error ? error.message : String(error),
			);
		}
		process.exit(1);
	}
}

// Get file path from command line arguments when executed directly
if (import.meta.main) {
	const filePath = process.argv[2];

	if (!filePath) {
		console.error("Usage: bun clean-md <file-path>");
		process.exit(1);
	}

	cleanMarkdownFile(filePath);
}

```

`src/cli.ts`

```ts
#!/usr/bin/env bun

import { existsSync, readFileSync, writeFileSync } from "node:fs";
import { dirname, join, resolve } from "node:path";
import { Command } from "commander";
import { cleanMarkdownFile, cleanMarkdownString } from "./clean_mr";
import { EnhancedCommands } from "./cli/EnhancedCommands";
import { ConfigManager } from "./config/ConfigManager";
import {
	convertMarkdownToTelegraphNodes,
	extractTitleAndContent,
	validateCleanedContent,
} from "./markdownConverter";
import { TelegraphPublisher } from "./telegraphPublisher";

// Configuration file name
const CONFIG_FILE_NAME = ".telegraph-publisher-config.json";

/**
 * Saves the access token to a configuration file in the specified directory.
 * @param directory The directory where the config file should be saved.
 * @param accessToken The access token to save.
 */
function saveAccessToken(directory: string, accessToken: string) {
	const configPath = join(directory, CONFIG_FILE_NAME);
	try {
		writeFileSync(configPath, JSON.stringify({ accessToken }), "utf-8");
		console.log(`‚úÖ Access token saved to ${configPath}`);
	} catch (error) {
		console.error(
			`‚ùå Error saving access token to ${configPath}:`,
			error instanceof Error ? error.message : String(error),
		);
	}
}

/**
 * Loads the access token from a configuration file in the specified directory.
 * @param directory The directory where the config file might be found.
 * @returns The access token if found, otherwise undefined.
 */
function loadAccessToken(directory: string): string | undefined {
	const configPath = join(directory, CONFIG_FILE_NAME);
	if (existsSync(configPath)) {
		try {
			const config = JSON.parse(readFileSync(configPath, "utf-8"));
			if (config.accessToken) {
				console.log(`‚úÖ Access token loaded from ${configPath}`);
				return config.accessToken;
			}
		} catch (error) {
			console.error(
				`‚ùå Error loading access token from ${configPath}:`,
				error instanceof Error ? error.message : String(error),
			);
		}
	}
	return undefined;
}

// Read package.json to get the version
const packageJsonPath = join(__dirname, "..", "package.json");
const packageJson = JSON.parse(readFileSync(packageJsonPath, "utf-8"));
const appVersion = packageJson.version;

const program = new Command();

program
	.name("telegraph-publisher")
	.description(
		"A CLI tool to publish Markdown content to Telegra.ph with metadata management and dependency resolution."
	)
	.version(appVersion);

// Add enhanced commands first (these are the new primary commands)
EnhancedCommands.addPublishCommand(program);
EnhancedCommands.addAnalyzeCommand(program);
EnhancedCommands.addConfigCommand(program);
EnhancedCommands.addStatusCommand(program);
EnhancedCommands.addResetCommand(program);
EnhancedCommands.addCheckLinksCommand(program);

// Keep original publish command as legacy support with enhanced workflow
program
	.command("publish-legacy")
	.description("Legacy publish command with enhanced workflow (use 'pub' for full features)")
	.option("-f, --file <path>", "Path to the Markdown file to publish")
	.option(
		"-t, --title <title>",
		"Title of the article (defaults to filename, or first heading in file if found)",
	)
	.option("-a, --author <name>", "Author's name")
	.option("-u, --author-url <url>", "Author's URL")
	.option(
		"--dry-run",
		"Process the file but do not publish to Telegra.ph, showing the resulting Telegraph Nodes (JSON).",
	)
	.option(
		"--token <token>",
		"Access token for your Telegra.ph account (optional, will try to load from/save to config file)",
	)
	.action(async (options) => {
		try {
			// Convert legacy options to enhanced command format
			const enhancedOptions = {
				file: options.file,
				author: options.author,
				title: options.title,
				authorUrl: options.authorUrl,
				dryRun: options.dryRun,
				token: options.token,
				withDependencies: false, // Legacy doesn't auto-publish dependencies
				forceRepublish: false,
				verbose: false
			};

			console.log("üîÑ Running legacy command with enhanced workflow...");

			// Use the enhanced command handler but with legacy parameters
			await EnhancedCommands.handleUnifiedPublishCommand(enhancedOptions);
		} catch (error) {
			console.error(
				"‚ùå Error in legacy publish:",
				error instanceof Error ? error.message : String(error),
			);
			process.exit(1);
		}
	});

program
	.command("clean-md")
	.description("Remove Markdown formatting from a file")
	.argument("<file-path>", "Path to the Markdown file to clean")
	.action((filePath) => {
		cleanMarkdownFile(filePath);
	});

program
	.command("list-pages")
	.description("List published pages on Telegra.ph for a given access token")
	.option(
		"-t, --token <token>",
		"Access token for your Telegra.ph account (optional, will try to load from config file)",
	)
	.action(async (options) => {
		try {
			const publisher = new TelegraphPublisher();
			let accessToken: string | undefined = options.token;
			if (!accessToken) {
				accessToken = loadAccessToken(process.cwd());
			}

			if (!accessToken) {
				console.error(
					"‚ùå Error: Access token is required. Please provide it via --token or ensure it's in the config file.",
				);
				process.exit(1);
			}
			publisher.setAccessToken(accessToken);

			console.log("üîé Fetching page list from Telegra.ph...");
			const pageList = await publisher.listPages();

			if (pageList.pages.length === 0) {
				console.log("ü§∑ No pages found for this access token.");
				return;
			}

			console.log(`
üìÑ Found ${pageList.total_count} pages:
`);
			pageList.pages.forEach((page, index) => {
				console.log(`${index + 1}. Title: ${page.title}`);
				console.log(`   URL: ${page.url}`);
				console.log(`   Path: ${page.path}`);
				if (page.author_name) {
					console.log(`   Author: ${page.author_name}`);
				}
				console.log(""); // Empty line for readability
			});
		} catch (error) {
			console.error(
				"‚ùå Error listing pages:",
				error instanceof Error ? error.message : String(error),
			);
			process.exit(1);
		}
	});



// Add help command with examples
program
	.command("help-examples")
	.description("Show usage examples")
	.action(() => {
		console.log(`
üìö Telegraph Publisher - Usage Examples

üöÄ Unified Publishing (Recommended):
   telegraph-publisher pub -f article.md -a "Author Name"

üìù Create and Publish New File:
   telegraph-publisher pub -f new-article.md -a "Author" --title "My Article"

üîó Publish with Dependencies:
   telegraph-publisher pub -f main.md -a "Author" --with-dependencies

üìÅ Publish Entire Directory:
   telegraph-publisher pub -a "Author"

üîÑ Force Republish:
   telegraph-publisher pub -f article.md -a "Author" --force-republish

üëÅÔ∏è Dry Run (Preview):
   telegraph-publisher pub -f article.md -a "Author" --dry-run

üìä Analyze Dependencies:
   telegraph-publisher analyze -f article.md --show-tree

‚öôÔ∏è Configuration Management:
   telegraph-publisher config --show
   telegraph-publisher config --username "Default Author"
   telegraph-publisher config --max-depth 10

üìã Check Status:
   telegraph-publisher status -f article.md

üîß Legacy Commands (with enhanced workflow):
   telegraph-publisher publish-legacy -f article.md -a "Author"
   telegraph-publisher list-pages --token YOUR_TOKEN

üîß First Time Setup:
   1. telegraph-publisher config --username "Your Name"
   2. telegraph-publisher pub -f your-file.md --token YOUR_TOKEN

üí° Tips:
   ‚Ä¢ Access token is saved automatically after first use
   ‚Ä¢ YAML front-matter is added to published files automatically (pub, publish-legacy)
   ‚Ä¢ Local links are replaced with Telegraph URLs in published content
   ‚Ä¢ Legacy commands now use enhanced workflow with metadata management
   ‚Ä¢ Unified 'pub' command creates/updates metadata and manages cache automatically
   ‚Ä¢ Use 'pub' for full features, 'publish-legacy' for backward-compatible interface

üìñ For more information, visit: https://github.com/your-repo/telegraph-publisher
`);
	});

// Error handling for unknown commands
program.on("command:*", () => {
	console.error(
		"‚ùå Error: Invalid command: %s\nSee --help for a list of available commands.",
		program.args.join(" "),
	);
	process.exit(1);
});

// Show help if no command provided
if (process.argv.length <= 2) {
	program.help();
}

program.parse(process.argv);

```

`src/integration.test.ts`

```ts
import { describe, expect, test } from "bun:test";
import { cleanMarkdownString } from "./clean_mr";
import {
	convertMarkdownToTelegraphNodes,
	extractTitleAndContent,
	validateCleanedContent,
} from "./markdownConverter";

// Mock content for end-to-end and integration tests
const mockMarkdownContent = `
# My Test Article

This is some **bold text** and *italic text*.

> This is a blockquote.

## Subheading

- List item 1
- List item 2

\`\`\`typescript
const a = 1;
const b = 2;
\`\`\`

Another paragraph.
`;

describe("Integration Tests", () => {
	test("end-to-end: should extract title, convert markdown, and retain formatting", () => {
		// 1. Extract title and content
		const { title: extractedTitle, content: remainingContent } =
			extractTitleAndContent(mockMarkdownContent);
		expect(extractedTitle).toBe("My Test Article");

		// 2. Clean the extracted title
		const cleanedTitle = cleanMarkdownString(extractedTitle || "");
		expect(cleanedTitle).toBe("My Test Article");

		// 3. Validate remaining content for unwanted HTML tags (should pass as it's pure markdown)
		expect(() => validateCleanedContent(remainingContent)).not.toThrow();

		// 4. Convert remaining content to Telegraph Nodes
		const telegraphNodes = convertMarkdownToTelegraphNodes(remainingContent);

		// 5. Assertions on Telegraph Nodes structure and content
		expect(telegraphNodes).toBeInstanceOf(Array);
		expect(telegraphNodes.length).toBeGreaterThan(0);

		// Check for presence of expected elements based on original markdown formatting
		// Paragraph with bold and italic text
		const firstParagraph = telegraphNodes[0];
		expect(firstParagraph).toEqual({
			tag: "p",
			children: [
				"This is some ",
				{ tag: "strong", children: ["bold text"] },
				" and ",
				{ tag: "em", children: ["italic text"] },
				".",
			],
		});

		// Blockquote
		const blockquote = telegraphNodes[1];
		expect(blockquote).toEqual({
			tag: "blockquote",
			children: ["This is a blockquote."],
		});

		// Subheading (H2) converted to h3 for Telegraph API compatibility
		const subheading = telegraphNodes[2];
		expect(subheading).toEqual({
			tag: "h3",
			children: ["Subheading"],
		});

		// List items
		const ulElement = telegraphNodes[3];
		expect(ulElement).toEqual({
			tag: "ul",
			children: [
				{ tag: "li", children: ["List item 1"] },
				{ tag: "li", children: ["List item 2"] },
			],
		});

		// Code block
		const preElement = telegraphNodes[4];
		expect(preElement).toEqual({
			tag: "pre",
			children: [
				{
					tag: "code",
					children: ["const a = 1;\nconst b = 2;"],
				},
			],
		});

		// Last paragraph
		const lastParagraph = telegraphNodes[5];
		expect(lastParagraph).toEqual({
			tag: "p",
			children: ["Another paragraph."],
		});

		// Ensure no unexpected raw HTML tags are present (validateCleanedContent already checks this)
		const hasUnexpectedRawHtml =
			JSON.stringify(telegraphNodes).includes("<") &&
			!JSON.stringify(telegraphNodes).includes("<blockquote>"); // Basic check for raw HTML tags that shouldn't be there
		expect(hasUnexpectedRawHtml).toBe(false);
	});

	test("end-to-end: should convert tables to nested lists", () => {
		const tableMarkdown = `# –¢–∞–±–ª–∏—Ü–∞ —Ç–æ–≤–∞—Ä–æ–≤

| –ù–∞–∑–≤–∞–Ω–∏–µ | –¶–µ–Ω–∞ | –í –Ω–∞–ª–∏—á–∏–∏ |
|----------|------|-----------|
| –¢–æ–≤–∞—Ä 1  | 100  | –î–∞        |
| –¢–æ–≤–∞—Ä 2  | 200  | –ù–µ—Ç       |

–ö–æ–Ω–µ—Ü —Ç–∞–±–ª–∏—Ü—ã.`;

		// 1. Extract title and content
		const { title: extractedTitle, content: remainingContent } =
			extractTitleAndContent(tableMarkdown);
		expect(extractedTitle).toBe("–¢–∞–±–ª–∏—Ü–∞ —Ç–æ–≤–∞—Ä–æ–≤");

		// 2. Clean the extracted title
		const cleanedTitle = cleanMarkdownString(extractedTitle || "");
		expect(cleanedTitle).toBe("–¢–∞–±–ª–∏—Ü–∞ —Ç–æ–≤–∞—Ä–æ–≤");

		// 3. Validate remaining content
		expect(() => validateCleanedContent(remainingContent)).not.toThrow();

		// 4. Convert remaining content to Telegraph Nodes
		const telegraphNodes = convertMarkdownToTelegraphNodes(remainingContent);

		// 5. Verify table conversion
		expect(telegraphNodes).toEqual([
			{
				tag: "ol",
				children: [
					{
						tag: "li",
						children: [
							"1",
							{
								tag: "ul",
								children: [
									{ tag: "li", children: ["–ù–∞–∑–≤–∞–Ω–∏–µ: –¢–æ–≤–∞—Ä 1"] },
									{ tag: "li", children: ["–¶–µ–Ω–∞: 100"] },
									{ tag: "li", children: ["–í –Ω–∞–ª–∏—á–∏–∏: –î–∞"] },
								],
							},
						],
					},
					{
						tag: "li",
						children: [
							"2",
							{
								tag: "ul",
								children: [
									{ tag: "li", children: ["–ù–∞–∑–≤–∞–Ω–∏–µ: –¢–æ–≤–∞—Ä 2"] },
									{ tag: "li", children: ["–¶–µ–Ω–∞: 200"] },
									{ tag: "li", children: ["–í –Ω–∞–ª–∏—á–∏–∏: –ù–µ—Ç"] },
								],
							},
						],
					},
				],
			},
			{
				tag: "p",
				children: ["–ö–æ–Ω–µ—Ü —Ç–∞–±–ª–∏—Ü—ã."],
			},
		]);
	});
});

```

`src/markdownConverter.numberedHeadings.test.ts`

```ts
import { expect, test } from "bun:test";
import { convertMarkdownToTelegraphNodes } from "./markdownConverter";

// Fix for issue: numbered headings should be parsed as headings, not list items
test("should correctly parse numbered headings as h3 tags instead of list items", () => {
  const markdown = "## 1. My Numbered Heading";
  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["1. My Numbered Heading"] }
  ]);

  // Ensure it's NOT parsed as a list
  expect(result).not.toEqual([
    {
      tag: "ol",
      children: [
        { tag: "li", children: ["My Numbered Heading"] }
      ]
    }
  ]);
});

test("should correctly parse multiple numbered headings with different levels", () => {
  const markdown = "## 1. First Section\n### 2. Subsection\n#### 3. Sub-subsection";
  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["1. First Section"] },
    { tag: "h3", children: ["2. Subsection"] },
    { tag: "h4", children: ["3. Sub-subsection"] }
  ]);
});

test("should parse numbered headings while preserving normal list functionality", () => {
  const markdown = "## 1. Heading\n\n1. First item\n2. Second item";
  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["1. Heading"] },
    {
      tag: "ol",
      children: [
        { tag: "li", children: ["First item"] },
        { tag: "li", children: ["Second item"] }
      ]
    }
  ]);
});

test("should correctly parse the specific user example: ## 1. –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏", () => {
  const markdown = "## 1. –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏";
  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["1. –ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏"] }
  ]);

  // Explicitly ensure it's NOT parsed as ordered list
  expect(result[0]?.tag).not.toBe("ol");
  expect(result).not.toEqual([
    {
      tag: "ol",
      children: [
        { tag: "li", children: ["–ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏"] }
      ]
    }
  ]);
});

test("should parse numbered headings with various formats", () => {
  const markdown = "# 10. Heading level 1\n## 2.5 Heading level 2\n### 100. Complex numbered heading";
  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["10. Heading level 1"] },
    { tag: "h3", children: ["2.5 Heading level 2"] },
    { tag: "h3", children: ["100. Complex numbered heading"] }
  ]);
});

test("should parse mixed content with numbered headings and lists", () => {
  const markdown = `## 1. Introduction

This is a paragraph.

### 2. Topics

Here's a list:
- First topic
- Second topic

#### 3. Conclusion

More numbered content:
1. Summary point
2. Final thoughts`;

  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["1. Introduction"] },
    { tag: "p", children: ["This is a paragraph."] },
    { tag: "h3", children: ["2. Topics"] },
    { tag: "p", children: ["Here's a list:"] },
    {
      tag: "ul",
      children: [
        { tag: "li", children: ["First topic"] },
        { tag: "li", children: ["Second topic"] }
      ]
    },
    { tag: "h4", children: ["3. Conclusion"] },
    { tag: "p", children: ["More numbered content:"] },
    {
      tag: "ol",
      children: [
        { tag: "li", children: ["Summary point"] },
        { tag: "li", children: ["Final thoughts"] }
      ]
    }
  ]);
});

test("should parse headings starting with numbers but not numbered format", () => {
  const markdown = "## 123abc Not a numbered heading\n## 1st Position heading";
  const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

  expect(result).toEqual([
    { tag: "h3", children: ["123abc Not a numbered heading"] },
    { tag: "h3", children: ["1st Position heading"] }
  ]);
});

test("should ensure numbered headings take precedence over list parsing", () => {
  // This is the core test to ensure the fix works
  const testCases = [
    "# 1. Level 1 heading",
    "## 2. Level 2 heading",
    "### 3. Level 3 heading",
    "#### 4. Level 4 heading",
    "##### 5. Level 5 heading",
    "###### 6. Level 6 heading"
  ];

  testCases.forEach((markdown, index) => {
    const result = convertMarkdownToTelegraphNodes(markdown, { generateToc: false });

    // All should be parsed as headings, not lists
    expect(result).toHaveLength(1);
    expect(result[0]?.tag).not.toBe("ol");
    expect(result[0]?.tag).not.toBe("ul");

    // Should contain the full text including the number
    const expectedText = `${index + 1}. Level ${index + 1} heading`;
    if (result[0]?.children && typeof result[0].children[0] === 'string') {
      expect(result[0].children[0]).toContain(expectedText);
    } else if (Array.isArray(result[0]?.children)) {
      // Handle cases where children might be processed as inline markdown
      const textContent = extractTextContent(result[0].children);
      expect(textContent).toContain(expectedText);
    }
  });
});

// Helper function to extract text content from potentially nested children
function extractTextContent(children: any[]): string {
  return children.map(child => {
    if (typeof child === 'string') {
      return child;
    } else if (child && child.children) {
      return extractTextContent(child.children);
    }
    return '';
  }).join('');
}
```

`src/markdownConverter.parentheses-bug.test.ts`

```ts
import { describe, it, expect } from 'bun:test';
import { convertMarkdownToTelegraphNodes } from './markdownConverter';

/**
 * Test for Telegraph JSON generation bug with parentheses in link anchors
 */
describe('MarkdownConverter - Telegraph JSON Parentheses Bug', () => {
  it('should reproduce the issue with orphaned parentheses in Telegraph JSON', () => {
    // Test case based on user's provided content that causes the issue
    const testMarkdown = `## [–ê–Ω–∞–ª–æ–≥–∏–∏](./–∞–Ω–∞–ª–æ–≥–∏–∏.md)

- [–ê–Ω–∞–ª–∏–∑ –∞–Ω–∞–ª–æ–≥–∏–π –∏–∑ –®—Ä–∏–º–∞–¥-–ë—Ö–∞–≥–∞–≤–∞—Ç–∞–º 1.1](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#–ê–Ω–∞–ª–∏–∑-–∞–Ω–∞–ª–æ–≥–∏–π-–∏–∑-–®—Ä–∏–º–∞–¥-–ë—Ö–∞–≥–∞–≤–∞—Ç–∞–º-1.1)
- [1. –ê–Ω–∞–ª–æ–≥–∏—è ¬´–î–µ—Ä–µ–≤–æ —Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.4)](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4))
- [2. –ê–Ω–∞–ª–æ–≥–∏—è ¬´–ö–∏–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞¬ª (–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∫ –®–ë 1.1.17)](./–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17))`;

    console.log('=== TESTING TELEGRAPH JSON GENERATION ===');
    const nodes = convertMarkdownToTelegraphNodes(testMarkdown, { generateToc: false });

    console.log('Generated Telegraph Nodes:');
    console.log(JSON.stringify(nodes, null, 2));

    // Helper function to find orphaned parentheses
    function findOrphanedParentheses(nodes: any[]): string[] {
      const orphans: string[] = [];
      
      function traverse(node: any) {
        if (typeof node === 'string' && node.trim() === ')') {
          orphans.push(')');
        } else if (Array.isArray(node)) {
          node.forEach(traverse);
        } else if (typeof node === 'object' && node !== null) {
          if (node.children) {
            traverse(node.children);
          }
        }
      }
      
      traverse(nodes);
      return orphans;
    }

    // Helper function to extract href values
    function extractHrefs(nodes: any[]): string[] {
      const hrefs: string[] = [];
      
      function traverse(node: any) {
        if (Array.isArray(node)) {
          node.forEach(traverse);
        } else if (typeof node === 'object' && node !== null) {
          if (node.tag === 'a' && node.attrs && node.attrs.href) {
            hrefs.push(node.attrs.href);
          }
          if (node.children) {
            traverse(node.children);
          }
        }
      }
      
      traverse(nodes);
      return hrefs;
    }

    const orphans = findOrphanedParentheses(nodes);
    const hrefs = extractHrefs(nodes);

    console.log('\n=== ANALYSIS ===');
    console.log(`Found ${orphans.length} orphaned parentheses:`, orphans);
    
    console.log('\nExtracted href values:');
    hrefs.forEach((href, i) => {
      const shouldEndWithParen = href.includes('-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4') || href.includes('-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17');
      const actuallyEndsWithParen = href.endsWith(')');
      const isCorrect = !shouldEndWithParen || actuallyEndsWithParen;
      console.log(`${i + 1}. ${href} ${isCorrect ? '‚úÖ' : '‚ùå INCOMPLETE'}`);
    });

    // Test assertions for FIXED behavior
    // After fix, there should be NO orphaned parentheses
    expect(orphans.length).toBe(0); // Fixed: no orphaned parentheses

    // After fix, hrefs should be COMPLETE 
    const problematicHrefs = hrefs.filter(href => 
      (href.includes('-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4') && !href.endsWith(')')) ||
      (href.includes('-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17') && !href.endsWith(')'))
    );
    expect(problematicHrefs.length).toBe(0); // Fixed: all hrefs should be complete

    // Verify specific problematic hrefs are now complete
    const expectedCompleteHrefs = [
      './–∞–Ω–∞–ª–æ–≥–∏–∏.md#1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4)',
      './–∞–Ω–∞–ª–æ–≥–∏–∏.md#2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17)'
    ];
    
    console.log('\nActual hrefs found:');
    hrefs.forEach((href, i) => console.log(`${i + 1}. "${href}"`));
    
    // Check that hrefs with parentheses end correctly
    const hrefs1 = hrefs.filter(h => h.includes('1.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–î–µ—Ä–µ–≤–æ-—Ü–∏–≤–∏–ª–∏–∑–∞—Ü–∏–∏¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.4'));
    const hrefs2 = hrefs.filter(h => h.includes('2.-–ê–Ω–∞–ª–æ–≥–∏—è-¬´–ö–∏–Ω–æ-–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ-–º–∏—Ä–∞¬ª-(–∏–∑-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è-–∫-–®–ë-1.1.17'));
    
    expect(hrefs1.length).toBe(1);
    expect(hrefs2.length).toBe(1);
    expect(hrefs1[0]).toEndWith(')');
    expect(hrefs2[0]).toEndWith(')');

    console.log('\n=== FIX VALIDATED ===');
    console.log(`Orphaned parentheses found: ${orphans.length} (should be 0)`);
    console.log(`Incomplete hrefs found: ${problematicHrefs.length} (should be 0)`);
  });

  it('should handle simple links correctly (regression test)', () => {
    // Simple link in a paragraph context (more realistic)
    const simpleMarkdown = `Here is a [Simple link](./file.md) in text.`;
    const nodes = convertMarkdownToTelegraphNodes(simpleMarkdown, { generateToc: false });

    console.log('Simple link test nodes:', JSON.stringify(nodes, null, 2));

    // Extract href values
    function extractHrefs(nodes: any[]): string[] {
      const hrefs: string[] = [];
      
      function traverse(node: any) {
        if (Array.isArray(node)) {
          node.forEach(traverse);
        } else if (typeof node === 'object' && node !== null) {
          if (node.tag === 'a' && node.attrs && node.attrs.href) {
            hrefs.push(node.attrs.href);
          }
          if (node.children) {
            traverse(node.children);
          }
        }
      }
      
      traverse(nodes);
      return hrefs;
    }

    const hrefs = extractHrefs(nodes);
    expect(hrefs).toHaveLength(1);
    expect(hrefs[0]).toBe('./file.md');
  });
});
```

`src/markdownConverter.test.ts`

```ts
import { expect, test } from "bun:test";
import { cleanMarkdownString } from "./clean_mr";
import {
	convertMarkdownToTelegraphNodes,
	extractTitleAndContent,
	validateCleanedContent,
} from "./markdownConverter";

test("should convert simple paragraph to Telegraph node", () => {
	const markdown = "Hello, World!";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([{ tag: "p", children: ["Hello, World!"] }]);
});

test("should convert headings to Telegraph API compatible nodes", () => {
	const markdown = "# Heading 1\n## Heading 2\n### Heading 3\n#### Heading 4";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		// ToC should be generated first (4 headings = 2+ requirement met)
		{
			tag: "aside",
			children: [{
				tag: "ul",
				children: [
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#Heading-1" }, children: ["Heading 1"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#Heading-2" }, children: ["Heading 2"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#Heading-3" }, children: ["Heading 3"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#Heading-4" }, children: ["Heading 4"] }] }
				]
			}]
		},
		{ tag: "h3", children: ["Heading 1"] }, // H1 ‚Üí h3
		{ tag: "h3", children: ["Heading 2"] }, // H2 ‚Üí h3
		{ tag: "h3", children: ["Heading 3"] }, // H3 ‚Üí h3
		{ tag: "h4", children: ["Heading 4"] }, // H4 ‚Üí h4
	]);
});

test("should convert H5/H6 headings to h4 with visual prefixes for anchor support", () => {
	const markdown = "##### Heading 5\n###### Heading 6";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		// ToC should be generated first (2 headings = 2+ requirement met)
		{
			tag: "aside",
			children: [{
				tag: "ul",
				children: [
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª-Heading-5" }, children: ["¬ª Heading 5"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª-Heading-6" }, children: ["¬ª¬ª Heading 6"] }] }
				]
			}]
		},
		{ tag: "h4", children: ["¬ª Heading 5"] }, // H5 ‚Üí h4 with ¬ª prefix
		{ tag: "h4", children: ["¬ª¬ª Heading 6"] }, // H6 ‚Üí h4 with ¬ª¬ª prefix
	]);
});

test("should handle all heading levels comprehensively", () => {
	const markdown = "# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n####### H7+";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		// ToC should be generated first (7 headings = 2+ requirement met)
		{
			tag: "aside",
			children: [{
				tag: "ul",
				children: [
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#H1" }, children: ["H1"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#H2" }, children: ["H2"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#H3" }, children: ["H3"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#H4" }, children: ["H4"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª-H5" }, children: ["¬ª H5"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª-H6" }, children: ["¬ª¬ª H6"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª¬ª-H7+" }, children: ["¬ª¬ª¬ª H7+"] }] }
				]
			}]
		},
		{ tag: "h3", children: ["H1"] },
		{ tag: "h3", children: ["H2"] },
		{ tag: "h3", children: ["H3"] },
		{ tag: "h4", children: ["H4"] },
		{ tag: "h4", children: ["¬ª H5"] },
		{ tag: "h4", children: ["¬ª¬ª H6"] },
		{ tag: "h4", children: ["¬ª¬ª¬ª H7+"] }, // Edge case: H7+ ‚Üí h4 with ¬ª¬ª¬ª prefix
	]);
});

test("should preserve inline formatting in H5/H6 with prefixes", () => {
	const markdown = "##### **Bold** H5 with *italic*\n###### Link [text](url) in H6";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		// ToC should be generated first (2 headings = 2+ requirement met)
		// Note: ToC uses processInlineMarkdown for children to render formatting
		{
			tag: "aside",
			children: [{
				tag: "ul",
				children: [
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª-**Bold**-H5-with-*italic*" }, children: ["¬ª ", { tag: "strong", children: ["Bold"] }, " H5 with ", { tag: "em", children: ["italic"] }] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª-Link-[text](url)-in-H6" }, children: ["¬ª¬ª Link ", { tag: "a", attrs: { href: "url" }, children: ["text"] }, " in H6"] }] }
				]
			}]
		},
		{
			tag: "h4",
			children: [
				"¬ª ",
				{ tag: "strong", children: ["Bold"] },
				" H5 with ",
				{ tag: "em", children: ["italic"] }
			]
		},
		{
			tag: "h4",
			children: [
				"¬ª¬ª Link ",
				{ tag: "a", attrs: { href: "url" }, children: ["text"] },
				" in H6"
			]
		}
	]);
});

test("should generate proper anchors for H5/H6 headings - integration test", () => {
	// This test verifies that H5/H6 headings with prefixes generate correct anchors
	// using the updated generateSlug function from LinkVerifier
	
	const markdown = `# Regular Heading
##### Important Section
###### Sub Important Section
##### –ú–æ–π —Ä–∞–∑–¥–µ–ª
###### Section with @#$% Special Characters!`;

	const result = convertMarkdownToTelegraphNodes(markdown);

	// Should include ToC as first element since there are 5 headings (2+ requirement met)
	expect(result).toEqual([
		// ToC aside element should be first
		{
			tag: "aside",
			children: [{
				tag: "ul",
				children: [
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#Regular-Heading" }, children: ["Regular Heading"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª-Important-Section" }, children: ["¬ª Important Section"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª-Sub-Important-Section" }, children: ["¬ª¬ª Sub Important Section"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª-–ú–æ–π-—Ä–∞–∑–¥–µ–ª" }, children: ["¬ª –ú–æ–π —Ä–∞–∑–¥–µ–ª"] }] },
					{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª-Section-with-@#$%-Special-Characters!" }, children: ["¬ª¬ª Section with @#$% Special Characters!"] }] }
				]
			}]
		},
		// Then the actual headings
		{ tag: "h3", children: ["Regular Heading"] }, // Should generate anchor: "Regular-Heading"
		{ tag: "h4", children: ["¬ª Important Section"] }, // Should generate anchor: "¬ª-Important-Section"
		{ tag: "h4", children: ["¬ª¬ª Sub Important Section"] }, // Should generate anchor: "¬ª¬ª-Sub-Important-Section"
		{ tag: "h4", children: ["¬ª –ú–æ–π —Ä–∞–∑–¥–µ–ª"] }, // Should generate anchor: "¬ª-–ú–æ–π-—Ä–∞–∑–¥–µ–ª"
		{ tag: "h4", children: ["¬ª¬ª Section with @#$% Special Characters!"] }, // Should generate anchor: "¬ª¬ª-Section-with-@#$%-Special-Characters!"
	]);
});

test("should generate Table of Contents for documents with 2+ headings", () => {
	const markdown = `# Main Title
## Section One
### Subsection
#### Details`;

	const result = convertMarkdownToTelegraphNodes(markdown);

	// First element should be ToC aside
	expect(result[0]).toEqual({
		tag: "aside",
		children: [{
			tag: "ul",
			children: [
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#Main-Title" }, children: ["Main Title"] }] },
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#Section-One" }, children: ["Section One"] }] },
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#Subsection" }, children: ["Subsection"] }] },
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#Details" }, children: ["Details"] }] }
			]
		}]
	});

	// Should have 5 total elements: 1 ToC + 4 headings
	expect(result).toHaveLength(5);
});

test("should NOT generate ToC for documents with fewer than 2 headings", () => {
	const markdownOne = `# Single Heading
Some content here.`;

	const markdownNone = `Just regular content.
No headings at all.`;

	const resultOne = convertMarkdownToTelegraphNodes(markdownOne);
	const resultNone = convertMarkdownToTelegraphNodes(markdownNone);

	// Should not include ToC (no aside element)
	expect(resultOne).toEqual([
		{ tag: "h3", children: ["Single Heading"] },
		{ tag: "p", children: ["Some content here."] }
	]);

	expect(resultNone).toEqual([
		{ tag: "p", children: ["Just regular content."] },
		{ tag: "p", children: ["No headings at all."] }
	]);
});

test("should handle ToC with H5/H6 prefixed headings correctly", () => {
	const markdown = `## Introduction
##### Important Note
###### Warning`;

	const result = convertMarkdownToTelegraphNodes(markdown);

	// Should generate ToC with prefixed headings
	expect(result[0]).toEqual({
		tag: "aside",
		children: [{
			tag: "ul",
			children: [
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#Introduction" }, children: ["Introduction"] }] },
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª-Important-Note" }, children: ["¬ª Important Note"] }] },
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#¬ª¬ª-Warning" }, children: ["¬ª¬ª Warning"] }] }
			]
		}]
	});
});

test("should handle ToC with Unicode and special characters", () => {
	const markdown = `# –¢–µ—Å—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫
## Special @#$% Characters!`;

	const result = convertMarkdownToTelegraphNodes(markdown);

	// Should preserve Unicode and special characters in ToC anchors
	expect(result[0]).toEqual({
		tag: "aside",
		children: [{
			tag: "ul",
			children: [
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#–¢–µ—Å—Ç-–∑–∞–≥–æ–ª–æ–≤–æ–∫" }, children: ["–¢–µ—Å—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫"] }] },
				{ tag: "li", children: [{ tag: "a", attrs: { href: "#Special-@#$%-Characters!" }, children: ["Special @#$% Characters!"] }] }
			]
		}]
	});
});

test("should convert bold text to strong nodes", () => {
	const markdown = "This is **bold** text and __also bold__.";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "p",
			children: [
				"This is ",
				{ tag: "strong", children: ["bold"] },
				" text and ",
				{ tag: "strong", children: ["also bold"] },
				".",
			],
		},
	]);
});

test("should convert italic text to em nodes", () => {
	const markdown = "This is *italic* text and _also italic_.";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "p",
			children: [
				"This is ",
				{ tag: "em", children: ["italic"] },
				" text and ",
				{ tag: "em", children: ["also italic"] },
				".",
			],
		},
	]);
});

test("should convert blockquotes to blockquote nodes", () => {
	const markdown =
		"> This is a blockquote\n> with multiple lines\n\nAnother paragraph after quote.";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "blockquote",
			children: ["This is a blockquote\nwith multiple lines"],
		},
		{ tag: "p", children: ["Another paragraph after quote."] },
	]);
});

test("should convert unordered lists to ul nodes", () => {
	const markdown = "- List item 1\n* List item 2";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "ul",
			children: [
				{ tag: "li", children: ["List item 1"] },
				{ tag: "li", children: ["List item 2"] },
			],
		},
	]);
});

test("should convert ordered lists to ol nodes", () => {
	const markdown = "1. First item\n2. Second item";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "ol",
			children: [
				{ tag: "li", children: ["First item"] },
				{ tag: "li", children: ["Second item"] },
			],
		},
	]);
});

test("should convert inline code to code nodes", () => {
	const markdown = "This is `inline code` in text.";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "p",
			children: [
				"This is ",
				{ tag: "code", children: ["inline code"] },
				" in text.",
			],
		},
	]);
});

test("should convert links to anchor nodes", () => {
	const markdown = "Visit [Google](https://google.com) for search.";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "p",
			children: [
				"Visit ",
				{
					tag: "a",
					attrs: { href: "https://google.com" },
					children: ["Google"],
				},
				" for search.",
			],
		},
	]);
});

test("should handle complex nested markdown", () => {
	const markdown =
		"## **Bold Heading**\n\nThis is a paragraph with **bold** and *italic* text.\n\n- List item with `code`\n- Another item";
	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result.length).toBe(3);
	expect(result[0]?.tag).toBe("h3"); // H2 ‚Üí h3 for Telegraph API compatibility
	expect(result[1]?.tag).toBe("p");
	expect(result[2]?.tag).toBe("ul");
});

test("extractTitleAndContent: should extract H1 title and remaining content", () => {
	const markdown = "# My Title\n\nContent goes here.";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBe("My Title");
	expect(content).toBe("\nContent goes here.");
});

test("extractTitleAndContent: should extract H2 title and remaining content", () => {
	const markdown = "## My Subtitle\nContent.";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBe("My Subtitle");
	expect(content).toBe("Content.");
});

test("extractTitleAndContent: should return null title if no heading is found as first non-empty line", () => {
	const markdown = "Just plain text.\nAnother line.";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBeNull();
	expect(content).toBe("Just plain text.\nAnother line.");
});

test("extractTitleAndContent: should handle leading empty lines before heading", () => {
	const markdown = "\n\n# Title\nContent";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBe("Title");
	expect(content).toBe("Content");
});

test("extractTitleAndContent: should handle leading empty lines before non-heading content", () => {
	const markdown = "\n\nPlain text.";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBeNull();
	expect(content).toBe("Plain text.");
});

test("extractTitleAndContent: should handle empty markdown string", () => {
	const markdown = "";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBeNull();
	expect(content).toBe("");
});

test("extractTitleAndContent: should remove leading empty lines from content after title extraction", () => {
	const markdown = "# Title\n\n\nContent";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBe("Title");
	expect(content).toBe("\n\nContent");
});

test("extractTitleAndContent: should extract title with inline formatting and remove it from content", () => {
	const markdown = "# My **Bold** *Title*\nContent.";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBe("My **Bold** *Title*");
	expect(content).toBe("Content.");
});

test("extractTitleAndContent: should extract bold/italic line as title if no heading is found", () => {
	const markdown = "**This is a Bold Title**\nContent here.\n";
	const { title, content } = extractTitleAndContent(markdown);
	expect(title).toBe("This is a Bold Title");
	expect(content).toBe("Content here.\n");

	const markdown2 = "*This is an Italic Title*\nContent here.\n";
	const { title: title2, content: content2 } =
		extractTitleAndContent(markdown2);
	expect(title2).toBe("This is an Italic Title");
	expect(content2).toBe("Content here.\n");

	const markdown3 = "__Another Bold Title__\nContent here.";
	const { title: title3, content: content3 } =
		extractTitleAndContent(markdown3);
	expect(title3).toBe("Another Bold Title");
	expect(content3).toBe("Content here.");

	const markdown4 = "_Another Italic Title_\nContent here.";
	const { title: title4, content: content4 } =
		extractTitleAndContent(markdown4);
	expect(title4).toBe("Another Italic Title");
	expect(content4).toBe("Content here.");
});

test("cleanMarkdownString: should remove only heading and inline markdown formatting from a title string", () => {
	const titleString =
		"# My **Bold** *Title* `with code` [and link](http://example.com).";
	const expectedCleanTitle = "My Bold Title with code and link.";
	const cleanedTitle = cleanMarkdownString(titleString);
	expect(cleanedTitle).toBe(expectedCleanTitle);
});

test("cleanMarkdownString: should handle a title string with no markdown", () => {
	const titleString = "A Plain Title";
	const expectedCleanTitle = "A Plain Title";
	const cleanedTitle = cleanMarkdownString(titleString);
	expect(cleanedTitle).toBe(expectedCleanTitle);
});

test("cleanMarkdownString: should handle an empty title string", () => {
	const titleString = "";
	const expectedCleanTitle = "";
	const cleanedTitle = cleanMarkdownString(titleString);
	expect(cleanedTitle).toBe(expectedCleanTitle);
});

test("validateCleanedContent: should pass for content without HTML tags", () => {
	const content =
		"This is plain Markdown content with **bold** text and - list items.";
	expect(() => validateCleanedContent(content)).not.toThrow();
});

test("validateCleanedContent: should throw error for content with HTML tags", () => {
	const content = "This text contains <p>HTML</p> tags.";
	expect(() => validateCleanedContent(content)).toThrow(
		"Content contains unsupported HTML tags. Only Markdown formatting is allowed.",
	);
});

test("validateCleanedContent: should throw error for content with incomplete HTML tags", () => {
	const content = "This text contains <p>incomplete HTML tags.";
	expect(() => validateCleanedContent(content)).toThrow(
		"Content contains unsupported HTML tags. Only Markdown formatting is allowed.",
	);
});

test("validateCleanedContent: should pass for content with angle brackets that are not HTML tags", () => {
	const content = "This text has < and > symbols but no HTML.";
	expect(() => validateCleanedContent(content)).not.toThrow();
});

test("should convert tables to nested lists", () => {
	const markdown = `| –ù–∞–∑–≤–∞–Ω–∏–µ | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|----------|
| –ü–µ—Ä–≤—ã–π   | 100      | –¢–µ—Å—Ç 1   |
| –í—Ç–æ—Ä–æ–π   | 200      | –¢–µ—Å—Ç 2   |
| –¢—Ä–µ—Ç–∏–π   | 300      | –¢–µ—Å—Ç 3   |`;

	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "ol",
			children: [
				{
					tag: "li",
					children: [
						"1",
						{
							tag: "ul",
							children: [
								{ tag: "li", children: ["–ù–∞–∑–≤–∞–Ω–∏–µ: –ü–µ—Ä–≤—ã–π"] },
								{ tag: "li", children: ["–ó–Ω–∞—á–µ–Ω–∏–µ: 100"] },
								{ tag: "li", children: ["–û–ø–∏—Å–∞–Ω–∏–µ: –¢–µ—Å—Ç 1"] },
							],
						},
					],
				},
				{
					tag: "li",
					children: [
						"2",
						{
							tag: "ul",
							children: [
								{ tag: "li", children: ["–ù–∞–∑–≤–∞–Ω–∏–µ: –í—Ç–æ—Ä–æ–π"] },
								{ tag: "li", children: ["–ó–Ω–∞—á–µ–Ω–∏–µ: 200"] },
								{ tag: "li", children: ["–û–ø–∏—Å–∞–Ω–∏–µ: –¢–µ—Å—Ç 2"] },
							],
						},
					],
				},
				{
					tag: "li",
					children: [
						"3",
						{
							tag: "ul",
							children: [
								{ tag: "li", children: ["–ù–∞–∑–≤–∞–Ω–∏–µ: –¢—Ä–µ—Ç–∏–π"] },
								{ tag: "li", children: ["–ó–Ω–∞—á–µ–Ω–∏–µ: 300"] },
								{ tag: "li", children: ["–û–ø–∏—Å–∞–Ω–∏–µ: –¢–µ—Å—Ç 3"] },
							],
						},
					],
				},
			],
		},
	]);
});

test("should handle table with empty cells", () => {
	const markdown = `| –ö–æ–ª–æ–Ω–∫–∞ 1 | –ö–æ–ª–æ–Ω–∫–∞ 2 |
|-----------|-----------|
| –ó–Ω–∞—á–µ–Ω–∏–µ  |           |
|           | –î—Ä—É–≥–æ–µ    |`;

	const result = convertMarkdownToTelegraphNodes(markdown);

	expect(result).toEqual([
		{
			tag: "ol",
			children: [
				{
					tag: "li",
					children: [
						"1",
						{
							tag: "ul",
							children: [
								{ tag: "li", children: ["–ö–æ–ª–æ–Ω–∫–∞ 1: –ó–Ω–∞—á–µ–Ω–∏–µ"] },
								{ tag: "li", children: ["–ö–æ–ª–æ–Ω–∫–∞ 2: "] },
							],
						},
					],
				},
				{
					tag: "li",
					children: [
						"2",
						{
							tag: "ul",
							children: [
								{ tag: "li", children: ["–ö–æ–ª–æ–Ω–∫–∞ 1: "] },
								{ tag: "li", children: ["–ö–æ–ª–æ–Ω–∫–∞ 2: –î—Ä—É–≥–æ–µ"] },
							],
						},
					],
				},
			],
		},
	]);
});

```

`src/markdownConverter.ts`

```ts
import type { TelegraphNode } from "./telegraphPublisher"; // Import TelegraphNode interface

/**
 * Validates cleaned content to ensure it does not contain unsupported syntax, like raw HTML tags.
 * @param content The content string (expected to be Markdown, potentially with HTML, which we want to disallow).
 * @throws Error if unsupported syntax (HTML tags) is found.
 */
export function validateCleanedContent(content: string): void {
	// This regex specifically looks for HTML-like tags, not Markdown syntax.
	// It should detect things like <p>, <div>, <span>, etc.
	const htmlTagRegex = /<\/?\w+\b[^>]*>/;
	if (htmlTagRegex.test(content)) {
		throw new Error(
			"Content contains unsupported HTML tags. Only Markdown formatting is allowed.",
		);
	}
}

/**
 * Extracts the first heading from Markdown content and returns it as a title,
 * along with the remaining content.
 * @param markdown The raw Markdown content.
 * @returns An object containing the extracted title (or null) and the modified content.
 */
export function extractTitleAndContent(markdown: string): {
	title: string | null;
	content: string;
} {
	const lines = markdown.split(/\r?\n/);
	let title: string | null = null;
	let contentStartIndex = 0;

	for (let i = 0; i < lines.length; i++) {
		const line = lines[i]?.trim() || "";
		if (line === "") {
			contentStartIndex++;
			continue; // Skip empty lines at the beginning
		}

		const headingMatch = line.match(/^(#+)\s*(.*)/);
		// Check if the first non-empty line is a heading or bold/italic text that looks like a title
		const boldItalicMatch = line.match(
			/^(?:\*{2}|__)(.*?)(?:\*{2}|__)$|^\*(.*?)\*$|^_(.*?)_$/,
		);

		if (
			headingMatch &&
			headingMatch.length > 2 &&
			headingMatch[2] !== undefined
		) {
			const headingText = headingMatch[2];
			title = headingText.trim();
			contentStartIndex = i + 1;
			break;
		} else if (boldItalicMatch) {
			// If it's a bold/italic line, consider it a title if no heading was found yet
			title = (
				boldItalicMatch[1] ||
				boldItalicMatch[2] ||
				boldItalicMatch[3] ||
				""
			).trim();
			contentStartIndex = i + 1;
			break;
		}
		// If the first non-empty line is not a recognized title format, treat entire content as is
		break;
	}

	const remainingContent = lines.slice(contentStartIndex).join("\n");

	return { title, content: remainingContent };
}

/**
 * Parses a Markdown table and converts it to a nested list structure.
 * @param tableLines Array of lines that form the table
 * @returns TelegraphNode representing the nested list structure
 */
function parseTable(tableLines: string[]): TelegraphNode {
	if (tableLines.length < 3) {
		// Not a valid table, return as paragraphs
		return { tag: "p", children: [tableLines.join("\n")] };
	}

	// Parse header row
	const headerLine = tableLines[0];
	if (!headerLine) {
		return { tag: "p", children: [tableLines.join("\n")] };
	}
	// Split by | but keep empty cells, then remove first and last empty cells
	const allHeaders = headerLine.split("|").map((cell) => cell.trim());
	const headers = allHeaders.slice(1, -1).filter((cell) => cell !== "");

	// Skip separator line (tableLines[1])

	// Parse data rows
	const dataRows = tableLines.slice(2);
	const listItems: TelegraphNode[] = [];

	dataRows.forEach((row, index) => {
		// Split by | but keep empty cells (don't filter them out)
		const allCells = row.split("|").map((cell) => cell.trim());
		// Remove first and last empty cells (they're from leading/trailing |)
		const cells = allCells.slice(1, -1);

		if (cells.length === 0) return; // Skip empty rows

		// Create nested list for this row
		const nestedItems: TelegraphNode[] = [];

		// Use the number of headers as the reference
		const numColumns = headers.length;

		for (let i = 0; i < numColumns; i++) {
			const header = headers[i] || `–ö–æ–ª–æ–Ω–∫–∞ ${i + 1}`;
			const value = cells[i] || "";
			nestedItems.push({
				tag: "li",
				children: [`${header}: ${value}`],
			});
		}

		// Create the main list item with number and nested list
		listItems.push({
			tag: "li",
			children: [
				`${index + 1}`,
				{
					tag: "ul",
					children: nestedItems,
				},
			],
		});
	});

	return {
		tag: "ol",
		children: listItems,
	};
}

/**
 * Generates a Table of Contents (ToC) as an aside element from Markdown content.
 * Only generates ToC if there are 2 or more headings in the document.
 * Uses the same heading processing logic as the main converter for consistency.
 * @param markdown The raw Markdown content to scan for headings.
 * @returns TelegraphNode for aside element with ToC, or null if insufficient headings.
 */
function generateTocAside(markdown: string): TelegraphNode | null {
	const headings: { level: number; text: string; displayText: string; textForAnchor: string }[] = [];
	const lines = markdown.split(/\r?\n/);

	// 1. Scan for all headings using the same regex as main converter
	for (const line of lines) {
		const headingMatch = line.match(/^(#+)\s+(.*)/);
		if (headingMatch?.[1] && headingMatch[2] !== undefined) {
			const level = headingMatch[1].length;
			const originalText = headingMatch[2].trim();
			let textForAnchor = originalText;
			
			// NEW: Check if the heading text is a Markdown link
			const linkInHeadingMatch = originalText.match(/^\[(.*?)\]\((.*?)\)$/);
			if (linkInHeadingMatch) {
				// If it's a link, use only its text part for the anchor
				textForAnchor = linkInHeadingMatch[1] || '';
			}

			let displayText = originalText;

			// 2. Apply the same heading strategy logic as main converter
			switch (level) {
				case 1:
				case 2:
				case 3:
				case 4:
					displayText = originalText;
					break;
				case 5:
					displayText = `¬ª ${originalText}`;
					textForAnchor = linkInHeadingMatch ? `¬ª ${linkInHeadingMatch[1]}` : `¬ª ${originalText}`;
					break;
				case 6:
					displayText = `¬ª¬ª ${originalText}`;
					textForAnchor = linkInHeadingMatch ? `¬ª¬ª ${linkInHeadingMatch[1]}` : `¬ª¬ª ${originalText}`;
					break;
				default:
					// Handle edge case: levels > 6
					displayText = `¬ª¬ª¬ª ${originalText}`;
					textForAnchor = linkInHeadingMatch ? `¬ª¬ª¬ª ${linkInHeadingMatch[1]}` : `¬ª¬ª¬ª ${originalText}`;
					break;
			}

			headings.push({ level, text: originalText, displayText, textForAnchor });
		}
	}

	// 3. Check if ToC should be generated (2+ headings required)
	if (headings.length < 2) {
		return null;
	}

	// 4. Build ToC structure as list items
	const listItems: TelegraphNode[] = [];
	for (const heading of headings) {
		// IMPORTANT: Use textForAnchor for anchor generation to handle link headings properly
		// Based on empirical research: remove only < and > characters, replace spaces with hyphens
		const anchor = heading.textForAnchor
			.trim()
			.replace(/[<>]/g, '') // 1. Remove < and > characters only
			.replace(/ /g, '-');  // 2. Replace spaces with hyphens
		
		const linkNode: TelegraphNode = {
			tag: 'a',
			attrs: { href: `#${anchor}` },
			children: [
				// Use processInlineMarkdown to render formatting in ToC text for better readability
				...processInlineMarkdown(heading.displayText)
			]
		};
		
		listItems.push({
			tag: 'li',
			children: [linkNode],
		});
	}

	// 5. Return aside element with unordered list
	return {
		tag: 'aside',
		children: [{
			tag: 'ul',
			children: listItems
		}]
	};
}

/**
 * Converts Markdown content directly into an array of TelegraphNode objects.
 * This function replaces the need for an intermediate HTML conversion step and 'mrkdwny' library.
 * It directly parses Markdown elements into the structure expected by the Telegra.ph API.
 * @param markdown The raw Markdown content.
 * @returns An array of TelegraphNode objects representing the parsed content.
 */
export function convertMarkdownToTelegraphNodes(
	markdown: string,
	options: { generateToc?: boolean } = { generateToc: true }
): TelegraphNode[] {
	const nodes: TelegraphNode[] = [];
	
	// Generate and add Table of Contents if enabled and there are 2+ headings
	if (options.generateToc !== false) {
		const tocAside = generateTocAside(markdown);
		if (tocAside) {
			nodes.push(tocAside);
		}
	}
	
	const lines = markdown.split(/\r?\n/);

	let inCodeBlock = false;
	let codeBlockContent: string[] = [];

	let inList = false;
	let currentListTag: "ul" | "ol" | "" = "";
	let currentListItems: TelegraphNode[] = [];

	let inBlockquote = false;
	let blockquoteContent: string[] = [];

	let inTable = false;
	let tableLines: string[] = [];

	for (let i = 0; i < lines.length; i++) {
		const line = lines[i] || "";

		// Handle code blocks first (they have highest priority)
		if (line.startsWith("```")) {
			if (inCodeBlock) {
				// End of code block
				// Close any open blocks first
				if (inTable) {
					nodes.push(parseTable(tableLines));
					inTable = false;
					tableLines = [];
				}
				if (inList) {
					nodes.push({ tag: currentListTag, children: currentListItems });
					inList = false;
					currentListItems = [];
				}
				if (inBlockquote) {
					nodes.push({
						tag: "blockquote",
						children: processInlineMarkdown(blockquoteContent.join("\n")),
					});
					inBlockquote = false;
					blockquoteContent = [];
				}

				nodes.push({
					tag: "pre",
					children: [
						{ tag: "code", children: [codeBlockContent.join("\n").trimEnd()] },
					],
				});
				codeBlockContent = [];
				inCodeBlock = false;
			} else {
				// Start of code block
				// Close any open blocks first
				if (inTable) {
					nodes.push(parseTable(tableLines));
					inTable = false;
					tableLines = [];
				}
				if (inList) {
					nodes.push({ tag: currentListTag, children: currentListItems });
					inList = false;
					currentListItems = [];
				}
				if (inBlockquote) {
					nodes.push({
						tag: "blockquote",
						children: processInlineMarkdown(blockquoteContent.join("\n")),
					});
					inBlockquote = false;
					blockquoteContent = [];
				}

				inCodeBlock = true;
			}
			continue;
		}

		if (inCodeBlock) {
			codeBlockContent.push(line);
			continue;
		}

		// Handle table detection and parsing
		const isTableLine = line.includes("|") && line.trim() !== "";
		const isTableSeparator = /^\s*\|?[\s\-|:]+\|?\s*$/.test(line);

		if (isTableLine || isTableSeparator) {
			if (!inTable) {
				// Close any open blocks first
				if (inList) {
					nodes.push({ tag: currentListTag, children: currentListItems });
					inList = false;
					currentListItems = [];
				}
				if (inBlockquote) {
					nodes.push({
						tag: "blockquote",
						children: processInlineMarkdown(blockquoteContent.join("\n")),
					});
					inBlockquote = false;
					blockquoteContent = [];
				}

				inTable = true;
				tableLines = [];
			}
			tableLines.push(line);
			continue;
		} else if (inTable) {
			// End of table
			nodes.push(parseTable(tableLines));
			inTable = false;
			tableLines = [];
			// Continue processing current line
		}

		// Handle blockquotes
		if (line.startsWith(">")) {
			if (!inBlockquote) {
				// If previously in a list, close it first
				if (inList) {
					nodes.push({ tag: currentListTag, children: currentListItems });
					inList = false;
					currentListItems = [];
				}
				inBlockquote = true;
				blockquoteContent = [];
			}
			blockquoteContent.push(line.substring(1).trimStart()); // Remove '>' and leading space
			continue;
		} else if (inBlockquote) {
			// If not a blockquote line, but was in blockquote, close it
			nodes.push({
				tag: "blockquote",
				children: processInlineMarkdown(blockquoteContent.join("\n")),
			});
			inBlockquote = false;
			blockquoteContent = [];
			// Process current line as a new element
		}

		// Handle headings (MOVED UP - before lists to prevent numbered headings from being parsed as list items)
		const headingMatch = line.match(/^(#+)\s*(.*)/);
		if (headingMatch?.[1] && headingMatch[2] !== undefined) {
			// Close any open blocks before adding a heading
			if (inList) {
				nodes.push({ tag: currentListTag, children: currentListItems });
				inList = false;
				currentListItems = [];
			}
			if (inBlockquote) {
				nodes.push({
					tag: "blockquote",
					children: processInlineMarkdown(blockquoteContent.join("\n")),
				});
				inBlockquote = false;
				blockquoteContent = [];
			}
			const level = headingMatch[1].length;
			const originalText = headingMatch[2] || "";
			let displayText = originalText;
			let tag: 'h3' | 'h4' = 'h3';

			// Map headings to Telegraph API compatible tags with visual hierarchy preservation
			// Telegraph API only supports h3 and h4 tags for headings
			switch (level) {
				case 1:
				case 2:
				case 3:
					// H1, H2, H3 ‚Üí h3 (highest available level in Telegraph API)
					tag = 'h3';
					displayText = originalText;
					break;
				case 4:
					// H4 ‚Üí h4 (direct mapping, supported by Telegraph API)
					tag = 'h4';
					displayText = originalText;
					break;
				case 5:
					// H5 ‚Üí h4 with visual prefix to preserve hierarchy and enable anchors
					tag = 'h4';
					displayText = `¬ª ${originalText}`;
					break;
				case 6:
					// H6 ‚Üí h4 with double visual prefix to preserve hierarchy and enable anchors
					tag = 'h4';
					displayText = `¬ª¬ª ${originalText}`;
					break;
				default:
					// Handle edge case: levels > 6 as h4 with triple visual prefix
					tag = 'h4';
					displayText = `¬ª¬ª¬ª ${originalText}`;
					break;
			}

			const processedChildren = processInlineMarkdown(displayText);
			nodes.push({ tag, children: processedChildren });
			continue;
		}

		// Handle lists (NOW AFTER HEADINGS)
		const listItemMatch = line.match(/^(-|\*)\s+(.*)|(\d+)\.\s+(.*)/);
		if (listItemMatch) {
			if (!inList) {
				// If previously in a blockquote, close it first
				if (inBlockquote) {
					nodes.push({
						tag: "blockquote",
						children: processInlineMarkdown(blockquoteContent.join("\n")),
					});
					inBlockquote = false;
					blockquoteContent = [];
				}
				inList = true;
				currentListTag = listItemMatch[1] ? "ul" : "ol";
				currentListItems = [];
			}
			let textContent = "";
			if (listItemMatch[2] !== undefined) {
				textContent = listItemMatch[2];
			} else if (listItemMatch[4] !== undefined) {
				textContent = listItemMatch[4];
			}
			if (textContent) {
				currentListItems.push({
					tag: "li",
					children: processInlineMarkdown(textContent.trim()),
				});
			}
			continue;
		} else if (inList) {
			// If not a list item, but was in a list, close it
			if (line.trim() === "") {
				// Empty line closes a list
				nodes.push({ tag: currentListTag, children: currentListItems });
				inList = false;
				currentListItems = [];
				continue; // Don't add empty paragraph for the empty line
			} else {
				// New content, close list and process as new paragraph
				nodes.push({ tag: currentListTag, children: currentListItems });
				inList = false;
				currentListItems = [];
				// Process current line as a new element (paragraph)
			}
		}

		// Handle horizontal rules (simple check for now)
		if (line.match(/^[*-]{3,}\s*$/)) {
			// Close any open blocks before adding HR
			if (inList) {
				nodes.push({ tag: currentListTag, children: currentListItems });
				inList = false;
				currentListItems = [];
			}
			if (inBlockquote) {
				nodes.push({
					tag: "blockquote",
					children: processInlineMarkdown(blockquoteContent.join("\n")),
				});
				inBlockquote = false;
				blockquoteContent = [];
			}
			nodes.push({ tag: "hr" });
			continue;
		}

		// Handle empty lines or plain paragraphs
		if (line.trim() === "") {
			// Do not add empty paragraphs if previous node was also an empty paragraph
			const lastNode = nodes[nodes.length - 1];
			if (
				nodes.length > 0 &&
				typeof lastNode === "object" &&
				lastNode.tag === "p" &&
				(!lastNode.children ||
					lastNode.children.length === 0 ||
					(lastNode.children.length === 1 && lastNode.children[0] === ""))
			) {
				continue; // Skip adding redundant empty paragraph
			}
			if (!inList && !inBlockquote && !inCodeBlock && !inTable) {
				// Only add empty paragraph if not inside a block
				nodes.push({ tag: "p", children: [""] });
			}
			continue;
		}

		// If we reach here, it's a plain paragraph line
		// Close any open blocks (lists, blockquotes) if this line is not part of them
		if (inList) {
			nodes.push({ tag: currentListTag, children: currentListItems });
			inList = false;
			currentListItems = [];
		}
		if (inBlockquote) {
			nodes.push({
				tag: "blockquote",
				children: processInlineMarkdown(blockquoteContent.join("\n")),
			});
			inBlockquote = false;
			blockquoteContent = [];
		}

		nodes.push({ tag: "p", children: processInlineMarkdown(line) });
	}

	// After loop, close any open blocks
	if (inCodeBlock) {
		nodes.push({
			tag: "pre",
			children: [
				{ tag: "code", children: [codeBlockContent.join("\n").trimEnd()] },
			],
		}); // Trim end for final code block
	}
	if (inList) {
		nodes.push({ tag: currentListTag, children: currentListItems });
	}
	if (inBlockquote) {
		nodes.push({
			tag: "blockquote",
			children: processInlineMarkdown(blockquoteContent.join("\n")),
		});
	}
	if (inTable) {
		nodes.push(parseTable(tableLines));
	}

	// Filter out any empty paragraph nodes that might have been created unnecessarily
	return nodes.filter(
		(node) =>
			!(
				node.tag === "p" &&
				(!node.children ||
					node.children.length === 0 ||
					(node.children.length === 1 && node.children[0] === ""))
			),
	);
}

function processInlineMarkdown(text: string): (string | TelegraphNode)[] {
	const result: (string | TelegraphNode)[] = [];
	let currentIndex = 0;

	// Define patterns for different inline elements
	const patterns = [
		{ regex: /\*\*(.*?)\*\*/g, tag: "strong" },
		{ regex: /__(.*?)__/g, tag: "strong" },
		{ regex: /\*(.*?)\*/g, tag: "em" },
		{ regex: /_(.*?)_/g, tag: "em" },
		{ regex: /`(.*?)`/g, tag: "code" },
		{ regex: /\[(.*?)\]\(([^()]*(?:\([^()]*\)[^()]*)*)\)/g, tag: "a", isLink: true },
	];

	// Find all matches with their positions
	const matches: Array<{
		index: number;
		length: number;
		tag: string;
		content: string;
		href?: string;
	}> = [];

	for (const pattern of patterns) {
		pattern.regex.lastIndex = 0; // Reset regex
		let match: RegExpExecArray | null = null;
		match = pattern.regex.exec(text);
		while (match !== null) {
			if (match.index !== undefined) {
				if (pattern.isLink) {
					matches.push({
						index: match.index,
						length: match[0].length,
						tag: pattern.tag,
						content: match[1] || "",
						href: match[2] || "",
					});
				} else {
					matches.push({
						index: match.index,
						length: match[0].length,
						tag: pattern.tag,
						content: match[1] || "",
					});
				}
			}
			match = pattern.regex.exec(text);
		}
	}

	// Sort matches by index to process them in order
	matches.sort((a, b) => a.index - b.index);

	// Process matches and build result
	for (const match of matches) {
		// Add plain text before this match
		if (match.index > currentIndex) {
			const plainText = text.substring(currentIndex, match.index);
			if (plainText) {
				result.push(plainText);
			}
		}

		// Skip if this match overlaps with previous processed content
		if (match.index < currentIndex) {
			continue;
		}

		// Add the formatted element
		if (match.tag === "a" && match.href !== undefined) {
			result.push({
				tag: "a",
				attrs: { href: match.href },
				children: [match.content],
			});
		} else {
			result.push({
				tag: match.tag,
				children: [match.content],
			});
		}

		currentIndex = match.index + match.length;
	}

	// Add any remaining plain text
	if (currentIndex < text.length) {
		const remainingText = text.substring(currentIndex);
		if (remainingText) {
			result.push(remainingText);
		}
	}

	// If no matches found, return the original text
	if (result.length === 0) {
		return [text];
	}

	return result;
}

```

`src/slice_book.ts`

```ts
import { join } from 'path';
import { file, write } from 'bun';
import { mkdir } from 'node:fs/promises';

async function sliceBook() {
  const baseDir = join(process.cwd(), '..', '–º–∞—Ç–µ—Ä–∏–∞–ª—ã', 'SatKriyaSaraDipika', 'sliced');
  const bookPath = join(process.cwd(), '..', '–º–∞—Ç–µ—Ä–∏–∞–ª—ã', 'SatKriyaSaraDipika', 'SatKriyaSaraDipika.md');
  const contentsPath = join(process.cwd(), '..', '–º–∞—Ç–µ—Ä–∏–∞–ª—ã', 'SatKriyaSaraDipika', 'book.md');

  // Ensure base directory exists
  await mkdir(baseDir, { recursive: true });

  // Read the full book content
  const bookContent = await file(bookPath).text();
  const contentsContent = await file(contentsPath).text();

  // Parse contents to get sections and their start pages
  const sections: { name: string; page: number; level: number; full_path_name: string; parent_full_path?: string }[] = [];
  const lines = contentsContent.split('\n');
  let currentParentFullPath = '';

  for (const line of lines) {
    // Regex for main sections: 001. **Section Name** Page
    const mainSectionRegex = /^\s*(\d{3}\. )\*\*([^*]+?)\*\*\s+(\d+)\s*$/;
    // Updated regex for sub-sections:     01. Sub Section Name Page
    const subSectionRegex = /^\s*(\d{2}\. )([^\d\*]+?)\s+(\d+)\s*$/;

    let match;

    match = line.match(mainSectionRegex);
    if (match) {
      const fullPath = match[1]?.trim() ?? '';
      const name = match[2]?.trim() ?? '';
      const page = parseInt(match[3] ?? '0');
      sections.push({ name, page, level: 0, full_path_name: fullPath.slice(0, -1) }); // Remove trailing dot for path
      currentParentFullPath = fullPath.slice(0, -1); // Store parent path without trailing dot
      continue;
    }

    match = line.match(subSectionRegex);
    if (match) {
      const subSectionNumber = match[1]?.trim() ?? '';
      const name = match[2]?.trim() ?? '';
      const page = parseInt(match[3] ?? '0');

      if (currentParentFullPath) {
        const fullPathName = `${currentParentFullPath}/${subSectionNumber.slice(0, -1)}`; // Combine parent and sub-section numbers
        sections.push({ name, page, level: 1, full_path_name: fullPathName, parent_full_path: currentParentFullPath });
      } else {
        // Fallback for unexpected structure, treat as top-level using just the subSectionNumber
        sections.push({ name, page, level: 0, full_path_name: subSectionNumber.slice(0, -1) });
        console.warn(`Sub-section '${name}' found without a preceding main section. Treating as main section.`);
        currentParentFullPath = subSectionNumber.slice(0, -1);
      }
    }
  }
  console.log('--- Sections Parsing Status ---');
  console.log(`Number of sections found: ${sections.length}`);
  if (sections.length > 0) {
    console.log('First 5 sections:', sections.slice(0, 5));
  }

  // Sort sections by page
  sections.sort((a, b) => a.page - b.page);

  // Split book into pages
  const pages: { [key: number]: string } = {};
  const pageRegex = /\*\*–°—Ç—Ä–∞–Ω–∏—Ü–∞ (\d+)\*\*/g;
  let lastIndex = 0;
  let matchPage;
  while ((matchPage = pageRegex.exec(bookContent)) !== null) {
    const pageNum = parseInt(matchPage[1] ?? '0');
    const start = matchPage.index + matchPage[0].length;
    const end = bookContent.indexOf('**–°—Ç—Ä–∞–Ω–∏—Ü–∞ ', start);
    const content = bookContent.slice(start, end !== -1 ? end : undefined).trim();
    pages[pageNum] = content;
    lastIndex = end;
  }
  // Last page - ensure it's captured correctly
  if (lastIndex !== -1 && lastIndex < bookContent.length) {
    const lastPageNum = Object.keys(pages).length > 0 ? Math.max(...Object.keys(pages).map(Number)) + 1 : 1;
    pages[lastPageNum] = bookContent.slice(lastIndex).trim();
  } else if (Object.keys(pages).length === 0 && bookContent.length > 0) {
    // Case where there are no page markers but content exists (e.g., a single page book)
    pages[1] = bookContent.trim();
  }

  console.log('--- Pages Parsing Status ---');
  console.log(`Number of pages found: ${Object.keys(pages).length}`);
  if (Object.keys(pages).length > 0) {
    console.log('Sample pages (1, 2, 3):', pages[1]?.substring(0, 100), pages[2]?.substring(0, 100), pages[3]?.substring(0, 100));
  }

  const allPageNumbers = Object.keys(pages).map(Number).sort((a, b) => a - b);

  // Create folders and save pages
  for (let i = 0; i < sections.length; i++) {
    const section = sections[i]!;
    const nextSection = sections[i + 1];
    const startPage = section.page;
    const endPage = nextSection ? nextSection.page - 1 : (allPageNumbers.length > 0 ? Math.max(...allPageNumbers) : 0);

    let targetDirForSection: string;

    if (section.level === 0) {
      targetDirForSection = join(baseDir, section.full_path_name); // Use full_path_name directly
      console.log(`Determined main section directory: ${targetDirForSection}`);
    } else if (section.level === 1 && section.parent_full_path) {
      targetDirForSection = join(baseDir, section.parent_full_path, section.full_path_name.split('/').pop()!); // Use parent path and only the last part of full_path_name
      console.log(`Determined sub-section directory: ${targetDirForSection}`);
    } else {
      targetDirForSection = baseDir;
      console.warn(`Unexpected section structure for ${section.name}. Placing in base directory.`);
    }

    await mkdir(targetDirForSection, { recursive: true }); // Ensure this directory exists

    console.log(`Processing section: ${section.name} (Path: ${targetDirForSection}), Start Page: ${startPage}, End Page: ${endPage}`);

    for (let p = startPage; p <= endPage; p++) {
      if (pages[p]) {
        let pageContent = pages[p] ?? '';

        // Add front matter to the beginning of the page content
        const frontMatter = `---\ntitle: Sat Kriya Sara Dipika - page - ${p}\n---\n\n`;
        pageContent = frontMatter + pageContent;

        // Add navigation links
        const prevPage = p > 1 ? p - 1 : null;
        const nextPage = p < allPageNumbers.length ? allPageNumbers[allPageNumbers.indexOf(p) + 1] : null; // Get the actual next page number from sorted list

        let navigationLinks = '';
        if (prevPage || nextPage) {
          navigationLinks += '\n\n***\n\n**Navigation:**\n';
          if (prevPage) {
            const prevFilePath = `./page_${prevPage.toString().padStart(3, '0')}.md`;
            navigationLinks += `\n* [Previous Page: ${prevPage}](${prevFilePath}) `;
          }
          if (nextPage) {
            const nextFilePath = `./page_${nextPage.toString().padStart(3, '0')}.md`;
            navigationLinks += `\n* [Next Page: ${nextPage}](${nextFilePath})`;
          }
        }
        pageContent += navigationLinks;

        const filePath = join(targetDirForSection, `page_${p.toString().padStart(3, '0')}.md`);
        console.log(`Writing page ${p} to ${filePath}`);
        await write(filePath, pageContent);
      } else {
        console.log(`Page ${p} not found in parsed content.`);
      }
    }
  }

  // Generate toc.md
  let tocContent = `---\ntitle: Sat Kriya Sara Dipika\n---\n\n# Contents\n\n`;
  let lastMainSectionFullPath = ''; // This variable is not strictly needed for the new TOC format, but kept for context

  for (const section of sections) {
    let sectionText = '';
    let indentation = '';

    const firstPageFile = `page_${section.page.toString().padStart(3, '0')}.md`;

    // Determine the relative path for the link based on section level
    let linkBaseDir = '';
    if (section.level === 0) {
      linkBaseDir = section.full_path_name; // e.g., '001'
    } else if (section.level === 1 && section.parent_full_path) {
      linkBaseDir = `${section.parent_full_path}/${section.full_path_name.split('/').pop()}`; // e.g., '005/01'
    }
    const linkPath = `./${linkBaseDir}/${firstPageFile}`;

    if (section.level === 0) {
      sectionText = `${section.full_path_name}. ${section.name}`;
      tocContent += `* [${sectionText}](${linkPath})\n`;
      // lastMainSectionFullPath = section.full_path_name; // No longer strictly needed as nesting is directly based on level
    } else if (section.level === 1) {
      indentation = '  '; // Two spaces for sub-sections for markdown bullet list
      sectionText = `${section.full_path_name.split('/').pop()}. ${section.name}`;
      tocContent += `${indentation}* [${sectionText}](${linkPath})\n`;
    }
  }

  const tocFilePath = join(baseDir, 'toc.md');
  console.log(`Writing TOC to ${tocFilePath}`);
  await write(tocFilePath, tocContent);

  console.log('Slicing and TOC generation completed.');
}

sliceBook().catch(console.error);

```

`src/telegraphPublisher.test.ts`

```ts
import { expect, test } from "bun:test";
import type { TelegraphNode } from "./telegraphPublisher";
import { TelegraphPublisher } from "./telegraphPublisher";

test("should create a Telegraph account", async () => {
	const publisher = new TelegraphPublisher();
	const account = await publisher.createAccount(
		"Test Author",
		"Test Author Name",
	);

	expect(account).toBeDefined();
	expect(account.short_name).toBe("Test Author");
	expect(account.author_name).toBe("Test Author Name");
	expect(account.access_token).toBeDefined();
});

test("should publish HTML content to Telegraph", async () => {
	const publisher = new TelegraphPublisher();
	await publisher.createAccount("Test Author", "Test Author Name");

	const htmlContent = "<h1>Test Title</h1><p>Test content</p>";
	const result = await publisher.publishHtml("Test Article", htmlContent);

	expect(result).toBeDefined();
	expect(result.title).toBe("Test Article");
	expect(result.url).toBeDefined();
	expect(result.path).toBeDefined();
});

test("should publish markdown content to Telegraph", async () => {
	const publisher = new TelegraphPublisher();
	await publisher.createAccount("Test Author", "Test Author Name");

	const markdownContent = "# Test Title\n\nTest content with **bold** text.";
	const result = await publisher.publishMarkdown(
		"Test Markdown Article",
		markdownContent,
	);

	expect(result).toBeDefined();
	expect(result.title).toBe("Test Markdown Article");
	expect(result.url).toBeDefined();
	expect(result.path).toBeDefined();
});

test("should throw an error if content size exceeds 64KB", () => {
	const publisher = new TelegraphPublisher();
	// Create a large array of nodes that will exceed 64KB when stringified
	const largeContentNodes: TelegraphNode[] = [];
	const singleNode = { tag: "p", children: ["a".repeat(1000)] }; // ~1KB per node
	for (let i = 0; i < 70; i++) {
		// 70 nodes should be > 64KB
		largeContentNodes.push(singleNode);
	}

	expect(() => publisher.checkContentSize(largeContentNodes)).toThrow(
		/Content size \(\d+\.\d{2} KB\) exceeds the Telegra.ph limit of 64 KB\. Please reduce the content size\./,
	);
});

test("should not throw an error if content size is within 64KB", () => {
	const publisher = new TelegraphPublisher();
	// Create a small array of nodes that will be within 64KB when stringified
	const smallContentNodes: TelegraphNode[] = [];
	const singleNode = { tag: "p", children: ["a".repeat(100)] }; // ~0.1KB per node
	for (let i = 0; i < 50; i++) {
		// 50 nodes should be < 64KB
		smallContentNodes.push(singleNode);
	}

	expect(() => publisher.checkContentSize(smallContentNodes)).not.toThrow();
});

test("should retrieve a list of pages", async () => {
	const mockAccessToken = "mock_access_token";
	const mockPageList = {
		ok: true,
		result: {
			total_count: 2,
			pages: [
				{
					path: "test-page-1",
					url: "http://telegra.ph/test-page-1",
					title: "Test Page 1",
					description: "Description for test page 1",
					author_name: "Test Author",
					author_url: "http://example.com/author",
				},
				{
					path: "test-page-2",
					url: "http://telegra.ph/test-page-2",
					title: "Test Page 2",
					description: "Description for test page 2",
					author_name: "Test Author",
					author_url: "http://example.com/author",
				},
			],
		},
	};

	// Mock the global fetch function
	const originalFetch = global.fetch;
	global.fetch = Object.assign(
		async (
			url: Parameters<typeof fetch>[0],
			init?: Parameters<typeof fetch>[1],
		) => {
			if (typeof url === "string" && url.includes("/getPageList")) {
				return new Response(JSON.stringify(mockPageList), {
					status: 200,
					headers: { "Content-Type": "application/json" },
				});
			}
			return originalFetch(url, init);
		},
		originalFetch,
	);

	const publisher = new TelegraphPublisher();
	publisher.setAccessToken(mockAccessToken);
	const result = await publisher.listPages();

	expect(result).toBeDefined();
	expect(result.total_count).toBe(2);
	expect(result.pages).toHaveLength(2);
	expect(result.pages[0]?.title).toBe("Test Page 1");
	expect(result.pages[1]?.url).toBe("http://telegra.ph/test-page-2");

	// Restore the original fetch function
	global.fetch = originalFetch;
});

test("should edit an existing page", async () => {
	const mockAccessToken = "mock_access_token";
	const mockPath = "test-page-to-edit";
	const newTitle = "Edited Test Page Title";
	const newContentNodes: TelegraphNode[] = [
		{ tag: "p", children: ["This is the edited content."] },
	];
	const newAuthorName = "Edited Author";
	const newAuthorUrl = "http://example.com/edited-author";

	const mockEditedPage = {
		ok: true,
		result: {
			path: mockPath,
			url: `http://telegra.ph/${mockPath}`,
			title: newTitle,
			description: "",
			author_name: newAuthorName,
			author_url: newAuthorUrl,
			views: 0,
			can_edit: true,
		},
	};

	// Mock the global fetch function
	const originalFetch = global.fetch;
	global.fetch = Object.assign(
		async (
			url: Parameters<typeof fetch>[0],
			init?: Parameters<typeof fetch>[1],
		) => {
			if (typeof url === "string" && url.includes(`/editPage/${mockPath}`)) {
				// Optionally, you can assert on the request body here
				const requestBody = JSON.parse(init?.body as string);
				expect(requestBody.title).toBe(newTitle);
				expect(requestBody.content).toBe(JSON.stringify(newContentNodes));
				expect(requestBody.author_name).toBe(newAuthorName);
				expect(requestBody.author_url).toBe(newAuthorUrl);
				return new Response(JSON.stringify(mockEditedPage), {
					status: 200,
					headers: { "Content-Type": "application/json" },
				});
			}
			return originalFetch(url, init);
		},
		originalFetch,
	);

	const publisher = new TelegraphPublisher();
	publisher.setAccessToken(mockAccessToken);
	const editedPage = await publisher.editPage(
		mockPath,
		newTitle,
		newContentNodes,
		newAuthorName,
		newAuthorUrl,
	);

	expect(editedPage).toBeDefined();
	expect(editedPage.title).toBe(newTitle);
	expect(editedPage.path).toBe(mockPath);
	expect(editedPage.author_name).toBe(newAuthorName);
	expect(editedPage.url).toBe(`http://telegra.ph/${mockPath}`);

	// Restore the original fetch function
	global.fetch = originalFetch;
});

```

`src/telegraphPublisher.ts`

```ts
import { convertMarkdownToTelegraphNodes } from "./markdownConverter";

interface ApiResponse<T> {
	ok: boolean;
	result?: T;
	error?: string;
}

export interface TelegraphAccount {
	short_name: string;
	author_name: string;
	author_url?: string;
	access_token: string;
	auth_url?: string;
	page_count?: number;
}

export interface TelegraphPage {
	path: string;
	url: string;
	title: string;
	description?: string;
	author_name?: string;
	author_url?: string;
	image_url?: string;
	views?: number;
	can_edit?: boolean;
}

export interface TelegraphPageList {
	total_count: number;
	pages: TelegraphPage[];
}

export interface TelegraphNode {
	tag?: string;
	attrs?: Record<string, string>;
	children?: (string | TelegraphNode)[];
}

export class TelegraphPublisher {
	private accessToken?: string;
	private authorName?: string;
	private authorUrl?: string;
	private readonly apiBase = "https://api.telegra.ph";

	setAccessToken(token: string) {
		this.accessToken = token;
	}

	async createAccount(
		shortName: string,
		authorName?: string,
		authorUrl?: string,
	): Promise<TelegraphAccount> {
		const params = new URLSearchParams({
			short_name: shortName,
		});

		if (authorName) {
			params.append("author_name", authorName);
		}

		if (authorUrl) {
			params.append("author_url", authorUrl);
		}

		const response = await fetch(`${this.apiBase}/createAccount`, {
			method: "POST",
			headers: {
				"Content-Type": "application/x-www-form-urlencoded",
			},
			body: params,
		});

		if (!response.ok) {
			throw new Error(
				`Failed to create account: ${response.status} ${response.statusText}`,
			);
		}

		const data = (await response.json()) as ApiResponse<TelegraphAccount>;

		if (!data.ok) {
			throw new Error(`Telegraph API error: ${data.error}`);
		}

		const account: TelegraphAccount = data.result as TelegraphAccount;
		this.accessToken = account.access_token;
		this.authorName = account.author_name;
		this.authorUrl = account.author_url;

		return account;
	}

	async getAccountInfo(accessToken: string): Promise<TelegraphAccount> {
		const params = new URLSearchParams({
			access_token: accessToken,
			fields: JSON.stringify([
				"short_name",
				"author_name",
				"author_url",
				"page_count",
			]),
		});

		const response = await fetch(
			`${this.apiBase}/getAccountInfo?${params.toString()}`,
		);

		if (!response.ok) {
			throw new Error(
				`Failed to get account info: ${response.status} ${response.statusText}`,
			);
		}

		const data = (await response.json()) as ApiResponse<TelegraphAccount>;

		if (!data.ok) {
			throw new Error(`Telegraph API error: ${data.error}`);
		}

		return data.result as TelegraphAccount;
	}

	async listPages(
		offset: number = 0,
		limit: number = 50,
	): Promise<TelegraphPageList> {
		if (!this.accessToken) {
			throw new Error("Access token is not set for listPages.");
		}
		const params = new URLSearchParams({
			access_token: this.accessToken,
			offset: offset.toString(),
			limit: limit.toString(),
		});

		const response = await fetch(
			`${this.apiBase}/getPageList?${params.toString()}`,
		);

		if (!response.ok) {
			throw new Error(
				`Failed to list pages: ${response.status} ${response.statusText}`,
			);
		}

		const data = (await response.json()) as ApiResponse<TelegraphPageList>;

		if (!data.ok) {
			throw new Error(`Telegraph API error: ${data.error}`);
		}

		return data.result as TelegraphPageList;
	}

	async publishHtml(
		title: string,
		htmlContent: string,
	): Promise<TelegraphPage> {
		// Store parameters for potential retry
		const originalTitle: string = title;
		const originalHtmlContent: string = htmlContent;

		if (!this.accessToken) {
			throw new Error("No access token. Please create an account first.");
		}

		const content = this.htmlToNodes(htmlContent);

		const params = new URLSearchParams({
			access_token: this.accessToken,
			title: title,
			content: JSON.stringify(content),
			return_content: "false",
		});

		if (this.authorName) {
			params.append("author_name", this.authorName);
		}

		if (this.authorUrl) {
			params.append("author_url", this.authorUrl);
		}

		const response = await fetch(`${this.apiBase}/createPage`, {
			method: "POST",
			headers: {
				"Content-Type": "application/x-www-form-urlencoded",
			},
			body: params,
		});

		if (!response.ok) {
			throw new Error(
				`Failed to create page: ${response.status} ${response.statusText}`,
			);
		}

		const data = (await response.json()) as ApiResponse<TelegraphPage>;

		if (!data.ok) {
			// Handle FLOOD_WAIT errors
			if (data.error && data.error.startsWith('FLOOD_WAIT_')) {
				const waitSeconds = parseInt(data.error!.split('_')[2]) || 5;
				console.warn(`Rate limited by Telegraph API. Waiting ${waitSeconds} seconds...`);
				await this.sleep(waitSeconds * 1000);
				// Retry the request
				return this.publishHtml(title, htmlContent);
			}
			throw new Error(`Telegraph API error: ${data.error}`);
		}

		return data.result as TelegraphPage;
	}

	async publishMarkdown(
		title: string,
		markdownContent: string,
	): Promise<TelegraphPage> {
		const nodes = convertMarkdownToTelegraphNodes(markdownContent);
		return this.publishNodes(title, nodes);
	}

	async publishNodes(
		title: string,
		nodes: TelegraphNode[],
	): Promise<TelegraphPage> {
		if (!this.accessToken) {
			throw new Error("Access token is not set for publishNodes.");
		}
		const payload = {
			access_token: this.accessToken,
			title,
			content: JSON.stringify(nodes),
		};

		const response = await fetch(`${this.apiBase}/createPage`, {
			method: "POST",
			headers: {
				"Content-Type": "application/json",
			},
			body: JSON.stringify(payload),
		});

		if (!response.ok) {
			throw new Error(`HTTP error! status: ${response.status}`);
		}

		const data = (await response.json()) as ApiResponse<TelegraphPage>;
		if (!data.ok) {
			throw new Error(`Telegraph API error: ${data.error}`);
		}

		if (!data.result) {
			throw new Error("Telegraph API returned empty result");
		}

		return data.result;
	}

	async editPage(
		path: string,
		title: string,
		nodes: TelegraphNode[],
		authorName?: string,
		authorUrl?: string,
	): Promise<TelegraphPage> {
		// Store parameters for potential retry
		const originalPath = path;
		const originalTitle = title;
		const originalNodes = nodes;
		const originalAuthorName = authorName;
		const originalAuthorUrl = authorUrl;

		if (!this.accessToken) {
			throw new Error("Access token is not set for editPage.");
		}
		const payload: Record<string, unknown> = {
			access_token: this.accessToken,
			path: path,
			title: title,
			content: JSON.stringify(nodes),
			return_content: false,
		};

		if (authorName) {
			payload.author_name = authorName;
		}
		if (authorUrl) {
			payload.author_url = authorUrl;
		}

		const response = await fetch(`${this.apiBase}/editPage/${path}`, {
			method: "POST",
			headers: {
				"Content-Type": "application/json",
			},
			body: JSON.stringify(payload),
		});

		if (!response.ok) {
			throw new Error(`HTTP error! status: ${response.status}`);
		}

		const data = (await response.json()) as ApiResponse<TelegraphPage>;
		if (!data.ok) {
			// Handle FLOOD_WAIT errors
			if (data.error && data.error.startsWith('FLOOD_WAIT_')) {
				const waitSeconds = parseInt(data.error!.split('_')[2]) || 5;
				console.warn(`Rate limited by Telegraph API. Waiting ${waitSeconds} seconds...`);
				await this.sleep(waitSeconds * 1000);
				// Retry the request with original parameters
				return this.editPage(path, title, nodes, authorName, authorUrl);
			}
			throw new Error(`Telegraph API error: ${data.error}`);
		}

		if (!data.result) {
			throw new Error("Telegraph API returned empty result");
		}

		return data.result;
	}

	private htmlToNodes(html: string): TelegraphNode[] {
		const nodes: TelegraphNode[] = [];
		const cleanHtml = html.replace(/<!--[\s\S]*?-->/g, "").trim();

		if (!cleanHtml) {
			return [{ tag: "p", children: [""] }];
		}

		// Regex to capture block-level elements or plain text segments
		// Captures: <tag>content</tag> OR <br/> OR plain_text
		const blockRegex =
			/(<h[1-6]>([\s\S]*?)<\/h[1-6]>|<p>([\s\S]*?)<\/p>|<ul>([\s\S]*?)<\/ul>|<ol>([\s\S]*?)<\/ol>|<li>([\s\S]*?)<\/li>|<blockquote>([\s\S]*?)<\/blockquote>|<br\s*\/?>)|([^<]+)/gi;
		let lastIndex = 0;
		let match: RegExpExecArray | null = null;

		match = blockRegex.exec(cleanHtml);
		while (match !== null) {
			// Handle text before the current match
			if (match.index > lastIndex) {
				const textSegment = cleanHtml.substring(lastIndex, match.index).trim();
				if (textSegment) {
					nodes.push({
						tag: "p",
						children: this.processInlineElements(textSegment),
					});
				}
			}

			const fullTagMatch = match[1]; // The entire matched HTML tag block (e.g., <p>...</p>)
			const plainText = match[8]; // Text not inside any recognized tag

			if (fullTagMatch) {
				// Determine the tag name and content
				const tagContentMatch = fullTagMatch.match(
					/^<(h[1-6]|p|ul|ol|li|blockquote|br)(?:[^>]*)?>(?:([\s\S]*?)<\/\1>)?|<(br\s*\/?)>$/i,
				);

				if (tagContentMatch) {
					const tagName = tagContentMatch[1]
						? tagContentMatch[1].toLowerCase()
						: "";
					const content =
						tagContentMatch[2] !== undefined ? tagContentMatch[2] : "";
					const isSelfClosingBr = !!tagContentMatch[3];

					if (isSelfClosingBr) {
						nodes.push({ tag: "br" });
					} else if (tagName) {
						if (
							["h1", "h2", "h3", "h4", "h5", "h6", "p", "blockquote"].includes(
								tagName,
							)
						) {
							nodes.push({
								tag: tagName,
								children: this.processInlineElements(content),
							});
						} else if (["ul", "ol"].includes(tagName)) {
							const listItems: TelegraphNode[] = [];
							// Extract list items using regex
							const listItemRegex = /<li>([\s\S]*?)<\/li>/gi;
							let listItemMatch: RegExpExecArray | null = null;
							listItemMatch = listItemRegex.exec(content);
							while (listItemMatch !== null) {
								if (listItemMatch[1]) {
									listItems.push({
										tag: "li",
										children: this.processInlineElements(listItemMatch[1]),
									});
								}
								listItemMatch = listItemRegex.exec(content);
							}
							nodes.push({ tag: tagName, children: listItems });
						} else if (tagName === "li") {
							// In htmlToNodes, li should be part of ul/ol processing, so this might be redundant based on blockRegex
							nodes.push({
								tag: tagName,
								children: this.processInlineElements(content),
							});
						}
					}
				}
			} else if (plainText) {
				nodes.push({
					tag: "p",
					children: this.processInlineElements(plainText.trim()),
				});
			}
			lastIndex = blockRegex.lastIndex;
			match = blockRegex.exec(cleanHtml);
		}

		// Handle any remaining text after the last match
		if (lastIndex < cleanHtml.length) {
			const remainingText = cleanHtml.substring(lastIndex).trim();
			if (remainingText) {
				nodes.push({
					tag: "p",
					children: this.processInlineElements(remainingText),
				});
			}
		}

		return nodes.filter((node) => {
			if (
				typeof node === "object" &&
				node.tag === "p" &&
				node.children &&
				node.children.length === 1 &&
				node.children[0] === ""
			) {
				return false; // Filter out empty paragraphs
			}
			return true;
		});
	}

	private processInlineElements(text: string): (string | TelegraphNode)[] {
		const result: (string | TelegraphNode)[] = [];
		let currentIndex = 0;

		const patterns = [
			{ regex: /\*\*(.*?)\*\*/g, tag: "strong" },
			{ regex: /__(.*?)__/g, tag: "strong" },
			{ regex: /\*(.*?)\*/g, tag: "em" },
			{ regex: /_(.*?)_/g, tag: "em" },
			{ regex: /`(.*?)`/g, tag: "code" },
			{ regex: /\[(.*?)\]\((.*?)\)/g, tag: "a", isLink: true },
		];

		const matches: Array<{
			index: number;
			length: number;
			tag: string;
			content: string;
			href?: string;
		}> = [];

		for (const pattern of patterns) {
			pattern.regex.lastIndex = 0; // Reset regex
			let match: RegExpExecArray | null = null;
			match = pattern.regex.exec(text);
			while (match !== null) {
				if (match.index !== undefined) {
					if (pattern.isLink) {
						matches.push({
							index: match.index,
							length: match[0].length,
							tag: pattern.tag,
							content: match[1] || "",
							href: match[2] || "",
						});
					} else {
						matches.push({
							index: match.index,
							length: match[0].length,
							tag: pattern.tag,
							content: match[1] || "",
						});
					}
				}
				match = pattern.regex.exec(text);
			}
		}

		matches.sort((a, b) => a.index - b.index);

		for (const match of matches) {
			if (match.index > currentIndex) {
				const plainText = text.substring(currentIndex, match.index);
				if (plainText) {
					result.push(plainText);
				}
			}

			if (match.index < currentIndex) {
				continue;
			}

			if (match.tag === "a" && match.href !== undefined) {
				result.push({
					tag: "a",
					attrs: { href: match.href },
					children: [match.content],
				});
			} else {
				result.push({
					tag: match.tag,
					children: [match.content],
				});
			}

			currentIndex = match.index + match.length;
		}

		if (currentIndex < text.length) {
			const remainingText = text.substring(currentIndex);
			if (remainingText) {
				result.push(remainingText);
			}
		}

		if (result.length === 0) {
			return [text];
		}

		return result;
	}

	private decodeHtmlEntities(text: string): string {
		const entities: { [key: string]: string } = {
			"&amp;": "&",
			"&lt;": "<",
			"&gt;": ">",
			"&quot;": '"',
			// Add more as needed
		};
		let decodedText = text;
		for (const entity in entities) {
			decodedText = decodedText.replace(
				new RegExp(entity, "g"),
				entities[entity] || "",
			);
		}
		return decodedText;
	}

	private stripHtmlTags(text: string): string {
		return text.replace(/<[^>]*>/g, "").trim();
	}

	/**
	 * Checks if the given array of Telegraph nodes exceeds the Telegra.ph content size limit (64 KB).
	 * @param nodes The array of TelegraphNode objects.
	 * @throws Error if the content size exceeds 64 KB.
	 */
	checkContentSize(nodes: TelegraphNode[]): void {
		const contentJson = JSON.stringify(nodes);
		const byteSize = new TextEncoder().encode(contentJson).length;
		const maxBytes = 64 * 1024; // 64 KB

		if (byteSize > maxBytes) {
			throw new Error(
				`Content size (${(byteSize / 1024).toFixed(2)} KB) exceeds the Telegra.ph limit of ${(maxBytes / 1024).toFixed(0)} KB. Please reduce the content size.`,
			);
		}
	}

	/**
	 * Sleep for specified number of milliseconds
	 * @param ms Milliseconds to sleep
	 */
	private async sleep(ms: number): Promise<void> {
		return new Promise(resolve => setTimeout(resolve, ms));
	}
}

```

`test_findLocalLinks.js`

```js
const { LinkResolver } = require('./dist/cli.js');

// Test content with local links
const testContent = `# Test Document

This is a [local link](./test.md) and another [relative link](../docs/guide.md).
This is an [external link](https://example.com) and [email](mailto:test@example.com).

Here's another [local file](./images/image.png).
`;

const testBasePath = '/Users/test/project';

console.log('Testing LinkResolver.findLocalLinks...');

try {
  const localLinks = LinkResolver.findLocalLinks(testContent, testBasePath);
  console.log('‚úÖ Success! Found', localLinks.length, 'local links');
  console.log('Local links:', JSON.stringify(localLinks, null, 2));
} catch (error) {
  console.error('‚ùå Error:', error.message);
}

```

`test-relative-links.test.ts`

```ts
import { describe, expect, test } from "bun:test";
import { dirname, join } from "node:path";
import { PagesCacheManager } from "./src/cache/PagesCacheManager";
import { LinkResolver } from "./src/links/LinkResolver";

describe("Relative Links in Nested Folders", () => {
  const testBaseDir = join(__dirname, "test-nested-links");

  test("should resolve relative links correctly from nested folders", () => {
    // Test file: test-nested-links/section1/page1.md
    const filePath = join(testBaseDir, "section1", "page1.md");
    const content = `
# Page 1 in Section 1

## Links to same directory
- [Page 2 in same section](page2.md)

## Links to parent directory
- [Main Index](../index.md)

## Links to sibling directory
- [Page 1 in Section 2](../section2/page1.md)
- [Page 2 in Section 2](../section2/page2.md)

## Links with complex paths
- [Complex path link](../section2/../section1/page2.md)
`;

    const links = LinkResolver.findLocalLinks(content, filePath);

    console.log("Found links:");
    links.forEach(link => {
      console.log(`  Original: ${link.originalPath}`);
      console.log(`  Resolved: ${link.resolvedPath}`);
      console.log(`  Text: ${link.text}`);
      console.log("---");
    });

    // Verify all links are resolved correctly
    expect(links).toHaveLength(5);

    // Same directory link
    const sameDirLink = links.find(l => l.originalPath === "page2.md");
    expect(sameDirLink?.resolvedPath).toBe(join(testBaseDir, "section1", "page2.md"));

    // Parent directory link
    const parentLink = links.find(l => l.originalPath === "../index.md");
    expect(parentLink?.resolvedPath).toBe(join(testBaseDir, "index.md"));

    // Sibling directory links
    const siblingLink1 = links.find(l => l.originalPath === "../section2/page1.md");
    expect(siblingLink1?.resolvedPath).toBe(join(testBaseDir, "section2", "page1.md"));

    const siblingLink2 = links.find(l => l.originalPath === "../section2/page2.md");
    expect(siblingLink2?.resolvedPath).toBe(join(testBaseDir, "section2", "page2.md"));

    // Complex path link
    const complexLink = links.find(l => l.originalPath === "../section2/../section1/page2.md");
    expect(complexLink?.resolvedPath).toBe(join(testBaseDir, "section1", "page2.md"));
  });

  test("should handle cache mapping with nested folder structures", () => {
    const accessToken = "test-token";
    const baseDir = testBaseDir;

    // Create cache manager for the test directory
    const cacheManager = new PagesCacheManager(baseDir, accessToken);

    // Add some pages to cache with nested paths
    const page1Path = join(baseDir, "section1", "page1.md");
    const page2Path = join(baseDir, "section2", "page1.md");

    cacheManager.addPage({
      telegraphUrl: "https://telegra.ph/page1",
      editPath: "page1",
      title: "Page 1 Title",
      authorName: "Test Author",
      publishedAt: new Date().toISOString(),
      lastUpdated: new Date().toISOString(),
      views: 0,
      localFilePath: page1Path
    });
    cacheManager.addPage({
      telegraphUrl: "https://telegra.ph/page2",
      editPath: "page2",
      title: "Page 2 Title",
      authorName: "Test Author",
      publishedAt: new Date().toISOString(),
      lastUpdated: new Date().toISOString(),
      views: 0,
      localFilePath: page2Path
    });

    // Test retrieval by local paths
    const cachedPage1 = cacheManager.getPageByLocalPath(page1Path);
    const cachedPage2 = cacheManager.getPageByLocalPath(page2Path);

    expect(cachedPage1).toBeDefined();
    expect(cachedPage1?.telegraphUrl).toBe("https://telegra.ph/page1");
    expect(cachedPage1?.localFilePath).toBe(page1Path);

    expect(cachedPage2).toBeDefined();
    expect(cachedPage2?.telegraphUrl).toBe("https://telegra.ph/page2");
    expect(cachedPage2?.localFilePath).toBe(page2Path);

    // Test URL lookup
    const url1 = cacheManager.getTelegraphUrl(page1Path);
    const url2 = cacheManager.getTelegraphUrl(page2Path);

    expect(url1).toBe("https://telegra.ph/page1");
    expect(url2).toBe("https://telegra.ph/page2");
  });
});
```



## –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∫–æ–º–∞–Ω–¥–æ–π:

```
prompt-fs-to-ai ./ -p "./**/*.{ts,js}" -p "./src/doc/**/*.md" -e "./dist/**/*" "./.vscode/**/*" "types/**/*" "logs/**/*" "node_modules/**/*" ".specstory/**/*" "memory-bank/**/*" -o "undefined"
```
